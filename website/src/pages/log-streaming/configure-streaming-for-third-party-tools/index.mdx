---
title: Configure Streaming for Third Party Tools
description: How does Log Streaming work & how to leverage it
keywords: 'ibm cloud'
---

In this document, We will show you how to stream log events from LogDNA to [Splunk](https://www.splunk.com/). In order to achieve this, we will use the below two capabilities:
- the streaming feature of logDNA that can be used to stream events to IBM Event Streams.
- the Kafka Splunk connect to stream events from IBM Event Streams to Splunk.
- Configure a Kafka Splunk Connect custom dashboard.


### Create an instance of IBM Event Streams

Click [here](https://cloud.ibm.com/catalog/services/event-streams) to create an instance of IBM Event Streams. Choose a `Standard` plan.

![Create event streams](./images/create_event_streams.png "Create event streams")   

### Create a topic on Event Streams

On the `Event Streams` console, click on `Manage` and then `Topics`.

![Click create topic](./images/click_create_topic.png "Click create topic") 

Enter a topic name `logdnatopic` and click `Next`.

![Enter topic name](./images/enter_topic_name.png "Enter topic name") 

Enter the number of partitions and click `Next`.

![Enter partitions](./images/enter_partitions.png "Enter partitions")

Select message retention time and click `Create Topic`.

![Create topic](./images/create_topic.png "Create topic")

### Note down credentials on Event Streams

Click on `Service credentials`. Note down the `apikey` and `broker urls`.
![Note credentials](./images/note_credentials.png "Note credentials")

### Configure streaming on LogDNA

On the LogDNA Dashboard,navigate to the navigate to the gear icon (settings) > Streaming to enter the credentials gathered in the previous step as follows:

a.	Username = user   //always “token”

b.	Password = api_key // apikey from Event Streams credentials.
c.	Kafka URLs = kafka_brokers_sasl // Entered in on individual lines.

d.	Enter the name of a topic that we created earlier in event streams instance and hit 
“Save”.  

e. Streaming may take up to 15 minutes to begin.


![LogDNA Streams configuration](./images/logdna_streams_config.png "LogDNA Streams Config")

### Set up Kafka Splunk Connect

#### Download Apache Kafka

Download Apache Kafka [here](https://www.apache.org/dyn/closer.cgi?path=/kafka/2.5.0/kafka_2.13-2.5.0.tgz).

Create a folder `kafka_splunk_integration`. 

Extract the contents into the directory `kafka_splunk_integration`. 

#### Build Kafka Splunk jars

1. Clone the repo from https://github.com/splunk/kafka-connect-splunk
2. Verify that Java8 JRE or JDK is installed.
3. Run mvn package. This will build the jar in the /target directory. The name will be splunk-kafka-connect-[VERSION].jar.

Create a folder `connector` in the directory `kafka_splunk_integration`. Copy the jar file to the `connector` folder.

#### Modify connect-distributed.properties for Kafka connect

Download the `connect-distributed.properties` [here](https://github.com/IBM/cloud-enterprise-examples/blob/master/artifacts/logdna-splunk-integration/connect-distrubuted.properties).
- Edit connect-distributed.properties replacing the [BOOTSTRAP_SERVERS] and [APIKEY] placeholders with your Event Streams credentials.
- Modify the `plugin.path` in the file contents to point to the directory `connector` we created in the previous step.
- Now copy the file `connect-distributed.properties` to the `config` folder under the `kafka_2.13-2.5.0` folder.

### Run Kafka connectOpen a terminal. Run the below commands. The [base dir] is the directory under which we created the folder `kafka_splunk_integration`.

```
$ export KAFKA_HOME=[base dir]/kafka_splunk_integration/kafka_2.13-2.5.0
$ $KAFKA_HOME/bin/connect-distributed.sh $KAFKA_HOME/config/connect-distributed.properties
```

### Install Splunk

Typically, those interested in this functionality already have Splunk implemented in their environment and have the technical expertise to configure it with the Kafka messaging services.  

The following instructions describe the steps necessary to create a Splunk instance that is deployed on a workstation using a Docker container.  Additionally, configuring the Splunk system for communication with the IBM Event Streams through Kafka is demonstrated.  

Check the following links to ensure that you have the latest installation pre-requisites satisfied for the platform being used.  

Docker:https://docs.docker.com/engine/install/binaries/

Splunk:https://docs.splunk.com/Documentation/Splunk/8.2.2/Installation/Beforeyouinstall


We will install Splunk inside a container here. 
Open a new terminal. Run the below commands.
```
$docker pull splunk/splunk:latest
$docker run -d -p 8000:8000 -p 8088:8088 -e 'SPLUNK_START_ARGS=--accept-license' -e 'SPLUNK_PASSWORD=Test1234' splunk/splunk:latest
```
Please check if the container is running successfully before moving to the next step.

### Configure Splunk

Open the url - `http://localhost:8000` on a browser. The username is `admin` and password is `Test1234`.
![Splunk Login](./images/splunk_login.png "Splunk Login")

#### Create an index

Click on `Settings` and then select `Indexes`.

![Click indexes](./images/click_indexes.png "Click indexes") 

Click on `New Index`.
![New Index](./images/new_index.png "New Index")  

Enter a name say `logdnaindex` and click on `Save`.

![Index details](./images/index_details.png "Index details")  


The index is now created. Make a note of the index name. We will need it to instantiate the connector.
Next, click on `Settings`-`Data Input`-`HTTP Event Collector` to go to `HTTP Event Collector` page. Click on `General Settings` on HTTP Event Collector page and un-select the `Enable SSL` option.

#### Create a token

Click on `Settings` and then select `Data Input`.

![Select data input](./images/select_data_input.png "Select data input")  

Click on `Add New` to create a new `Http Event Collector`.

![Add new HEC](./images/add_new_hec.png "Add new HEC")

Enter a name say `logdnatoken` and select `Enable Indexer`. Click on `Next`.

![Enter token details](./images/enter_token_details.png "Enter token details")  

Select the index we created earlier and click `Review`.

![Select HEC index](./images/select_hec_index.png "Select HEC Index")

Click `Submit`.

![Click token submit](./images/click_token_submit.png "Click token submit")

Copy the created token. We will need it to instantiate the connector.

![Copy token](./images/copy_token.png "Copy token")  

### Create an instance of Kafka Splunk connector

Open a new terminal. Run the below command after specifying token. Also note that `topics` field is `logdnatopic` that we created earlier in `Event Streams`, and `splunk.indexes` is `logdnaindex` that we created on Splunk.

```
$curl localhost:8083/connectors -X POST -H "Content-Type: application/json" -d '{
   "name": "kafka-connect-splunk",
   "config": {
     "connector.class": "com.splunk.kafka.connect.SplunkSinkConnector",
     "tasks.max": "3",
     "splunk.indexes": "logdnaindex",
     "topics":"logdnatopic",
     "splunk.hec.uri": "http://localhost:8088",
     "splunk.hec.token": "[token]"
   }
 }'
 ```
 
 ### View the log data
 
 #### Create report
 
 Click `Settings` and select `Searches, Reports, and Alerts`.
 
 ![Select reports](./images/select_reports.png "Select reports") 
 
 Click `New Report`.
 
![New report](./images/new_report.png "New Report") 

Enter `Title`, `Search` criteria and click on `Save`.

![Report details](./images/report_details.png "Report details")

#### Run the report and view data

![View data](./images/view_data.png "View data")  
 
### Configure a Kafka Splunk Connect custom dashboard
Go to `Settings` and choose `Searches, reports, and alerts`.

![Create dashboard](./images/create_dashboard2.png "Create dashboard")  

Choose the report you wish to run, similar to the screen below.  The `Gsi Logdna` report is used in this screenshot

![Create dashboard](./images/create_dashboard3d.png "Create dashboard") 

Click on the `Dashboards` icon, similar to the screen shown below. 

![Create dashboard](./images/create_dashboard4.png "Create dashboard") 

Click on the `Create New Dashboard` icon, similar to the screen shown below. 

![Create dashboard](./images/create_dashboard5.png "Create dashboard")

Provide a `Title` for your dashboard and set `Permissions`, similar to the screen shown below. 

![Create dashboard](./images/create_dashboard6.png "Create dashboard")

Click the `+ Add Panel` button, then choose from the `Add Panel` menu,`Messages by minute last 3 hours`, shown below. 

![Create dashboard](./images/create_dashboard8.png "Create dashboard")

Next `Select Visualization` and click on the `Edit Drilldown` icon and choose `Trellis`. 

You can also add a name to the `No title` section, **similar to the screen shown below**. 

![Create dashboard](./images/create_dashboard10e.png "Create dashboard")

Select `Use Trellis Layout`, choose `Size`, `Scale`, and select `Independent`.

![Create dashboard](./images/create_dashboard11.png "Create dashboard")

The `Select Visualization` `Pie Chart` is shown below for the **Messages by minute last 3 hours** Report.

![Create dashboard](./images/create_dashboard13c.png "Create dashboard")

Shown below is the `Select Visualization` `Line Chart` for the **Messages by minute last 3 hours** Report.

![Create dashboard](./images/create_dashboard15b.png "Create dashboard")

You can configure a Splunk dashboard with multiple event data types, similar to the screen shown below. 

![Create dashboard](./images/create_dashboard16.png "Create dashboard")

After configuring the desired dashboard, click `Save`. Your dashboard should show up saved, similar to the screen shown below. 

![Create dashboard](./images/create_dashboard17.png "Create dashboard")

<InlineNotification>

**For more information on configuring Splunk Dashboards**, **see** [Dashboards and Visualizations](https://docs.splunk.com/Documentation/Splunk/8.0.4/Viz/DashboardEditor), [Create dashboards & panels](https://docs.splunk.com/Documentation/Splunk/8.0.4/SearchTutorial/Createnewdashboard) **and** [Splunk application for Kafka Smart Monitoring](https://telegraf-kafka.readthedocs.io/en/latest/)

</InlineNotification>














