{"componentChunkName":"component---src-pages-iac-resources-compute-index-mdx","path":"/iac-resources/compute/","result":{"pageContext":{"frontmatter":{"title":"IaC for Compute & Storage Resources","description":"Use IaC to automate compute resources, images and instances","keywords":"terraform,ibm cloud,compute,instances,images"},"relativePagePath":"/iac-resources/compute/index.mdx","titleType":"page","MdxNode":{"id":"613f35ef-3f6a-5096-bcbe-c820500fad46","children":[],"parent":"44ce4672-34cc-5709-b0cd-4771d1ac426e","internal":{"content":"---\ntitle: IaC for Compute & Storage Resources\ndescription: Use IaC to automate compute resources, images and instances\nkeywords: 'terraform,ibm cloud,compute,instances,images'\n---\n\n<!--\n\nThe pattern to document the resources is like follow:\n- Introduce the resource with an example\n- List all or the most important input parameters\n- If will be used, list the most important output parameters\n- Provide instructions to get the value of the input parameters, either using `ibmcloud`, API or the Web console.\n- If needed, instructions to execute the code either with Terraform or Schematics\n\n-->\n\n<!--\nTODO: Is is possible to get the image and profile from ibm_is_images and ibm_is_instance_profiles ??\nTODO: The code is failing, there is no access to the VSIs, could be the Health Check\nTODO: Move all the Hard Coded code to variables, document how to get the values\nTODO: Update the final code\n-->\n\n<PageDescription>\n\nAutomating compute resources with Terraform and Schematics. Compute images and instances life cycle\n\n</PageDescription>\n\nOne of the basic and atomic services every cloud provide is compute. Compute on IBM Cloud covers different services however this pattern guide is about the IBM Cloud Virtual Servers, also called Virtual Server Instances or VSI, how to customize public and private virtual servers that scale up or down to suit your needs, using Terraform and IBM Cloud Schematics. Other IBM Cloud Compute services such as Kubernetes, OpenShift, Container Registry and Cloud Functions are covered in other pattern guides.\n\nBased on the [Network Resources](/iac-resources/network) pattern we would like to add a few compute resources and storage to host and expose an API with movies.\n\nThe following diagram shows the proposed architecture.\n\n![architecture](./images/IaC-Compute_Resources.png \"Compute Resources Architecture\")\n\nThe code to build these resources can be downloaded from the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [07-compute](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/07-compute).\n\n<AnchorLinks small>\n  <AnchorLink>Virtual Server Instance</AnchorLink>\n  <AnchorLink>Virtual Server Instance with IBM Cloud Schematics</AnchorLink>\n  <AnchorLink>Load Balance a Cluster of VSI</AnchorLink>\n  <AnchorLink>IBM Cloud Object Storage</AnchorLink>\n  <AnchorLink>Volumes</AnchorLink>\n  <AnchorLink>Final Terraform code</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n  <AnchorLink>Compute Resources & Data Source Reference</AnchorLink>\n</AnchorLinks>\n\n<InlineNotification>\n\n**Requirements**\n\nTo be able to execute and complete the instructions in this page, make sure you have an **IBM Cloud account**: if you don't have one yet, you can [create a Lite account](https://cloud.ibm.com/docs/overview?topic=overview-quickstart_lite#prereqs-lite) for free.\n\nAlso make sure you have the environment setup as explained in the [Setup Environment](/iac/setup-environment) page to have installed the IBM Cloud CLI, login into your account with the IBM Cloud CLI and create a SSH Key.\n\n</InlineNotification>\n\n## Virtual Server Instance\n\nBefore create a VSI we need to have all the networking resources created. The list of resources includes: VPC, Subnets in one or more zones, Public gateway for public internet communication, ACL's for inbound and outbound traffic to the subnets. All these resources management is covered in the [Networking section](/iac/networking) where the required network resources were created in the `networking.tf` file.\n\nThe following code is an example to create a VSI, for this we use the `ibm_is_instance` resource.\n\n```hcl path=compute.tf\nresource \"ibm_is_ssh_key\" \"iac_app_key\" {\n  name       = \"${var.project_name}-${var.environment}-key\"\n  public_key = var.public_key\n}\n\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  name    = \"${var.project_name}-${var.environment}-instance\"\n  image   = \"r006-14140f94-fcc4-11e9-96e7-a72723715315\"\n  profile = \"cx2-2x4\"\n\n  primary_network_interface {\n    name            = \"eth1\"\n    subnet          = ibm_is_subnet.iac_app_subnet.id\n    security_groups = [ibm_is_security_group.iac_app_security_group.id]\n  }\n\n  vpc  = ibm_is_vpc.iac_app_vpc.id\n  zone = \"us-south-1\"\n  keys = [ibm_is_ssh_key.iac_app_key.id]\n\n  user_data = <<-EOUD\n              #!/bin/bash\n              echo \"Hello World\" > index.html\n              nohup busybox httpd -f -p ${var.port} &\n              EOUD\n\n  tags = [\"iac-${var.project_name}-${var.environment}\"]\n}\n```\n\nThe variables used in this code are defined at `variables.tf` like so:\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"public_key\" {}\n\nvariable \"port\" {\n  default = 8080\n}\n```\n\nThe value of the variables are set in the `terraform.tfvars` and `*.auto.tfvars` files. As we are going to use this code with the Terraform CLI on our local host and with IBM Cloud Schematics we should not use `file(pathexpand(var.public_key_file))` to get the value of a public key file like `~/.ssh/id_rsa.pub` because it doesn't work on IBM Cloud Schematics. Instead let's send the content of the public key file to the variable `public_key` in the `secrets.auto.tfvars` file using the following command.\n\n```bash\necho -n \"public_key = \\\"$(cat ~/.ssh/id_rsa.pub)\\\"\\n\" > secrets.auto.tfvars\n```\n\nIt is **important** you make sure to include the file `secrets.auto.tfvars` to the `.gitignore` file so you don't share your secrets to the world.\n\nTo test this VSI we'll deploy a server to expose an API from a [movies database](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/07-compute/db.json) JSON file. This is a sample of it:\n\n```json\n{\n  \"movies\": [{\n    \"id\": \"83\",\n    \"title\": \"Akira Kurosawa's Dreams\",\n    \"summary\": \"This is essentially eight separate short films, with some overlaps in characters and thematic material - that of man's relationship with his environment. 'Sunshine Through The Rain' - 'The Peach Orchard' - 'The Blizzard' - 'Crows' - 'Mount Fuji in Red' - The Weeping Demon' - 'Village of the Watermills'\",\n    \"year\": \"1990\",\n    \"duration\": \"7173610\",\n    \"originallyAvailableAt\": \"1990-05-11\",\n    \"addedAt\": \"1348382738\",\n    \"updatedAt\": \"1531023643\",\n    \"genre\": \"Sci-Fi & Fantasy\",\n    \"director\": [\"Akira Kurosawa\", \"Ishir√¥ Honda\"],\n    \"writer\": [\"Akira Kurosawa\"],\n    \"cast\": [\"Akira Terao\", \"Mitsuko Baisho\", \"Toshie Negishi\"]\n  }, {\n    ...\n  }]\n}\n```\n\nTo deploy the JSON file to the provisioned VSI the first step is to load the file using the data source [local_file](https://www.terraform.io/docs/providers/local/d/file.html), the file content can be obtained with the attribute `content` or `content_base64` if you require the file content encoded.\n\nIn the `user_data` attribute use the command `echo` with `base64` to print the decoded content of the JSON file that was previously encoded using the `content_base64` attribute of the `local_file` data source. Terraform sends the content of the file to the IBM Cloud engine through HTTP, it is recommended to encode this text otherwise we can get unexpected results. That's why we'll use the attribute `content_base64` instead of `content`, and we use the command `base64`, at the server side, to decode the received text.\n\nModify the value of the `user_data` attribute of `ibm_is_instance.iac_app_instance` and add the `local_file` data source, like this:\n\n```hcl path=compute.tf\ndata \"local_file\" \"db\" {\n  filename = \"${path.module}/db.min.json\"\n}\n\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  ...\n  user_data = <<-EOUD\n            #!/bin/bash\n            echo '${data.local_file.db.content_base64}' | base64 --decode > /var/lib/db.min.json\n\n            echo '* libraries/restart-without-asking boolean true' | debconf-set-selections\n            apt update\n            curl -sL https://deb.nodesource.com/setup_13.x | sudo -E bash -\n            apt-get install -y nodejs\n            npm install -g json-server\n\n            json-server --watch /var/lib/db.min.json --port ${var.port} --host 0.0.0.0 &\n            EOUD\n  ...\n}\n```\n\n<InlineNotification>\n\n**user_data vs provisioners**\n\nShould I use `user_data` or provisioners such as `file` and `remote-exec`?\n\nTerraform is a provisioning tool, it should not do configuration tasks maybe just trigger them. Among the configuration management tools are: Ansible, Cloud-Init, Puppet and (the less recommended) shell scripts. Thus, the appropriate way to configure a server or trigger the configuration should be using `user_data`, it can execute either a Cloud-Init code, a script or to start a configuration management tool like Ansible. The `user_data` is also used by the auto-scaling in IBM Cloud and Terraform provisioners are not.\n\nThe provisioners such as `file` and `remote-exec` are specific to Terraform that's why they are not suggested for configuration management. If you would like to use them, read [Using Terraform Provisioners](/iac/resources/compute/provisioners).\n\n</InlineNotification>\n\nWe are introducing a new Terraform plugin (`local`) with the `local_file` data source, so before apply the infrastructure code and deploy the application we need to download this plugin with the `terraform init` command. Execute the following commands to apply the new code:\n\n```bash\nterraform init\nterraform plan\nterraform apply\n```\n\nIn about a minute the VSI is provisioned but the API server will be running in about an extra minute. To verify it's working get the `public_ip` output variable to fetch some movies, like so:\n\n```bash\ncurl \"$(terraform output entrypoint)/movies\" | jq\ncurl \"$(terraform output entrypoint)/movies/675\"\ncurl \"$(terraform output entrypoint)/movies?id=1067&id=1649\"\n```\n\nIf you do not see any response, please, give it a minute or two, it takes some time to install all the dependencies to run the API server.\n\n## Virtual Server Instance with IBM Cloud Schematics\n\nAs seen in the [IBM Cloud Schematics](/iac/schematics) pattern a similar HCL code can be used with IBM Cloud Schematics and Terraform CLI, the only change is that Schematics cannot access your local filesystem but it can access the files located in the git repository or remote files.\n\nTo create this project on IBM Cloud Schematics create a JSON Workspace template file like so:\n\n```json path=workspace.tmpl.json\n{\n  \"name\": \"iac_schematics_test\",\n  \"type\": [\n    \"terraform_v0.12\"\n  ],\n  \"description\": \"Sample workspace to test IBM Cloud Schematics. Deploys an web server on a VSI with a Hello World response\",\n  \"tags\": [\n    \"app:helloworld\",\n    \"env:dev\"\n  ],\n  \"template_repo\": {\n    \"url\": \"https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/07-compute\"\n  },\n  \"template_data\": [{\n    \"folder\": \".\",\n    \"type\": \"terraform_v0.12\",\n    \"variablestore\": [{\n        \"name\": \"project_name\",\n        \"value\": \"iac_schematics_test\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"environment\",\n        \"value\": \"dev\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"public_key\",\n        \"value\": \"{ PUBLIC_KEY }\",\n        \"type\": \"string\"\n      }\n    ]\n  }]\n}\n```\n\nThen execute the following commands to create the Workspace, provision the VSI and deploy the application.\n\n<!--\n\nIn the current version of the IBM Cloud Schematics plugin there is an error when the terraform_v0.12 is used.\nTo workaround this error export the variable:\n\n  export X_FEATURE_SANDBOX=true\n\nThis error should be fixed in the next release of the plugin\n\n-->\n\n```bash\n# Verify you are logged in to the right account\nibmcloud target\n\n# Get the content of the SSH public key and render it into the JSON file using the template\nPUBLIC_KEY=\"$(cat ~/.ssh/id_rsa.pub)\"\nsed \"s|{ PUBLIC_KEY }|$PUBLIC_KEY|\" workspace.tmpl.json > workspace.json\n\n# Create the Schematics Workspace\nibmcloud schematics workspace new --file workspace.json\n\n# List the Workspaces and get the Workspace ID\nibmcloud schematics workspace list\nID=$(ibmcloud schematics workspace list --json | jq -r '.workspaces[] | select(.name == \"iac_schematics_test\") | .id')\n\n# Planning and Provisioning\nact_ID=$(ibmcloud schematics plan --id $ID --json | jq -r '.activityid')\nibmcloud schematics logs --id $ID --act-id $act_ID\n\nact_ID=$(ibmcloud schematics apply --id $ID --force --json | jq -r '.activityid')\nibmcloud schematics logs --id $ID --act-id $act_ID\n```\n\nYou may have to wait 2-3 seconds between the execution of an action (i.e. `apply`) and the retrieval of the logs. Also, you may have to execute the retrieval of the `apply` logs several times until the task is completed.\n\nIf the Workspace JSON file is modified, for example a variable value, you can update it with the command `ibmcloud schematics workspace update`.\n\nTo verify the application is running, get the output variables and use `curl` to get the page:\n\n```bash\nibmcloud schematics workspace output --id $ID --json\nIP=$(ibmcloud schematics workspace output --id $ID --json | jq -r '.[].output_values[].ip_address.value')\nADDR=$(ibmcloud schematics workspace output --id $ID --json | jq -r '.[].output_values[].entrypoint.value')\n\ncurl \"${ADDR}/movies/675\"\n```\n\n<InlineNotification kind=\"warning\">\n\n**Sensitive data in JSON file**\n\nThe file `workspace.json` contain your SSH public key, make sure this file is not committed to the Git repository. Add the `workspace.json` filename to the `.gitignore` file.\n\n</InlineNotification>\n\n## Identify Input Parameters Values\n\nIn the Getting Started with Terraform section was explained how to get the value of the input parameters such as the instance `image` and `profile` using the IBM Cloud CLI and some Unix commands.\n\nIt's also possible to get these parameters using the Data Sources `ibm_is_instance_profile` and `ibm_is_image`. Modify the `compute.tf` file to add the following data sources.\n\n```hcl path=compute.tf\ndata \"ibm_is_image\" \"ds_iac_app_image\" {\n  name = \"ibm-ubuntu-18-04-1-minimal-amd64-1\"\n}\n\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  ...\n  image   = data.ibm_is_image.ds_iac_app_image.id\n  ...\n}\n```\n\nThis change is not much but give more information to the developer reading the code if the the image name is used instead of the ID. You still need to execute the following IBM Cloud CLI commands to find the image and profile name, like so:\n\n```bash\nibmcloud is images\nibmcloud is images | grep available | grep ubuntu-18 | grep amd64 | cut -f2 -d\" \"\nibmcloud is images --json | jq -r '.[] | select(.status==\"available\" and .operating_system.name==\"ubuntu-18-04-amd64\").name'\n\nibmcloud is instance-profiles\nibmcloud is instance-profiles | grep amd64 | sort -k4 -k5 -n | head -1 | cut -f1 -d\" \"\nibmcloud is instance-profiles --json | jq -r 'map(select(.vcpu_architecture.value==\"amd64\")) | sort_by(.memory.value)[0].name'\n```\n\n<!-- TODO: Is is possible to get the image and profile from ibm_is_images and ibm_is_instance_profiles ?? -->\n\n## Load Balance a Cluster of VSI\n\nRunning a single server serve the purpose but it's a single point of failure, if this single server fails the API is not accessible. The solution is to have a cluster of Virtual Servers, routing the traffic to the servers that are working and scaling the size up or down based on the traffic load. IBM Cloud allow us to get this features with the Load Balancer service and Terraform resources.\n\nYou can read more about the type of Load Balancers, Listeners, Pools, LB Methods and more in the [Load Balancer documentation](https://cloud.ibm.com/docs/vpc?topic=vpc-load-balancers) for IBM CLoud Gen 2.\n\nWe'll begin the Load Balancer code by creating a `ibm_is_lb` resource in the `lb.tf` file, like so:\n\n```hcl path=lb.tf\nresource \"ibm_is_lb\" \"iac_app_lb\" {\n  name    = \"${var.project_name}-${var.environment}-lb\"\n  subnets = [ibm_is_subnet.iac_app_subnet.id]\n}\n```\n\nIt requires an ID reference of the subnets where the LB is located. The VSI that are served by this Load Balancer should be in the same VPC and region. This specific LB name is formed by the project name and environment to not collide with other Load Balancers. Other important parameter is `type` to define if the LB will be `public` (default) or `private`, this would be public so the fully qualified domain name (FQDN) will be accessible from the internet and have assigned multiple public IP addresses.\n\nFor private load balancers the access is restricted to internal clients on the same subnet, region and VPC. It also has assigned a FQDN with multiple IP addresses and only accepts traffic from [RFC1918](https://tools.ietf.org/html/rfc1918) address spaces such as those in the blocks 10.0.0.0/8, 172.16.0.0/12 and 192.168.0.0/16.\n\nThe Load Balancer needs a listener to listen on a given port and protocol. The FQDN and port assigned to the load balancer are exposed to the public internet. The traffic is then redirected to a pool member or VSI from the assigned default pool. The API on the VSI is listening/serving on port `8080` by default, unless a different port is set in the `port` variable. We can make the LB listener to use the same port or different, in this case, let's use the same.\n\nThe available protocols on the LB listener are **HTTP**, **HTTPS** and **TCP**, this one uses the HTTP protocol. The pool or VSI protocols supported are only HTTP and TCP, in this case it's HTTP. Let's append the following code to `lb.tf` to define the LB listener:\n\n```hcl path=lb.tf\nresource \"ibm_is_lb_listener\" \"iac_app_lb_listener\" {\n  lb       = ibm_is_lb.iac_app_lb.id\n  port     = var.port\n  protocol = \"http\"\n  default_pool = ibm_is_lb_pool.iac_app_lb_pool.id\n}\n```\n\nThe `ibm_is_lb` ID, `port` and `protocol` are required parameters. Other optional parameters are `default_pool`, `certificate_instance` and `connection_limit`, the last two are not needed for this project.\n\nOn the other side of the Load Balancer are the VSI or backend application which are identified as a Pool Member (`ibm_is_lb_pool_member`) that is, obviously, a member of the defined pool (`ibm_is_lb_pool`). Let's begin modifying the VSI `ibm_is_instance.iac_app_instance` resource in the `compute.tf` file to have multiple instances adding the Terraform `count` parameter and modifying the `name` parameter to include the number of that instance. Like so:\n\n```hcl path=compute.tf\n...\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  name    = \"${var.project_name}-${var.environment}-instance-${format(\"%02s\", count.index)}\"\n  ...\n  count   = var.max_size\n  ...\n}\n```\n\nThe new variable `max_size` defines how many VSI, therefore how many pool members too, will be created. This variable is defined like this in the `variables.tf` file:\n\n```hcl\nvariable \"max_size\" {\n  default = 3\n}\n```\n\nAll is set to create the Pool and the Pool Members, add the following code to the `lb.tf` file:\n\n```hcl path=lb.tf\nresource \"ibm_is_lb_pool\" \"iac_app_lb_pool\" {\n  name                = \"${var.project_name}-${var.environment}-lb-pool\"\n  lb                  = ibm_is_lb.iac_app_lb.id\n  algorithm           = \"round_robin\"\n  protocol            = \"http\"\n  health_delay        = 5\n  health_retries      = 2\n  health_timeout      = 2\n  health_type         = \"http\"\n  health_monitor_url  = \"/\"\n  health_monitor_port = var.port\n}\n\nresource \"ibm_is_lb_pool_member\" \"iac_app_lb_pool_mem\" {\n  count          = var.max_size\n  lb             = ibm_is_lb.iac_app_lb.id\n  pool           = ibm_is_lb_pool.iac_app_lb_pool.id\n  port           = var.port\n  target_address = ibm_is_instance.iac_app_instance[count.index].primary_network_interface.0.primary_ipv4_address\n  weight         = (100 - count.index)\n}\n```\n\nThe `ibm_is_lb_pool.iac_app_lb_pool` resource requires the following input attributes:\n\n| Input parameter | Description |\n|---|---|\n| `name` | name of the pool. In this case it has the name of the project and environment to not collide with other pools  |\n| `lb` | ID of the load balancer is linked to |\n| `algorithm` | load balancing algorithm. Supported values are `round_robin`, `weighted_round_robin`, or `least_connections` |\n| `protocol` | pool protocol. Supported values are `http`, and `tcp` |\n| `health_delay` | health check interval in seconds. Interval must be greater than timeout value |\n| `health_retries` | health check max retries |\n| `health_timeout` | health check timeout in seconds |\n| `health_type` | pool protocol. Supported values are `http`, and `tcp` |\n\nOther input and output parameters are described in the `ibm_is_lb_pool` [resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources#lb-pool). The 3 different load balancing methods to set in the `algorithm` input parameter are described in the [Load Balancers documentation](https://cloud.ibm.com/docs/vpc?topic=vpc-load-balancers#load-balancing-methods).\n\nThe `ibm_is_lb_pool_member.iac_app_lb_pool_mem` is actually a list of resources because it also has the Terraform `count` attribute, just like the `ibm_is_instance` resource and this is because we need one pool member per VSI.\n\nThe `ibm_is_lb_pool_member` uses the `target_address` attribute to link the pool member to the VSI using its IP address. Here we use the `count.index` to reference to the VSI with the same index, so the pool member 0 is linked to the VSI 0, and so on. As we did to link the Floating IP to the IP address of the VSI, this IP address is identified by the `ibm_is_instance.iac_app_instance` resource output parameter `primary_network_interface.0.primary_ipv4_address`.\n\nThe required input attributes for `ibm_is_lb_pool_member` are:\n\n| Input parameter | Description |\n|---|---|\n| `pool` | ID of the load balancer pool |\n| `lb` | load balancer ID |\n| `port` | port number of the application running in the server member, in this case is in the variable `port` |\n| `target_address` | IP address of the pool member or VSI |\n| `weight` | weight of the server member. This parameter is optional and it takes effect only when the load balancing algorithm of its belonging pool is `weighted_round_robin` |\n\nMore information about this resource can be found at the `ibm_is_lb_pool_member` [resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources#lb-pool-member).\n\nThe load balancer adjusts its capacity automatically according to the load. When this adjustment occurs, you may see a change in the number of IP addresses associated with the load balancer's DNS name. To know the load balancer's DNS name and the associated IP addresses we use the following output variables to be included in the file `output.tf`\n\n```hcl path=output.tf\noutput \"lb_ip_address\" {\n  value = ibm_is_lb.iac_app_lb.public_ips\n}\n\noutput \"entrypoint\" {\n  value = \"http://${ibm_is_lb.iac_app_lb.hostname}:${var.port}\"\n}\n```\n\nThe new value of `entrypoint` has now the hostname or FQDN of the load balancer.\n\nHaving a load balancer there is no need to have floating IP assigned to every VSI so you can remove them but in case you want to keep them you need one per VSI, so you have to do something similar done with the pool members modifying the `ibm_is_floating_ip.iac_app_floating_ip` and the `ip_address` output variable that uses this resource, like so:\n\n```hcl network.tf\nresource \"ibm_is_floating_ip\" \"iac_app_floating_ip\" {\n  name   = \"${var.project_name}-${var.environment}-ip\"\n  target = ibm_is_instance.iac_app_instance[count.index].primary_network_interface.0.id\n  count  = var.max_size\n}\n```\n\n```hcl\noutput \"ip_address\" {\n  value = ibm_is_floating_ip.iac_app_floating_ip[*].address\n}\n```\n\nHere we use `[*]` to let Terraform knows that we need the `address` of all the `iac_app_floating_ip` resources. This variable would be a list just like `lb_ip_address`.\n\nBut, again, there is no need to have Floating IP's assigned per VSI, the Load Balancer provide the FQDN and an IP per pool member or VSI. Remove the Floating IP's once you verify the Load Balancer works.\n\n## Health Checks\n\nHealth check definitions are mandatory for back-end pools. Not having health checks cause the pool identify the pool members as unhealthy therefore not forwarding new connections to them.\n\nThe health check is configured in the `ibm_is_lb_pool` resource using the `health_*` attributes. Read the [Health Check documentation](https://cloud.ibm.com/docs/vpc?topic=vpc-load-balancers#health-checks) and the `ibm_is_lb_pool` [input attributes](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources#lb-pool-input) that can be used for health check.\n\nHealth checks can be configured on back-end ports or on a separate health check port, based on the application. For this application, the API is being served with HTTP protocol, so we ser the input parameter `health_type` of `ibm_is_lb_pool` to `http`. The port to monitor is the same port where the API is exposed so the `health_monitor_port` parameter is set to the value we have on the variable `port` but if not set, the port used in the pool member will be used. Finally, the URL to monitor is the path to be used for the health check, the default value is `/` but we may have something like `/health`. If the HTTP response code is `200` then the pool member is considered healthy.\n\n## IBM Cloud Object Storage\n\nThere are different type of storage objects in cloud environment and you use different types for different situations Review the [IBM Cloud Object Storage](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-about-cloud-object-storage) resources to know how to use them and which storage object to use.\n\nWe use the resource `ibm_cos_bucket` to create an Object Storage bucket to store data but it requires a `ibm_resource_instance` to be created in advance. There are 5 different storage classes to choose:\n\n- **Smart Tier** (`smart`) can be used for any workload, especially dynamic workloads where access patterns are unknown or difficult to predict. Smart Tier provides a simplified pricing structure and automatic cost optimization by classifying the data into \"hot\", \"cool\", and \"cold\" tiers based on monthly usage patterns. All data in the bucket is then billed at the lowest applicable rate. There are no threshold object sizes or storage periods, and there are no retrieval fees.\n- **Standard** (`standard`) is used for active workloads, with no charge for data retrieved (other than the cost of the operational request itself).\n- **Vault** (`vault`) is used for cool workloads where data is accessed less than once a month - an extra retrieval charge ($/GB) is applied each time data is read. The service includes a minimum threshold for object size and storage period consistent with the intended use of this service for cooler, less-active data.\n- **Cold Vault** (`cold`) is used for cold workloads where data is accessed every 90 days or less - a larger extra retrieval charge ($/GB) is applied each time data is read. The service includes a longer minimum threshold for object size and storage period consistent with the intended use of this service for cold, inactive data.\n- **Flex** (`flex`) is being replaced by Smart Tier for dynamic workloads.\n\nIn our demo application we use this bucket to store images with the movie covers and they are actively used, so the object class to choose is `standard`. For more information about storage classes, see [Use storage classes](https://cloud.ibm.com/docs/services/cloud-object-storage?topic=cloud-object-storage-classes).\n\nLets create the storage with the following code:\n\n```hcl\nresource \"ibm_resource_instance\" \"iac_app_cos_instance\" {\n  name     = \"${var.project_name}-${var.environment}-cos-instance\"\n  service  = \"cloud-object-storage\"\n  plan     = \"standard\"\n  location = \"global\"\n}\n\nresource \"ibm_cos_bucket\" \"iac_app_cos_bucket\" {\n  bucket_name          = \"${var.project_name}-${var.environment}-bucket\"\n  resource_instance_id = ibm_resource_instance.iac_app_cos_instance.id\n  region_location      = \"us-south\"\n  storage_class        = \"smart\"\n}\n```\n\nThe `ibm_cos_bucket` resource requires the following input parameters:\n\n| Input parameter | Description |\n|---|---|\n| `bucket_name` | to name the bucket |\n| `resource_instance_id` | ID of the `ibm_resource_instance` service instance for which you want to create a bucket |\n| `storage_class` | The storage class that you want to use for the bucket |\n| `region_location` | location of a regional bucket. Do not use this parameter with other `*_location` parameter |\n| `single_site_location` | location for a single site bucket. Do not use this parameter with other `*_location` parameter |\n| `cross_region_location` | location for a cross-regional bucket. Do not use this parameter with other `*_location` parameter |\n\nFor more information about other optional input parameters and the output parameters read the `ibm_cos_bucket` [IBM Cloud Object Storage resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-object-storage-resources#cos-bucket).\n\nThe upload of data to the bucket is not done with Terraform nor Schematics, but can be done using code in different languages such as [Go](https://cloud.ibm.com/docs/services/cloud-object-storage/libraries?topic=cloud-object-storage-go), [Python](https://cloud.ibm.com/docs/services/cloud-object-storage/libraries?topic=cloud-object-storage-python), [Node](https://cloud.ibm.com/docs/services/cloud-object-storage/libraries?topic=cloud-object-storage-node) or [Java](https://cloud.ibm.com/docs/services/cloud-object-storage/libraries?topic=cloud-object-storage-java) using the SDK, or using the Linux commands (i.e. `curl`) with the [Cloud Storage API](https://cloud.ibm.com/docs/services/cloud-object-storage/api-reference?topic=cloud-object-storage-compatibility-api), or using any file transfer tools such as [Cyberduck](https://cyberduck.io/) or [Transmit](https://panic.com/transmit/) and command-line utilities like [s3cmd](https://github.com/s3tools/s3cmd) or [Minio Client](https://github.com/minio/mc), and many others.\n\n## Block Storage for Volumes\n\nBesides IBM Cloud Storage Object we can attach volumes to the instance to store data or files to access though file system interface, file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage.\n\nBlock Storage are block-level volumes that can be attached to VSI as either a boot volume or as a data volume. The boot volumes are attached by default. The [Block Storage for VPC](https://cloud.ibm.com/docs/vpc?topic=vpc-block-storage-about) documentation can give you more information.\n\nTo create a data volume per instance we use the `ibm_is_volume` resource with a code like this one.\n\n```hcl path=storage.tf\nresource \"ibm_is_volume\" \"iac_app_volume\" {\n  count    = var.max_size\n  name     = \"${var.project_name}-${var.environment}-volume-${format(\"%02s\", count.index)}\"\n  profile  = \"10iops-tier\"\n  zone     = \"us-south-1\"\n  capacity = 100\n}\n```\n\nThe main input parameters for `ibm_is_volume` are described in the following table.\n\n| Input parameter | Description |\n|---|---|\n| `name` | to name the volume |\n| `profile` | volume profile |\n| `zone` | location of the volume |\n| `capacity` | capacity of the volume in gigabytes. the default value is `100` |\n| `iops` | total input/ output operations per second (IOPS) for your storage. This value is required for custom storage profiles only |\n\nThe [Block Storage Capacity and Performance](https://cloud.ibm.com/docs/vpc?topic=vpc-capacity-performance) documentation give you information about the available capacities and the performance (IOPS and Throughput) per capacity.\n\nTo get a list of volume profiles with the CLI use the following command, also the [Profiles](https://cloud.ibm.com/docs/vpc?topic=vpc-block-storage-profiles) documentation list the available profiles and explain how to define a custom one.\n\n```bash\nibmcloud is volume-profiles\n```\n\nHaving the Block Storage is not enough, you need to assign the volume to the VSI using the `volumes` list attribute of the `ibm_is_instance` resource, like so:\n\n```hcl path=compute.tf\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  ...\n  volumes = [ibm_is_volume.iac_app_volume[count.index].id]\n  ...\n}\n```\n\nNotice the use of `count.index` to get one volume id. This way we ensure the volume `0` is assigned to the instance `0` and so on. The volume is mounted in the root partition, so there is no need to modify the user data to change the location of the JSON DB file.\n\nVerify and apply all the changes executing the commands:\n\n```bash\nterraform plan\nterraform apply\n```\n\nTo test the API works execute the following command to get the Load Balancer FQDN or entrypoint to be used by the `curl` command:\n\n```bash\ncurl $(terraform output entrypoint)/movies/675\n```\n\n## Final Terraform code\n\nYou can download the code from the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [07-compute](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/07-compute) where the main files are:\n\n```hcl path=compute.tf\nresource \"ibm_is_ssh_key\" \"iac_app_key\" {\n  name       = \"${var.project_name}-${var.environment}-key\"\n  public_key = var.public_key\n}\n\ndata \"local_file\" \"db\" {\n  filename = \"${path.module}/db.min.json\"\n}\n\ndata \"ibm_is_image\" \"ds_iac_app_image\" {\n  name = \"ibm-ubuntu-18-04-1-minimal-amd64-1\"\n}\n\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  count   = var.max_size\n  name    = \"${var.project_name}-${var.environment}-instance-${format(\"%02s\", count.index)}\"\n  image   = data.ibm_is_image.ds_iac_app_image.id\n  profile = \"cx2-2x4\"\n\n  primary_network_interface {\n    name            = \"eth1\"\n    subnet          = ibm_is_subnet.iac_app_subnet.id\n    security_groups = [ibm_is_security_group.iac_app_security_group.id]\n  }\n\n  vpc     = ibm_is_vpc.iac_app_vpc.id\n  zone    = \"us-south-1\"\n  keys    = [ibm_is_ssh_key.iac_app_key.id]\n  volumes = [ibm_is_volume.iac_app_volume[count.index].id]\n\n  user_data = <<-EOUD\n            #!/bin/bash\n            echo '${data.local_file.db.content_base64}' | base64 --decode > /var/lib/db.min.json\n\n            echo '* libraries/restart-without-asking boolean true' | debconf-set-selections\n\n            apt update\n            curl -sL https://deb.nodesource.com/setup_13.x | sudo -E bash -\n            apt-get install -y nodejs\n            npm install -g json-server\n\n            json-server --watch /var/lib/db.min.json --port ${var.port} --host 0.0.0.0 &\n            EOUD\n\n  tags = [\"iac-${var.project_name}-${var.environment}\"]\n}\n```\n\n```hcl path=lb.tf\nresource \"ibm_is_lb\" \"iac_app_lb\" {\n  name    = \"${var.project_name}-${var.environment}-lb\"\n  subnets = [ibm_is_subnet.iac_app_subnet.id]\n}\n\nresource \"ibm_is_lb_listener\" \"iac_app_lb_listener\" {\n  lb       = ibm_is_lb.iac_app_lb.id\n  port     = var.port\n  protocol = \"http\"\n  default_pool = ibm_is_lb_pool.iac_app_lb_pool.id\n}\n\nresource \"ibm_is_lb_pool\" \"iac_app_lb_pool\" {\n  name                = \"${var.project_name}-${var.environment}-lb-pool\"\n  lb                  = ibm_is_lb.iac_app_lb.id\n  algorithm           = \"round_robin\"\n  protocol            = \"http\"\n  health_delay        = 5\n  health_retries      = 2\n  health_timeout      = 2\n  health_type         = \"http\"\n  health_monitor_url  = \"/\"\n  health_monitor_port = var.port\n}\n\nresource \"ibm_is_lb_pool_member\" \"iac_app_lb_pool_mem\" {\n  count          = var.max_size\n  lb             = ibm_is_lb.iac_app_lb.id\n  pool           = ibm_is_lb_pool.iac_app_lb_pool.id\n  port           = var.port\n  target_address = ibm_is_instance.iac_app_instance[count.index].primary_network_interface.0.primary_ipv4_address\n  weight         = (100 - count.index)\n}\n```\n\n```hcl path=storage\nresource \"ibm_resource_instance\" \"iac_app_cos_instance\" {\n  name     = \"${var.project_name}-${var.environment}-cos-instance\"\n  service  = \"cloud-object-storage\"\n  plan     = \"standard\"\n  location = \"global\"\n}\n\nresource \"ibm_cos_bucket\" \"iac_app_cos_bucket\" {\n  bucket_name          = \"${var.project_name}-${var.environment}-bucket\"\n  resource_instance_id = ibm_resource_instance.iac_app_cos_instance.id\n  region_location      = \"us-south\"\n  storage_class        = \"smart\"\n}\n\nresource \"ibm_is_volume\" \"iac_app_volume\" {\n  count    = var.max_size\n  name     = \"${var.project_name}-${var.environment}-volume-${format(\"%02s\", count.index)}\"\n  profile  = \"10iops-tier\"\n  zone     = \"us-south-1\"\n  capacity = 100\n}\n```\n\n```hcl path=output.tf\noutput \"lb_ip_address\" {\n  value = ibm_is_lb.iac_app_lb.public_ips\n}\n\noutput \"entrypoint\" {\n  value = \"http://${ibm_is_lb.iac_app_lb.hostname}:${var.port}\"\n}\n```\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"public_key\" {}\n\nvariable \"port\" {\n  default = 8080\n}\nvariable \"max_size\" {\n  default = 3\n}\n```\n\n```json path=workspace.tmpl.json\n{\n  \"name\": \"iac_schematics_test\",\n  \"type\": [\n    \"terraform_v0.12\"\n  ],\n  \"description\": \"Sample workspace to test IBM Cloud Schematics. Deploys an web server on a VSI with a Hello World response\",\n  \"tags\": [\n    \"app:helloworld\",\n    \"env:dev\"\n  ],\n  \"template_repo\": {\n    \"url\": \"https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/07-compute\"\n  },\n  \"template_data\": [{\n    \"folder\": \".\",\n    \"type\": \"terraform_v0.12\",\n    \"variablestore\": [{\n        \"name\": \"project_name\",\n        \"value\": \"iac_schematics_test\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"environment\",\n        \"value\": \"dev\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"public_key\",\n        \"value\": \"{ PUBLIC_KEY }\",\n        \"type\": \"string\"\n      }\n    ]\n  }]\n}\n```\n\n## Clean up\n\nIn this section you have created the same infrastructure using Terraform CLI and IBM Cloud Schematics.\n\nTo destroy everything created with Terraform CLI, execute:\n\n```bash\nterraform destroy\n```\n\nTo delete everything you've created with IBM Cloud Schematics, execute the following command to destroy the infrastructure:\n\n```bash\nID=$(ibmcloud schematics workspace list --json | jq -r '.workspaces[] | select(.name == \"iac_schematics_test\") | .id')\nibmcloud schematics destroy --id $ID\n\n# Or:\nact_ID=$(ibmcloud schematics destroy --id $ID --force --json | jq -r '.activityid')\nibmcloud schematics logs --id $ID --act-id $act_ID\n```\n\nFinally, to delete the Workspace, execute these commands:\n\n```bash\nibmcloud schematics workspace delete --id $ID --force\nibmcloud schematics workspace list\n```\n\n## Compute Resources & Data Source Reference\n\nThe following Terraform Resources or Data Source are used to handle compute resources. Most of them are covered in this section, for those that were not, the links include a description, examples, the input and output parameters. There are also other useful links related to the resources that you may find useful.\n\n<AnchorLinks small>\n  <AnchorLink>ibm_is_ssh_key</AnchorLink>\n  <AnchorLink>ibm_is_instance</AnchorLink>\n  <AnchorLink>ibm_is_lb</AnchorLink>\n  <AnchorLink>ibm_is_lb_listener</AnchorLink>\n  <AnchorLink>ibm_is_lb_listener_policy</AnchorLink>\n  <AnchorLink>ibm_is_lb_listener_policy_rule</AnchorLink>\n  <AnchorLink>ibm_is_lb_pool</AnchorLink>\n  <AnchorLink>ibm_is_lb_pool_member</AnchorLink>\n  <AnchorLink>ibm_is_volume</AnchorLink>\n  <AnchorLink>ibm_is_images</AnchorLink>\n  <AnchorLink>ibm_is_instance_profile</AnchorLink>\n  <AnchorLink>ibm_is_instance_profiles</AnchorLink>\n  <AnchorLink>ibm_is_region</AnchorLink>\n</AnchorLinks>\n\n### ibm_is_ssh_key\n\n- [Resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#ssh-key) with examples, input and output parameters.\n- [Data Source documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-data-sources&-access-data-sources#vpc-ssh-key) retrieves non sensitive information from the given ssh key name\n- Terraform [input variables](https://www.terraform.io/docs/configuration/variables.html) are required to load the content of the SSH public key, using filesystem functions such as [file](https://www.terraform.io/docs/configuration/functions/file.html) and Terraform data source like [local_file](https://www.terraform.io/docs/providers/local/d/file.html).\n- Command [`ssh-keygen`](https://www.ssh.com/ssh/keygen/) is required to generate the SSH key pair files.\n\n### ibm_is_instance\n\n- [Resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#provider-instance) with examples, input and output parameters.\n\n### ibm_is_lb\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb)) with examples, input and output parameters.\n\n### ibm_is_lb_listener\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb-listener)) with examples, input and output parameters.\n\n### ibm_is_lb_listener_policy\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb-listener-policy)) with examples, input and output parameters.\n\n### ibm_is_lb_listener_policy_rule\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb-listener-policy-rule)) with examples, input and output parameters.\n\n### ibm_is_lb_pool\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb-pool)) with examples, input and output parameters.\n\n### ibm_is_lb_pool_member\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb-pool-member)) with examples, input and output parameters.\n\n### ibm_is_volume\n\n- [Resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#volume) with examples, input and output parameters.\n\n### ibm_cos_bucket\n\n- [Resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-object-storage-resources#cos-bucket) with examples, input and output parameters.\n- [Data Source documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-object-storage-data-sources#cos-bucket) retrieves information about the given bucket\n\n### ibm_is_images\n\n- [Data Source documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-data-sources&-access-data-sources#vpc-image) retrieves all the compute images\n\n### ibm_is_instance_profile\n\n- [Data Source documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-data-sources&-access-data-sources#vpc-instance-profile)) retrieves an instance profile from a given name.\n\n### ibm_is_instance_profiles\n\n- [Data Source documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-data-sources&-access-data-sources#vpc-instance-profiles)) retrieves all the instance profiles\n\n### ibm_is_region\n\n- [Data Source documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-data-sources&-access-data-sources#vpc-region)) retrieves all the regions.\n","type":"Mdx","contentDigest":"d91c33c8e9d3fc1b345482d07a34ebd5","counter":589,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"IaC for Compute & Storage Resources","description":"Use IaC to automate compute resources, images and instances","keywords":"terraform,ibm cloud,compute,instances,images"},"exports":{},"rawBody":"---\ntitle: IaC for Compute & Storage Resources\ndescription: Use IaC to automate compute resources, images and instances\nkeywords: 'terraform,ibm cloud,compute,instances,images'\n---\n\n<!--\n\nThe pattern to document the resources is like follow:\n- Introduce the resource with an example\n- List all or the most important input parameters\n- If will be used, list the most important output parameters\n- Provide instructions to get the value of the input parameters, either using `ibmcloud`, API or the Web console.\n- If needed, instructions to execute the code either with Terraform or Schematics\n\n-->\n\n<!--\nTODO: Is is possible to get the image and profile from ibm_is_images and ibm_is_instance_profiles ??\nTODO: The code is failing, there is no access to the VSIs, could be the Health Check\nTODO: Move all the Hard Coded code to variables, document how to get the values\nTODO: Update the final code\n-->\n\n<PageDescription>\n\nAutomating compute resources with Terraform and Schematics. Compute images and instances life cycle\n\n</PageDescription>\n\nOne of the basic and atomic services every cloud provide is compute. Compute on IBM Cloud covers different services however this pattern guide is about the IBM Cloud Virtual Servers, also called Virtual Server Instances or VSI, how to customize public and private virtual servers that scale up or down to suit your needs, using Terraform and IBM Cloud Schematics. Other IBM Cloud Compute services such as Kubernetes, OpenShift, Container Registry and Cloud Functions are covered in other pattern guides.\n\nBased on the [Network Resources](/iac-resources/network) pattern we would like to add a few compute resources and storage to host and expose an API with movies.\n\nThe following diagram shows the proposed architecture.\n\n![architecture](./images/IaC-Compute_Resources.png \"Compute Resources Architecture\")\n\nThe code to build these resources can be downloaded from the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [07-compute](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/07-compute).\n\n<AnchorLinks small>\n  <AnchorLink>Virtual Server Instance</AnchorLink>\n  <AnchorLink>Virtual Server Instance with IBM Cloud Schematics</AnchorLink>\n  <AnchorLink>Load Balance a Cluster of VSI</AnchorLink>\n  <AnchorLink>IBM Cloud Object Storage</AnchorLink>\n  <AnchorLink>Volumes</AnchorLink>\n  <AnchorLink>Final Terraform code</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n  <AnchorLink>Compute Resources & Data Source Reference</AnchorLink>\n</AnchorLinks>\n\n<InlineNotification>\n\n**Requirements**\n\nTo be able to execute and complete the instructions in this page, make sure you have an **IBM Cloud account**: if you don't have one yet, you can [create a Lite account](https://cloud.ibm.com/docs/overview?topic=overview-quickstart_lite#prereqs-lite) for free.\n\nAlso make sure you have the environment setup as explained in the [Setup Environment](/iac/setup-environment) page to have installed the IBM Cloud CLI, login into your account with the IBM Cloud CLI and create a SSH Key.\n\n</InlineNotification>\n\n## Virtual Server Instance\n\nBefore create a VSI we need to have all the networking resources created. The list of resources includes: VPC, Subnets in one or more zones, Public gateway for public internet communication, ACL's for inbound and outbound traffic to the subnets. All these resources management is covered in the [Networking section](/iac/networking) where the required network resources were created in the `networking.tf` file.\n\nThe following code is an example to create a VSI, for this we use the `ibm_is_instance` resource.\n\n```hcl path=compute.tf\nresource \"ibm_is_ssh_key\" \"iac_app_key\" {\n  name       = \"${var.project_name}-${var.environment}-key\"\n  public_key = var.public_key\n}\n\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  name    = \"${var.project_name}-${var.environment}-instance\"\n  image   = \"r006-14140f94-fcc4-11e9-96e7-a72723715315\"\n  profile = \"cx2-2x4\"\n\n  primary_network_interface {\n    name            = \"eth1\"\n    subnet          = ibm_is_subnet.iac_app_subnet.id\n    security_groups = [ibm_is_security_group.iac_app_security_group.id]\n  }\n\n  vpc  = ibm_is_vpc.iac_app_vpc.id\n  zone = \"us-south-1\"\n  keys = [ibm_is_ssh_key.iac_app_key.id]\n\n  user_data = <<-EOUD\n              #!/bin/bash\n              echo \"Hello World\" > index.html\n              nohup busybox httpd -f -p ${var.port} &\n              EOUD\n\n  tags = [\"iac-${var.project_name}-${var.environment}\"]\n}\n```\n\nThe variables used in this code are defined at `variables.tf` like so:\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"public_key\" {}\n\nvariable \"port\" {\n  default = 8080\n}\n```\n\nThe value of the variables are set in the `terraform.tfvars` and `*.auto.tfvars` files. As we are going to use this code with the Terraform CLI on our local host and with IBM Cloud Schematics we should not use `file(pathexpand(var.public_key_file))` to get the value of a public key file like `~/.ssh/id_rsa.pub` because it doesn't work on IBM Cloud Schematics. Instead let's send the content of the public key file to the variable `public_key` in the `secrets.auto.tfvars` file using the following command.\n\n```bash\necho -n \"public_key = \\\"$(cat ~/.ssh/id_rsa.pub)\\\"\\n\" > secrets.auto.tfvars\n```\n\nIt is **important** you make sure to include the file `secrets.auto.tfvars` to the `.gitignore` file so you don't share your secrets to the world.\n\nTo test this VSI we'll deploy a server to expose an API from a [movies database](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/07-compute/db.json) JSON file. This is a sample of it:\n\n```json\n{\n  \"movies\": [{\n    \"id\": \"83\",\n    \"title\": \"Akira Kurosawa's Dreams\",\n    \"summary\": \"This is essentially eight separate short films, with some overlaps in characters and thematic material - that of man's relationship with his environment. 'Sunshine Through The Rain' - 'The Peach Orchard' - 'The Blizzard' - 'Crows' - 'Mount Fuji in Red' - The Weeping Demon' - 'Village of the Watermills'\",\n    \"year\": \"1990\",\n    \"duration\": \"7173610\",\n    \"originallyAvailableAt\": \"1990-05-11\",\n    \"addedAt\": \"1348382738\",\n    \"updatedAt\": \"1531023643\",\n    \"genre\": \"Sci-Fi & Fantasy\",\n    \"director\": [\"Akira Kurosawa\", \"Ishir√¥ Honda\"],\n    \"writer\": [\"Akira Kurosawa\"],\n    \"cast\": [\"Akira Terao\", \"Mitsuko Baisho\", \"Toshie Negishi\"]\n  }, {\n    ...\n  }]\n}\n```\n\nTo deploy the JSON file to the provisioned VSI the first step is to load the file using the data source [local_file](https://www.terraform.io/docs/providers/local/d/file.html), the file content can be obtained with the attribute `content` or `content_base64` if you require the file content encoded.\n\nIn the `user_data` attribute use the command `echo` with `base64` to print the decoded content of the JSON file that was previously encoded using the `content_base64` attribute of the `local_file` data source. Terraform sends the content of the file to the IBM Cloud engine through HTTP, it is recommended to encode this text otherwise we can get unexpected results. That's why we'll use the attribute `content_base64` instead of `content`, and we use the command `base64`, at the server side, to decode the received text.\n\nModify the value of the `user_data` attribute of `ibm_is_instance.iac_app_instance` and add the `local_file` data source, like this:\n\n```hcl path=compute.tf\ndata \"local_file\" \"db\" {\n  filename = \"${path.module}/db.min.json\"\n}\n\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  ...\n  user_data = <<-EOUD\n            #!/bin/bash\n            echo '${data.local_file.db.content_base64}' | base64 --decode > /var/lib/db.min.json\n\n            echo '* libraries/restart-without-asking boolean true' | debconf-set-selections\n            apt update\n            curl -sL https://deb.nodesource.com/setup_13.x | sudo -E bash -\n            apt-get install -y nodejs\n            npm install -g json-server\n\n            json-server --watch /var/lib/db.min.json --port ${var.port} --host 0.0.0.0 &\n            EOUD\n  ...\n}\n```\n\n<InlineNotification>\n\n**user_data vs provisioners**\n\nShould I use `user_data` or provisioners such as `file` and `remote-exec`?\n\nTerraform is a provisioning tool, it should not do configuration tasks maybe just trigger them. Among the configuration management tools are: Ansible, Cloud-Init, Puppet and (the less recommended) shell scripts. Thus, the appropriate way to configure a server or trigger the configuration should be using `user_data`, it can execute either a Cloud-Init code, a script or to start a configuration management tool like Ansible. The `user_data` is also used by the auto-scaling in IBM Cloud and Terraform provisioners are not.\n\nThe provisioners such as `file` and `remote-exec` are specific to Terraform that's why they are not suggested for configuration management. If you would like to use them, read [Using Terraform Provisioners](/iac/resources/compute/provisioners).\n\n</InlineNotification>\n\nWe are introducing a new Terraform plugin (`local`) with the `local_file` data source, so before apply the infrastructure code and deploy the application we need to download this plugin with the `terraform init` command. Execute the following commands to apply the new code:\n\n```bash\nterraform init\nterraform plan\nterraform apply\n```\n\nIn about a minute the VSI is provisioned but the API server will be running in about an extra minute. To verify it's working get the `public_ip` output variable to fetch some movies, like so:\n\n```bash\ncurl \"$(terraform output entrypoint)/movies\" | jq\ncurl \"$(terraform output entrypoint)/movies/675\"\ncurl \"$(terraform output entrypoint)/movies?id=1067&id=1649\"\n```\n\nIf you do not see any response, please, give it a minute or two, it takes some time to install all the dependencies to run the API server.\n\n## Virtual Server Instance with IBM Cloud Schematics\n\nAs seen in the [IBM Cloud Schematics](/iac/schematics) pattern a similar HCL code can be used with IBM Cloud Schematics and Terraform CLI, the only change is that Schematics cannot access your local filesystem but it can access the files located in the git repository or remote files.\n\nTo create this project on IBM Cloud Schematics create a JSON Workspace template file like so:\n\n```json path=workspace.tmpl.json\n{\n  \"name\": \"iac_schematics_test\",\n  \"type\": [\n    \"terraform_v0.12\"\n  ],\n  \"description\": \"Sample workspace to test IBM Cloud Schematics. Deploys an web server on a VSI with a Hello World response\",\n  \"tags\": [\n    \"app:helloworld\",\n    \"env:dev\"\n  ],\n  \"template_repo\": {\n    \"url\": \"https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/07-compute\"\n  },\n  \"template_data\": [{\n    \"folder\": \".\",\n    \"type\": \"terraform_v0.12\",\n    \"variablestore\": [{\n        \"name\": \"project_name\",\n        \"value\": \"iac_schematics_test\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"environment\",\n        \"value\": \"dev\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"public_key\",\n        \"value\": \"{ PUBLIC_KEY }\",\n        \"type\": \"string\"\n      }\n    ]\n  }]\n}\n```\n\nThen execute the following commands to create the Workspace, provision the VSI and deploy the application.\n\n<!--\n\nIn the current version of the IBM Cloud Schematics plugin there is an error when the terraform_v0.12 is used.\nTo workaround this error export the variable:\n\n  export X_FEATURE_SANDBOX=true\n\nThis error should be fixed in the next release of the plugin\n\n-->\n\n```bash\n# Verify you are logged in to the right account\nibmcloud target\n\n# Get the content of the SSH public key and render it into the JSON file using the template\nPUBLIC_KEY=\"$(cat ~/.ssh/id_rsa.pub)\"\nsed \"s|{ PUBLIC_KEY }|$PUBLIC_KEY|\" workspace.tmpl.json > workspace.json\n\n# Create the Schematics Workspace\nibmcloud schematics workspace new --file workspace.json\n\n# List the Workspaces and get the Workspace ID\nibmcloud schematics workspace list\nID=$(ibmcloud schematics workspace list --json | jq -r '.workspaces[] | select(.name == \"iac_schematics_test\") | .id')\n\n# Planning and Provisioning\nact_ID=$(ibmcloud schematics plan --id $ID --json | jq -r '.activityid')\nibmcloud schematics logs --id $ID --act-id $act_ID\n\nact_ID=$(ibmcloud schematics apply --id $ID --force --json | jq -r '.activityid')\nibmcloud schematics logs --id $ID --act-id $act_ID\n```\n\nYou may have to wait 2-3 seconds between the execution of an action (i.e. `apply`) and the retrieval of the logs. Also, you may have to execute the retrieval of the `apply` logs several times until the task is completed.\n\nIf the Workspace JSON file is modified, for example a variable value, you can update it with the command `ibmcloud schematics workspace update`.\n\nTo verify the application is running, get the output variables and use `curl` to get the page:\n\n```bash\nibmcloud schematics workspace output --id $ID --json\nIP=$(ibmcloud schematics workspace output --id $ID --json | jq -r '.[].output_values[].ip_address.value')\nADDR=$(ibmcloud schematics workspace output --id $ID --json | jq -r '.[].output_values[].entrypoint.value')\n\ncurl \"${ADDR}/movies/675\"\n```\n\n<InlineNotification kind=\"warning\">\n\n**Sensitive data in JSON file**\n\nThe file `workspace.json` contain your SSH public key, make sure this file is not committed to the Git repository. Add the `workspace.json` filename to the `.gitignore` file.\n\n</InlineNotification>\n\n## Identify Input Parameters Values\n\nIn the Getting Started with Terraform section was explained how to get the value of the input parameters such as the instance `image` and `profile` using the IBM Cloud CLI and some Unix commands.\n\nIt's also possible to get these parameters using the Data Sources `ibm_is_instance_profile` and `ibm_is_image`. Modify the `compute.tf` file to add the following data sources.\n\n```hcl path=compute.tf\ndata \"ibm_is_image\" \"ds_iac_app_image\" {\n  name = \"ibm-ubuntu-18-04-1-minimal-amd64-1\"\n}\n\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  ...\n  image   = data.ibm_is_image.ds_iac_app_image.id\n  ...\n}\n```\n\nThis change is not much but give more information to the developer reading the code if the the image name is used instead of the ID. You still need to execute the following IBM Cloud CLI commands to find the image and profile name, like so:\n\n```bash\nibmcloud is images\nibmcloud is images | grep available | grep ubuntu-18 | grep amd64 | cut -f2 -d\" \"\nibmcloud is images --json | jq -r '.[] | select(.status==\"available\" and .operating_system.name==\"ubuntu-18-04-amd64\").name'\n\nibmcloud is instance-profiles\nibmcloud is instance-profiles | grep amd64 | sort -k4 -k5 -n | head -1 | cut -f1 -d\" \"\nibmcloud is instance-profiles --json | jq -r 'map(select(.vcpu_architecture.value==\"amd64\")) | sort_by(.memory.value)[0].name'\n```\n\n<!-- TODO: Is is possible to get the image and profile from ibm_is_images and ibm_is_instance_profiles ?? -->\n\n## Load Balance a Cluster of VSI\n\nRunning a single server serve the purpose but it's a single point of failure, if this single server fails the API is not accessible. The solution is to have a cluster of Virtual Servers, routing the traffic to the servers that are working and scaling the size up or down based on the traffic load. IBM Cloud allow us to get this features with the Load Balancer service and Terraform resources.\n\nYou can read more about the type of Load Balancers, Listeners, Pools, LB Methods and more in the [Load Balancer documentation](https://cloud.ibm.com/docs/vpc?topic=vpc-load-balancers) for IBM CLoud Gen 2.\n\nWe'll begin the Load Balancer code by creating a `ibm_is_lb` resource in the `lb.tf` file, like so:\n\n```hcl path=lb.tf\nresource \"ibm_is_lb\" \"iac_app_lb\" {\n  name    = \"${var.project_name}-${var.environment}-lb\"\n  subnets = [ibm_is_subnet.iac_app_subnet.id]\n}\n```\n\nIt requires an ID reference of the subnets where the LB is located. The VSI that are served by this Load Balancer should be in the same VPC and region. This specific LB name is formed by the project name and environment to not collide with other Load Balancers. Other important parameter is `type` to define if the LB will be `public` (default) or `private`, this would be public so the fully qualified domain name (FQDN) will be accessible from the internet and have assigned multiple public IP addresses.\n\nFor private load balancers the access is restricted to internal clients on the same subnet, region and VPC. It also has assigned a FQDN with multiple IP addresses and only accepts traffic from [RFC1918](https://tools.ietf.org/html/rfc1918) address spaces such as those in the blocks 10.0.0.0/8, 172.16.0.0/12 and 192.168.0.0/16.\n\nThe Load Balancer needs a listener to listen on a given port and protocol. The FQDN and port assigned to the load balancer are exposed to the public internet. The traffic is then redirected to a pool member or VSI from the assigned default pool. The API on the VSI is listening/serving on port `8080` by default, unless a different port is set in the `port` variable. We can make the LB listener to use the same port or different, in this case, let's use the same.\n\nThe available protocols on the LB listener are **HTTP**, **HTTPS** and **TCP**, this one uses the HTTP protocol. The pool or VSI protocols supported are only HTTP and TCP, in this case it's HTTP. Let's append the following code to `lb.tf` to define the LB listener:\n\n```hcl path=lb.tf\nresource \"ibm_is_lb_listener\" \"iac_app_lb_listener\" {\n  lb       = ibm_is_lb.iac_app_lb.id\n  port     = var.port\n  protocol = \"http\"\n  default_pool = ibm_is_lb_pool.iac_app_lb_pool.id\n}\n```\n\nThe `ibm_is_lb` ID, `port` and `protocol` are required parameters. Other optional parameters are `default_pool`, `certificate_instance` and `connection_limit`, the last two are not needed for this project.\n\nOn the other side of the Load Balancer are the VSI or backend application which are identified as a Pool Member (`ibm_is_lb_pool_member`) that is, obviously, a member of the defined pool (`ibm_is_lb_pool`). Let's begin modifying the VSI `ibm_is_instance.iac_app_instance` resource in the `compute.tf` file to have multiple instances adding the Terraform `count` parameter and modifying the `name` parameter to include the number of that instance. Like so:\n\n```hcl path=compute.tf\n...\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  name    = \"${var.project_name}-${var.environment}-instance-${format(\"%02s\", count.index)}\"\n  ...\n  count   = var.max_size\n  ...\n}\n```\n\nThe new variable `max_size` defines how many VSI, therefore how many pool members too, will be created. This variable is defined like this in the `variables.tf` file:\n\n```hcl\nvariable \"max_size\" {\n  default = 3\n}\n```\n\nAll is set to create the Pool and the Pool Members, add the following code to the `lb.tf` file:\n\n```hcl path=lb.tf\nresource \"ibm_is_lb_pool\" \"iac_app_lb_pool\" {\n  name                = \"${var.project_name}-${var.environment}-lb-pool\"\n  lb                  = ibm_is_lb.iac_app_lb.id\n  algorithm           = \"round_robin\"\n  protocol            = \"http\"\n  health_delay        = 5\n  health_retries      = 2\n  health_timeout      = 2\n  health_type         = \"http\"\n  health_monitor_url  = \"/\"\n  health_monitor_port = var.port\n}\n\nresource \"ibm_is_lb_pool_member\" \"iac_app_lb_pool_mem\" {\n  count          = var.max_size\n  lb             = ibm_is_lb.iac_app_lb.id\n  pool           = ibm_is_lb_pool.iac_app_lb_pool.id\n  port           = var.port\n  target_address = ibm_is_instance.iac_app_instance[count.index].primary_network_interface.0.primary_ipv4_address\n  weight         = (100 - count.index)\n}\n```\n\nThe `ibm_is_lb_pool.iac_app_lb_pool` resource requires the following input attributes:\n\n| Input parameter | Description |\n|---|---|\n| `name` | name of the pool. In this case it has the name of the project and environment to not collide with other pools  |\n| `lb` | ID of the load balancer is linked to |\n| `algorithm` | load balancing algorithm. Supported values are `round_robin`, `weighted_round_robin`, or `least_connections` |\n| `protocol` | pool protocol. Supported values are `http`, and `tcp` |\n| `health_delay` | health check interval in seconds. Interval must be greater than timeout value |\n| `health_retries` | health check max retries |\n| `health_timeout` | health check timeout in seconds |\n| `health_type` | pool protocol. Supported values are `http`, and `tcp` |\n\nOther input and output parameters are described in the `ibm_is_lb_pool` [resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources#lb-pool). The 3 different load balancing methods to set in the `algorithm` input parameter are described in the [Load Balancers documentation](https://cloud.ibm.com/docs/vpc?topic=vpc-load-balancers#load-balancing-methods).\n\nThe `ibm_is_lb_pool_member.iac_app_lb_pool_mem` is actually a list of resources because it also has the Terraform `count` attribute, just like the `ibm_is_instance` resource and this is because we need one pool member per VSI.\n\nThe `ibm_is_lb_pool_member` uses the `target_address` attribute to link the pool member to the VSI using its IP address. Here we use the `count.index` to reference to the VSI with the same index, so the pool member 0 is linked to the VSI 0, and so on. As we did to link the Floating IP to the IP address of the VSI, this IP address is identified by the `ibm_is_instance.iac_app_instance` resource output parameter `primary_network_interface.0.primary_ipv4_address`.\n\nThe required input attributes for `ibm_is_lb_pool_member` are:\n\n| Input parameter | Description |\n|---|---|\n| `pool` | ID of the load balancer pool |\n| `lb` | load balancer ID |\n| `port` | port number of the application running in the server member, in this case is in the variable `port` |\n| `target_address` | IP address of the pool member or VSI |\n| `weight` | weight of the server member. This parameter is optional and it takes effect only when the load balancing algorithm of its belonging pool is `weighted_round_robin` |\n\nMore information about this resource can be found at the `ibm_is_lb_pool_member` [resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources#lb-pool-member).\n\nThe load balancer adjusts its capacity automatically according to the load. When this adjustment occurs, you may see a change in the number of IP addresses associated with the load balancer's DNS name. To know the load balancer's DNS name and the associated IP addresses we use the following output variables to be included in the file `output.tf`\n\n```hcl path=output.tf\noutput \"lb_ip_address\" {\n  value = ibm_is_lb.iac_app_lb.public_ips\n}\n\noutput \"entrypoint\" {\n  value = \"http://${ibm_is_lb.iac_app_lb.hostname}:${var.port}\"\n}\n```\n\nThe new value of `entrypoint` has now the hostname or FQDN of the load balancer.\n\nHaving a load balancer there is no need to have floating IP assigned to every VSI so you can remove them but in case you want to keep them you need one per VSI, so you have to do something similar done with the pool members modifying the `ibm_is_floating_ip.iac_app_floating_ip` and the `ip_address` output variable that uses this resource, like so:\n\n```hcl network.tf\nresource \"ibm_is_floating_ip\" \"iac_app_floating_ip\" {\n  name   = \"${var.project_name}-${var.environment}-ip\"\n  target = ibm_is_instance.iac_app_instance[count.index].primary_network_interface.0.id\n  count  = var.max_size\n}\n```\n\n```hcl\noutput \"ip_address\" {\n  value = ibm_is_floating_ip.iac_app_floating_ip[*].address\n}\n```\n\nHere we use `[*]` to let Terraform knows that we need the `address` of all the `iac_app_floating_ip` resources. This variable would be a list just like `lb_ip_address`.\n\nBut, again, there is no need to have Floating IP's assigned per VSI, the Load Balancer provide the FQDN and an IP per pool member or VSI. Remove the Floating IP's once you verify the Load Balancer works.\n\n## Health Checks\n\nHealth check definitions are mandatory for back-end pools. Not having health checks cause the pool identify the pool members as unhealthy therefore not forwarding new connections to them.\n\nThe health check is configured in the `ibm_is_lb_pool` resource using the `health_*` attributes. Read the [Health Check documentation](https://cloud.ibm.com/docs/vpc?topic=vpc-load-balancers#health-checks) and the `ibm_is_lb_pool` [input attributes](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources#lb-pool-input) that can be used for health check.\n\nHealth checks can be configured on back-end ports or on a separate health check port, based on the application. For this application, the API is being served with HTTP protocol, so we ser the input parameter `health_type` of `ibm_is_lb_pool` to `http`. The port to monitor is the same port where the API is exposed so the `health_monitor_port` parameter is set to the value we have on the variable `port` but if not set, the port used in the pool member will be used. Finally, the URL to monitor is the path to be used for the health check, the default value is `/` but we may have something like `/health`. If the HTTP response code is `200` then the pool member is considered healthy.\n\n## IBM Cloud Object Storage\n\nThere are different type of storage objects in cloud environment and you use different types for different situations Review the [IBM Cloud Object Storage](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-about-cloud-object-storage) resources to know how to use them and which storage object to use.\n\nWe use the resource `ibm_cos_bucket` to create an Object Storage bucket to store data but it requires a `ibm_resource_instance` to be created in advance. There are 5 different storage classes to choose:\n\n- **Smart Tier** (`smart`) can be used for any workload, especially dynamic workloads where access patterns are unknown or difficult to predict. Smart Tier provides a simplified pricing structure and automatic cost optimization by classifying the data into \"hot\", \"cool\", and \"cold\" tiers based on monthly usage patterns. All data in the bucket is then billed at the lowest applicable rate. There are no threshold object sizes or storage periods, and there are no retrieval fees.\n- **Standard** (`standard`) is used for active workloads, with no charge for data retrieved (other than the cost of the operational request itself).\n- **Vault** (`vault`) is used for cool workloads where data is accessed less than once a month - an extra retrieval charge ($/GB) is applied each time data is read. The service includes a minimum threshold for object size and storage period consistent with the intended use of this service for cooler, less-active data.\n- **Cold Vault** (`cold`) is used for cold workloads where data is accessed every 90 days or less - a larger extra retrieval charge ($/GB) is applied each time data is read. The service includes a longer minimum threshold for object size and storage period consistent with the intended use of this service for cold, inactive data.\n- **Flex** (`flex`) is being replaced by Smart Tier for dynamic workloads.\n\nIn our demo application we use this bucket to store images with the movie covers and they are actively used, so the object class to choose is `standard`. For more information about storage classes, see [Use storage classes](https://cloud.ibm.com/docs/services/cloud-object-storage?topic=cloud-object-storage-classes).\n\nLets create the storage with the following code:\n\n```hcl\nresource \"ibm_resource_instance\" \"iac_app_cos_instance\" {\n  name     = \"${var.project_name}-${var.environment}-cos-instance\"\n  service  = \"cloud-object-storage\"\n  plan     = \"standard\"\n  location = \"global\"\n}\n\nresource \"ibm_cos_bucket\" \"iac_app_cos_bucket\" {\n  bucket_name          = \"${var.project_name}-${var.environment}-bucket\"\n  resource_instance_id = ibm_resource_instance.iac_app_cos_instance.id\n  region_location      = \"us-south\"\n  storage_class        = \"smart\"\n}\n```\n\nThe `ibm_cos_bucket` resource requires the following input parameters:\n\n| Input parameter | Description |\n|---|---|\n| `bucket_name` | to name the bucket |\n| `resource_instance_id` | ID of the `ibm_resource_instance` service instance for which you want to create a bucket |\n| `storage_class` | The storage class that you want to use for the bucket |\n| `region_location` | location of a regional bucket. Do not use this parameter with other `*_location` parameter |\n| `single_site_location` | location for a single site bucket. Do not use this parameter with other `*_location` parameter |\n| `cross_region_location` | location for a cross-regional bucket. Do not use this parameter with other `*_location` parameter |\n\nFor more information about other optional input parameters and the output parameters read the `ibm_cos_bucket` [IBM Cloud Object Storage resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-object-storage-resources#cos-bucket).\n\nThe upload of data to the bucket is not done with Terraform nor Schematics, but can be done using code in different languages such as [Go](https://cloud.ibm.com/docs/services/cloud-object-storage/libraries?topic=cloud-object-storage-go), [Python](https://cloud.ibm.com/docs/services/cloud-object-storage/libraries?topic=cloud-object-storage-python), [Node](https://cloud.ibm.com/docs/services/cloud-object-storage/libraries?topic=cloud-object-storage-node) or [Java](https://cloud.ibm.com/docs/services/cloud-object-storage/libraries?topic=cloud-object-storage-java) using the SDK, or using the Linux commands (i.e. `curl`) with the [Cloud Storage API](https://cloud.ibm.com/docs/services/cloud-object-storage/api-reference?topic=cloud-object-storage-compatibility-api), or using any file transfer tools such as [Cyberduck](https://cyberduck.io/) or [Transmit](https://panic.com/transmit/) and command-line utilities like [s3cmd](https://github.com/s3tools/s3cmd) or [Minio Client](https://github.com/minio/mc), and many others.\n\n## Block Storage for Volumes\n\nBesides IBM Cloud Storage Object we can attach volumes to the instance to store data or files to access though file system interface, file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage.\n\nBlock Storage are block-level volumes that can be attached to VSI as either a boot volume or as a data volume. The boot volumes are attached by default. The [Block Storage for VPC](https://cloud.ibm.com/docs/vpc?topic=vpc-block-storage-about) documentation can give you more information.\n\nTo create a data volume per instance we use the `ibm_is_volume` resource with a code like this one.\n\n```hcl path=storage.tf\nresource \"ibm_is_volume\" \"iac_app_volume\" {\n  count    = var.max_size\n  name     = \"${var.project_name}-${var.environment}-volume-${format(\"%02s\", count.index)}\"\n  profile  = \"10iops-tier\"\n  zone     = \"us-south-1\"\n  capacity = 100\n}\n```\n\nThe main input parameters for `ibm_is_volume` are described in the following table.\n\n| Input parameter | Description |\n|---|---|\n| `name` | to name the volume |\n| `profile` | volume profile |\n| `zone` | location of the volume |\n| `capacity` | capacity of the volume in gigabytes. the default value is `100` |\n| `iops` | total input/ output operations per second (IOPS) for your storage. This value is required for custom storage profiles only |\n\nThe [Block Storage Capacity and Performance](https://cloud.ibm.com/docs/vpc?topic=vpc-capacity-performance) documentation give you information about the available capacities and the performance (IOPS and Throughput) per capacity.\n\nTo get a list of volume profiles with the CLI use the following command, also the [Profiles](https://cloud.ibm.com/docs/vpc?topic=vpc-block-storage-profiles) documentation list the available profiles and explain how to define a custom one.\n\n```bash\nibmcloud is volume-profiles\n```\n\nHaving the Block Storage is not enough, you need to assign the volume to the VSI using the `volumes` list attribute of the `ibm_is_instance` resource, like so:\n\n```hcl path=compute.tf\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  ...\n  volumes = [ibm_is_volume.iac_app_volume[count.index].id]\n  ...\n}\n```\n\nNotice the use of `count.index` to get one volume id. This way we ensure the volume `0` is assigned to the instance `0` and so on. The volume is mounted in the root partition, so there is no need to modify the user data to change the location of the JSON DB file.\n\nVerify and apply all the changes executing the commands:\n\n```bash\nterraform plan\nterraform apply\n```\n\nTo test the API works execute the following command to get the Load Balancer FQDN or entrypoint to be used by the `curl` command:\n\n```bash\ncurl $(terraform output entrypoint)/movies/675\n```\n\n## Final Terraform code\n\nYou can download the code from the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [07-compute](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/07-compute) where the main files are:\n\n```hcl path=compute.tf\nresource \"ibm_is_ssh_key\" \"iac_app_key\" {\n  name       = \"${var.project_name}-${var.environment}-key\"\n  public_key = var.public_key\n}\n\ndata \"local_file\" \"db\" {\n  filename = \"${path.module}/db.min.json\"\n}\n\ndata \"ibm_is_image\" \"ds_iac_app_image\" {\n  name = \"ibm-ubuntu-18-04-1-minimal-amd64-1\"\n}\n\nresource \"ibm_is_instance\" \"iac_app_instance\" {\n  count   = var.max_size\n  name    = \"${var.project_name}-${var.environment}-instance-${format(\"%02s\", count.index)}\"\n  image   = data.ibm_is_image.ds_iac_app_image.id\n  profile = \"cx2-2x4\"\n\n  primary_network_interface {\n    name            = \"eth1\"\n    subnet          = ibm_is_subnet.iac_app_subnet.id\n    security_groups = [ibm_is_security_group.iac_app_security_group.id]\n  }\n\n  vpc     = ibm_is_vpc.iac_app_vpc.id\n  zone    = \"us-south-1\"\n  keys    = [ibm_is_ssh_key.iac_app_key.id]\n  volumes = [ibm_is_volume.iac_app_volume[count.index].id]\n\n  user_data = <<-EOUD\n            #!/bin/bash\n            echo '${data.local_file.db.content_base64}' | base64 --decode > /var/lib/db.min.json\n\n            echo '* libraries/restart-without-asking boolean true' | debconf-set-selections\n\n            apt update\n            curl -sL https://deb.nodesource.com/setup_13.x | sudo -E bash -\n            apt-get install -y nodejs\n            npm install -g json-server\n\n            json-server --watch /var/lib/db.min.json --port ${var.port} --host 0.0.0.0 &\n            EOUD\n\n  tags = [\"iac-${var.project_name}-${var.environment}\"]\n}\n```\n\n```hcl path=lb.tf\nresource \"ibm_is_lb\" \"iac_app_lb\" {\n  name    = \"${var.project_name}-${var.environment}-lb\"\n  subnets = [ibm_is_subnet.iac_app_subnet.id]\n}\n\nresource \"ibm_is_lb_listener\" \"iac_app_lb_listener\" {\n  lb       = ibm_is_lb.iac_app_lb.id\n  port     = var.port\n  protocol = \"http\"\n  default_pool = ibm_is_lb_pool.iac_app_lb_pool.id\n}\n\nresource \"ibm_is_lb_pool\" \"iac_app_lb_pool\" {\n  name                = \"${var.project_name}-${var.environment}-lb-pool\"\n  lb                  = ibm_is_lb.iac_app_lb.id\n  algorithm           = \"round_robin\"\n  protocol            = \"http\"\n  health_delay        = 5\n  health_retries      = 2\n  health_timeout      = 2\n  health_type         = \"http\"\n  health_monitor_url  = \"/\"\n  health_monitor_port = var.port\n}\n\nresource \"ibm_is_lb_pool_member\" \"iac_app_lb_pool_mem\" {\n  count          = var.max_size\n  lb             = ibm_is_lb.iac_app_lb.id\n  pool           = ibm_is_lb_pool.iac_app_lb_pool.id\n  port           = var.port\n  target_address = ibm_is_instance.iac_app_instance[count.index].primary_network_interface.0.primary_ipv4_address\n  weight         = (100 - count.index)\n}\n```\n\n```hcl path=storage\nresource \"ibm_resource_instance\" \"iac_app_cos_instance\" {\n  name     = \"${var.project_name}-${var.environment}-cos-instance\"\n  service  = \"cloud-object-storage\"\n  plan     = \"standard\"\n  location = \"global\"\n}\n\nresource \"ibm_cos_bucket\" \"iac_app_cos_bucket\" {\n  bucket_name          = \"${var.project_name}-${var.environment}-bucket\"\n  resource_instance_id = ibm_resource_instance.iac_app_cos_instance.id\n  region_location      = \"us-south\"\n  storage_class        = \"smart\"\n}\n\nresource \"ibm_is_volume\" \"iac_app_volume\" {\n  count    = var.max_size\n  name     = \"${var.project_name}-${var.environment}-volume-${format(\"%02s\", count.index)}\"\n  profile  = \"10iops-tier\"\n  zone     = \"us-south-1\"\n  capacity = 100\n}\n```\n\n```hcl path=output.tf\noutput \"lb_ip_address\" {\n  value = ibm_is_lb.iac_app_lb.public_ips\n}\n\noutput \"entrypoint\" {\n  value = \"http://${ibm_is_lb.iac_app_lb.hostname}:${var.port}\"\n}\n```\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"public_key\" {}\n\nvariable \"port\" {\n  default = 8080\n}\nvariable \"max_size\" {\n  default = 3\n}\n```\n\n```json path=workspace.tmpl.json\n{\n  \"name\": \"iac_schematics_test\",\n  \"type\": [\n    \"terraform_v0.12\"\n  ],\n  \"description\": \"Sample workspace to test IBM Cloud Schematics. Deploys an web server on a VSI with a Hello World response\",\n  \"tags\": [\n    \"app:helloworld\",\n    \"env:dev\"\n  ],\n  \"template_repo\": {\n    \"url\": \"https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/07-compute\"\n  },\n  \"template_data\": [{\n    \"folder\": \".\",\n    \"type\": \"terraform_v0.12\",\n    \"variablestore\": [{\n        \"name\": \"project_name\",\n        \"value\": \"iac_schematics_test\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"environment\",\n        \"value\": \"dev\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"public_key\",\n        \"value\": \"{ PUBLIC_KEY }\",\n        \"type\": \"string\"\n      }\n    ]\n  }]\n}\n```\n\n## Clean up\n\nIn this section you have created the same infrastructure using Terraform CLI and IBM Cloud Schematics.\n\nTo destroy everything created with Terraform CLI, execute:\n\n```bash\nterraform destroy\n```\n\nTo delete everything you've created with IBM Cloud Schematics, execute the following command to destroy the infrastructure:\n\n```bash\nID=$(ibmcloud schematics workspace list --json | jq -r '.workspaces[] | select(.name == \"iac_schematics_test\") | .id')\nibmcloud schematics destroy --id $ID\n\n# Or:\nact_ID=$(ibmcloud schematics destroy --id $ID --force --json | jq -r '.activityid')\nibmcloud schematics logs --id $ID --act-id $act_ID\n```\n\nFinally, to delete the Workspace, execute these commands:\n\n```bash\nibmcloud schematics workspace delete --id $ID --force\nibmcloud schematics workspace list\n```\n\n## Compute Resources & Data Source Reference\n\nThe following Terraform Resources or Data Source are used to handle compute resources. Most of them are covered in this section, for those that were not, the links include a description, examples, the input and output parameters. There are also other useful links related to the resources that you may find useful.\n\n<AnchorLinks small>\n  <AnchorLink>ibm_is_ssh_key</AnchorLink>\n  <AnchorLink>ibm_is_instance</AnchorLink>\n  <AnchorLink>ibm_is_lb</AnchorLink>\n  <AnchorLink>ibm_is_lb_listener</AnchorLink>\n  <AnchorLink>ibm_is_lb_listener_policy</AnchorLink>\n  <AnchorLink>ibm_is_lb_listener_policy_rule</AnchorLink>\n  <AnchorLink>ibm_is_lb_pool</AnchorLink>\n  <AnchorLink>ibm_is_lb_pool_member</AnchorLink>\n  <AnchorLink>ibm_is_volume</AnchorLink>\n  <AnchorLink>ibm_is_images</AnchorLink>\n  <AnchorLink>ibm_is_instance_profile</AnchorLink>\n  <AnchorLink>ibm_is_instance_profiles</AnchorLink>\n  <AnchorLink>ibm_is_region</AnchorLink>\n</AnchorLinks>\n\n### ibm_is_ssh_key\n\n- [Resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#ssh-key) with examples, input and output parameters.\n- [Data Source documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-data-sources&-access-data-sources#vpc-ssh-key) retrieves non sensitive information from the given ssh key name\n- Terraform [input variables](https://www.terraform.io/docs/configuration/variables.html) are required to load the content of the SSH public key, using filesystem functions such as [file](https://www.terraform.io/docs/configuration/functions/file.html) and Terraform data source like [local_file](https://www.terraform.io/docs/providers/local/d/file.html).\n- Command [`ssh-keygen`](https://www.ssh.com/ssh/keygen/) is required to generate the SSH key pair files.\n\n### ibm_is_instance\n\n- [Resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#provider-instance) with examples, input and output parameters.\n\n### ibm_is_lb\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb)) with examples, input and output parameters.\n\n### ibm_is_lb_listener\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb-listener)) with examples, input and output parameters.\n\n### ibm_is_lb_listener_policy\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb-listener-policy)) with examples, input and output parameters.\n\n### ibm_is_lb_listener_policy_rule\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb-listener-policy-rule)) with examples, input and output parameters.\n\n### ibm_is_lb_pool\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb-pool)) with examples, input and output parameters.\n\n### ibm_is_lb_pool_member\n\n- [Resource documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#lb-pool-member)) with examples, input and output parameters.\n\n### ibm_is_volume\n\n- [Resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-resources&-access-data-sources#volume) with examples, input and output parameters.\n\n### ibm_cos_bucket\n\n- [Resource documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-object-storage-resources#cos-bucket) with examples, input and output parameters.\n- [Data Source documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-object-storage-data-sources#cos-bucket) retrieves information about the given bucket\n\n### ibm_is_images\n\n- [Data Source documentation](https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-data-sources&-access-data-sources#vpc-image) retrieves all the compute images\n\n### ibm_is_instance_profile\n\n- [Data Source documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-data-sources&-access-data-sources#vpc-instance-profile)) retrieves an instance profile from a given name.\n\n### ibm_is_instance_profiles\n\n- [Data Source documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-data-sources&-access-data-sources#vpc-instance-profiles)) retrieves all the instance profiles\n\n### ibm_is_region\n\n- [Data Source documentation]((https://cloud.ibm.com/docs/terraform?topic=terraform-vpc-gen2-data-sources&-access-data-sources#vpc-region)) retrieves all the regions.\n","fileAbsolutePath":"/Users/johandry/Workspace/ibm/att-cloudnative/ibmcloud-pattern-guide/src/pages/iac-resources/compute/index.mdx"}}}}