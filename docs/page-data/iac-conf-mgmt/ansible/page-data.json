{"componentChunkName":"component---src-pages-iac-conf-mgmt-ansible-index-mdx","path":"/iac-conf-mgmt/ansible/","result":{"pageContext":{"frontmatter":{"title":"IaC Configuration Management with Ansible","description":"Use Ansible to do configuration management of the provisioned instances","keywords":"terraform,ibm cloud,config management,ansible"},"relativePagePath":"/iac-conf-mgmt/ansible/index.mdx","titleType":"page","MdxNode":{"id":"36f40260-66e4-5cd0-8e24-39e7e9040658","children":[],"parent":"0205078e-5b29-559a-9b41-d82e55e59af6","internal":{"content":"---\ntitle: IaC Configuration Management with Ansible\ndescription: Use Ansible to do configuration management of the provisioned instances\nkeywords: \"terraform,ibm cloud,config management,ansible\"\n---\n\n<PageDescription>\n\nConfigure the provisioned instance using Ansible\n\n</PageDescription>\n\n<AnchorLinks small>\n  <AnchorLink>Ansible Introduction</AnchorLink>\n  <AnchorLink>Ansible and Terraform Integration</AnchorLink>\n  <AnchorLink>Final Terraform and Ansible Code</AnchorLink>\n  <AnchorLink>Ansible from a Bastion Host</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\nTerraform is great for infrastructure provisioning but it's not a configuration management tool. That's where Ansible comes in. After Terraform finishes the provisioning of an instance, control goes to Ansible to finish up the configuration of the instance system and applications.\n\n<InlineNotification>\n\n**Other Configuration Management Tools**\n\nAnsible is a Configuration Management Tool but is not the only one. There are others such as [Puppet](https://puppet.com), [Chef](https://www.chef.io) and [SaltStack](https://www.saltstack.com). This pattern explains how to use Ansible for Configuration Management after Terraform finishes the Provisioning and creates the input data for Ansible. In a similar way you can setup the input data for Puppet, Chef, SaltStack or other configuration management tools.\n\n</InlineNotification>\n\n## Ansible Introduction\n\nAnsible is one of the most used Configuration Management tools, it's simple and easy to learn. Ansible use SSH to connect to the servers or instances to execute the configuration tasks defined in YAML files or playbooks. Ansible tasks are idempotent, this means that they can be applied multiple times without changing the result beyond the initial application, this is what makes Ansible reliable and differentiates it from some other configuration management tools.\n\nTo use Ansible we need one host with Ansible, the playbooks and all the Ansible configuration files. There are different designs, the one shown in this example is to use Ansible from the same host where Terraform is executed. The Terraform and Ansible example code is in the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [12-ansible](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/12-ansible).\n\n![Ansible at Local Host](./images/IaC-Ansible_Design_1.png \"Running Ansible from Local Host\")\n\n### Install Ansible\n\nInstall Ansible in the same host where Terraform is installed. Follow these [instructions](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html) to install Ansible on your platform. However, the most generic way to install Ansible is using `pip` (Python package manager).\n\n```bash\npip install --user ansible\n```\n\nIf you are on MacOS, other option to install Ansible is using `brew`, it'll install Ansible and its dependencies.\n\n```bash\nbrew install ansible\n```\n\nTo verify Ansible is correctly installed, just execute `ansible --version` and you should get the latest version, at the time this document is written, it's version `2.9.7`.\n\nThe use case exposed in this section is to install to the LogDNA agent on the instance for log analysis. All the hosts require the LogDNA agent to send the logs to the LogDNA service.\n\n### Infrastructure Setup\n\nClone the example project and go to the `12-user-data/start` folder. You will already need to have set up the terraform cli and the API key for terraform as described in [Setup Environment](/iac/setup-environment). Run these commands to initially deploy the virtual machines for the Ansible examples.\n\n```bash\nterraform init\nterraform plan\nterraform apply\n```\n\n### Inventory File\n\nThe first file used by Ansible is the inventory file with the list of hosts to configure. The [inventory file documentation](https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html) shows all the possible parameters and variables you can set. Create a `inventory.yaml` file in the `12-ansible/start` folder with the content below, but update with the IP addresses output from `terraform apply`.\n\n```yaml path=inventory.yaml\nall:\n  vars:\n    ansible_user: ubuntu\n    ansible_ssh_private_key_file: ~/.ssh/id_rsa\n    ansible_ssh_common_args: \"-o StrictHostKeyChecking=no\"\n  children:\n    appservers:\n      hosts:\n        52.116.136.206:\n        52.116.132.187:\n        52.116.136.220:\n\n```\n\nAs the example progresses, we will add more information into this file such as variables and the hostnames, for now we just need the IP addresses grouped by server tier (`appserver`) and the following ansible variables:\n\n- `ansible_user` defines the user to login into the hosts, in this case is `ubuntu` user\n- `ansible_ssh_private_key_file` path to the private SSH key used to login to the hosts, if you [used the public key](/iac-resources/compute) `~/.ssh/id_rsa.pub` then provide the private key `~/.ssh/id_rsa` pair\n- `ansible_ssh_common_args` this variable set extra parameters to the SSH command used by Ansible to connect to the hosts. With no extra arguments SSH will ask confirmation to the user to add the keys to the `known_hosts` file. Add the parameter `-o StrictHostKeyChecking=no` to make this process non-interactive. This can also be achieved with the setting `host_key_checking = False` into the `ansible.cfg` file in the same directory.\n\nTo verify we can reach the hosts we execute Ansible using the `ping` module.\n\n```bash\nansible all -m ping -i inventory.yaml\n```\n\n### Playbook and Roles\n\nA [Playbook](http://docs.ansible.com/playbooks_intro.html) is yaml files with a set of tasks or roles to be executed. A [Role](https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html) is a collection of related tasks and handlers, to encapsulate data such as variables, files, templates and metadata, to execute one specific macro-task. For example, our (macro) task is to get LogDNA up and running on a given host, to execute this task we need to execute (atomic) tasks to install packages, execute configuration commands and start services, using the given configuration parameters (data in form of variables). We can use any of the open sourced [Ansible Galaxy Roles](https://galaxy.ansible.com) but in this example we'll create a simple role.\n\nStill working in the `start` directory, create the directory path `roles/logdna` and inside the directory a `tasks` sub-directory with the file `main.yaml`. Normal convention with Ansible will often add directories such as `handlers`, `defaults`, `vars`, `files`, `templates` and `meta` but they are not needed and are out of the scope of this article.\n\n```bash\nmkdir -p roles/logdna/tasks\n```\n\nThis example only covers the installation of LogDNA. If you'd like to install or configure something else such as deploing the API application or deploying the initial database, then you would create more roles under the directory `roles` for those tasks.\n\nInside the file `roles/logdna/tasks/main.yaml`\n\n```yaml path=roles/logdna/tasks/main.yaml\n- debug:\n    msg: \"Checking Ansible is working!\"\n```\n\nWe'll develop the tasks role more, but for now let's have a debug message just to verify it's working.\n\nTo tell Ansible which roles to execute, create the following playbook file named `playbook.yaml`.\n\n```yaml path=playbook.yaml\n- hosts: all\n  become: yes\n  roles:\n    - role: logdna\n```\n\nThis playbook file instructs Ansible to execute the instructions on `all` hosts. The instructions should be executed as root, so the `become: yes` is used. And finally, we list all the roles to run on every host. You may have a list of roles per host groups such as `appservers` or `dbservers`. Then execute the playbook with the `ansible-playbook` command, like so.\n\n```bash\nansible-playbook -i inventory.yaml ./playbook.yaml\n```\n\nYou should see, among a bunch of lines, the following lines starting with `ok` and at the end a report summary like the following.\n\n```text\nok: [52.116.136.206] => {\n    \"msg\": \"Checking Ansible is working!\"\n}\nok: [52.116.132.187] => {\n    \"msg\": \"Checking Ansible is working!\"\n}\nok: [52.116.136.220] => {\n    \"msg\": \"Checking Ansible is working!\"\n}\n\n52.116.132.187             : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n52.116.136.206             : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n52.116.136.220             : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n```\n\n### Modules\n\nAnsible uses [modules](http://docs.ansible.com/modules_by_category.html) to perform most tasks. In this example we'll use a few modules to install packages for Ubuntu, executing commands, writing files, and handling the services.\n\n<InlineNotification>\n\n**Running a single module**\n\nIt's possible to just execute a single module or task with the `ansible` command. This is very handy when you need to execute a command for a group of hosts. However, excessive use of this approach is a bad practice, if this command is required frequently then it should be in a role or playbook.\n\nAn example of a single task or module execution is the previous ansible line using `ping` to verify the hosts are accessible. Another example may be using the `apt` module to install a package:\n\n```bash\nansible all -s -m apt -a 'pkg=nginx state=installed update_cache=true'\n```\n\n</InlineNotification>\n\nIn the `logdna/tasks/install.yaml` file use the following modules to install the LogDNA packages.\n\n```yaml path=logdna/tasks/install.yaml\n- name: LogDNA Agent Package Public Key\n  apt_key:\n    url: https://repo.logdna.com/logdna.gpg\n\n- name: LogDNA Agent Repository\n  apt_repository:\n    repo: deb https://repo.logdna.com stable main\n\n- name: Install LogDNA Agent\n  apt:\n    state: present\n    name: logdna-agent\n    allow_unauthenticated: true\n    update_cache: true\n```\n\nEvery task can start with the key `name` to document or describe the task, this text is printed on the console when it's executed. All the modules in this file deal with `apt`, the package manager of Debian and Ubuntu. Here are links to the documentation of each module to know the different parameters that can be used.\n\n- [apt_key](https://docs.ansible.com/ansible/latest/modules/apt_key_module.html) is used to add, remove or download apt keys\n- [apt_repository](https://docs.ansible.com/ansible/latest/modules/apt_repository_module.html) to add or remove APT repositories\n- [apt](https://docs.ansible.com/ansible/latest/modules/apt_module.html) to install, remove, upgrade, update and manage in general APT packages on Debian and Ubuntu\n\nCreate the file `logdna/tasks/configure.yaml` to execute the tasks to configure LogDNA.\n\n```yaml path=logdna/tasks/configure.yaml\n- name: Setting LogDNA Ingestion Key\n  shell: \"logdna-agent -k {{ conf_key }}\"\n  when: conf_key != \"\"\n\n- name: Adding Log Directories\n  shell: \"logdna-agent -d {{ conf_logdir }}\"\n  when: conf_logdir != \"\"\n\n- name: Adding API Host\n  shell: \"logdna-agent -s LOGDNA_APIHOST={{ logdna_api_host }}\"\n  when: logdna_api_host != \"\"\n\n- name: Adding Log Host\n  shell: \"logdna-agent -s LOGDNA_LOGHOST={{ logdna_log_host }}\"\n  when: logdna_log_host != \"\"\n\n- name: Setting Tags for This Host\n  shell: \"logdna-agent -t {{ conf_tags }}\"\n  when: conf_tags != \"\"\n```\n\nIn this file we only use the [shell](https://docs.ansible.com/ansible/latest/modules/shell_module.html) module to execute commands on the host. This module is useful when there is no module to do the tasks we want or need. However, exercise caution with the shell module because the outcome of the execution of these commands may not be idempotent, breaking the main characteristics of Ansible.\n\nHere we use the `when` [conditional directive](https://docs.ansible.com/ansible/latest/user_guide/playbooks_conditionals.html) to execute a task only if the output of the condition is True.\n\nIn this file we also use [Ansible variables](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html) enclosed with `{{ }}`, these variables can be set in different forms, in our case we'll set them in the inventory and the default values in the role.\n\nThe default variable values or the role variable values are important in case you execute this role alone on any environment. For example to test the role or playbook with Vagrant or Docker and the default variable values will be used. Other sources of the role variables is they can be local variables, you may have variables that are set from other values provided by the user or constant values. Define some variable default values in the role file `roles/logdna/defaults/main.yaml`:\n\n<!-- TODO: find default values for the following variables -->\n\n```yaml path=logdna/defaults/main.yaml\nconf_tags: \"\"\nconf_logdir: \"\" # default to /var/log\n\nlogdna_api_host: api.us-south.logging.cloud.ibm.com\nlogdna_log_host: logs.us-south.logging.cloud.ibm.com\n```\n\nWe are not setting a default for `conf_key` because the log ingestion key is only provided by the user and associated with a specific service instance. Values for variables provided by the user come from the inventory file `inventory.yaml` so we add the following variables in the `vars:` section (update with values from your LogDNA instance, the api and log host may be the same if you are using us-south).\n\n```yaml\nall:\n  vars:\n    ansible_user: ubuntu\n    ansible_ssh_private_key_file: ~/.ssh/id_rsa\n    ansible_ssh_common_args: \"-o StrictHostKeyChecking=no\"\n\n    # Input Variables:\n    conf_key: e86611977....\n    conf_logdir: /var/log\n    conf_tags: app=movies,environment=dev\n    logdna_api_host: api.us-south.logging.cloud.ibm.com\n    logdna_log_host: logs.us-south.logging.cloud.ibm.com\n\n  children:\n    appservers:\n      hosts:\n        52.116.136.206:\n        52.116.132.187:\n        52.116.136.220:\n```\n\nThe last step in the LogDNA install is to startup the LogDNA agent service, let's do that in the file `logdna/tasks/service.yaml`, like so.\n\n```yaml path=logdna/tasks/service.yaml\n- name: Activating LogDNA Agent Service\n  shell: \"update-rc.d logdna-agent defaults\"\n\n- name: Start LogDNA agent\n  service:\n    name: logdna-agent\n    enabled: true\n    state: \"{{ agent_service }}\"\n  when: (conf_key != \"\" and agent_service) or agent_service == \"stopped\"\n\n- debug:\n    msg: \"LogDNA Agent has {{ agent_service }}!\"\n  when: (conf_key != \"\" and agent_service) or agent_service == \"stopped\"\n```\n\nHere we again use the `shell` module to activate the service and the module [service](https://docs.ansible.com/ansible/latest/modules/service_module.html) to set the state of the service to the value of the variable `agent_service`. The possible values, according to the documentation, can be `started`, `stopped`, `restarted` and `reloaded`. Again, we use the directive `when` to print the message notifying the state of the agent.\n\nThis also requires to add the variable `agent_service` to the default role variables, not to the inventory because in this user case the required action is to have the service up and running.\n\n```yaml path=logdna/defaults/main.yaml\nagent_service: started\nlogdna_api_host: api.us-south.logging.cloud.ibm.com\nlogdna_log_host: logs.us-south.logging.cloud.ibm.com\n```\n\nTo link all these files to the playbook we call them from the `main.yaml` file of the role, replace the content of the file for the following lines.\n\n```yaml path=roles/logdna/tasks/main.yaml\n- include_tasks: ./install.yaml\n\n- include_tasks: ./configure.yml\n\n- include_tasks: ./service.yaml\n```\n\nTo run this code to let Ansible install and configure LogDNA on every host created we just need to execute the `ansible-playbook` command like before.\n\n```bash\nansible-playbook -i inventory.yaml ./playbook.yaml\n```\n\nAfter this has completed running, you can open the LogDNA dashboard and then connect to one of the virtual machines over ssh.\n\n```bash\nIPADDR=$(terraform output -json ip_address | jq .[0] | tr -d \\\")\nssh ubuntu@$IPADDR\n```\n\nOver in LogDNA output similar to the following will appear as a result of the login to the system.\n\n![Screenshot from LogDNA](./images/LogDNA_log.png \"Login information from instance in LogDNA\")\n\nThis completes the walk through of the general Ansible flow. Next, this example will show how to integrate Terraform with Ansible working from the `12-ansible` folder where the final code resides. To clean up from the `start` directory, execute `terraform destroy` before moving on to the next section.\n\n```bash\nterraform destroy\n```\n\n## Ansible and Terraform Integration\n\nEverything to this point of the example still requires the user to manually configure the values in the inventory and then execute Ansible manually. To automate this process we use Terraform to manage all of the steps, chaining to the Ansible execution. After the changes shown here, `terraform` it will be the only command to execute.\n\nChange to the `12-ansible` folder to review the final code and create the `terraform.tfvars` file specified below.\n\nThe first part is to automatically populate the Ansible inventory file. This is done with Terraform template files. Move the `inventory.yaml` file to a new `templates` directory and rename it to `templates/inventory.tmpl.yaml` with the following content.\n\n```yaml path=templates/inventory.tmpl.yaml\nall:\n  vars:\n    ansible_user: ubuntu\n    ansible_ssh_private_key_file: ${private_key_file}\n    ansible_ssh_common_args: \"-o StrictHostKeyChecking=no\"\n\n    # Input Variables:\n    conf_key: ${logdna_ingestion_key}\n    conf_logdir: /var/log\n    conf_tags: app=${project_name},environment=${environment}\n    logdna_api_host: ${logdna_api_host}\n    logdna_log_host: ${logdna_log_host}\n\n  children:\n${appservers}\n```\n\n<InlineNotification>\n\n**LogDNA ingestion key**\n\nThis Terraform code does not include the provisioning or configuration of LogDNA on IBM Cloud. This has to be done manually following the instructions from the [Logging pattern](logging/content-overview). Once the instance is created, get the service instance Ingestion Key and the API and Log hostnames to set them in the terraform variables as explained below.\n\n</InlineNotification>\n\nThis architecture design requires direct access from your host to the provisioned instances, therefore every instance requires a public Floating IP, enable the Floating IPs by adding to the `network.tf`.\n\n```hcl path=network.tf\nresource \"ibm_is_floating_ip\" \"iac_app_floating_ip\" {\n  name   = \"${var.project_name}-${var.environment}-ip-${format(\"%02s\", count.index)}\"\n  target = ibm_is_instance.iac_app_instance[count.index].primary_network_interface.0.id\n  count  = var.max_size\n}\n```\n\nThe Terraform code to generate the template is as follows, in a new file called `configuration.tf` to handle all the Ansible configuration.\n\n```hcl path=configuration.tf\ndata \"template_file\" \"inventory\" {\n  depends_on = ibm_is_instance.iac_app_instance\n  template = file(\"${path.module}/inventory.yaml\")\n  vars = {\n    private_key_file = pathexpand(var.private_key_file)\n    logdna_ingestion_key = var.logdna_ingestion_key\n    project_name = var.project_name\n    environment = var.environment\n    logdna_api_host = var.logdna_api_host\n    logdna_log_host = var.logdna_log_host\n    appservers = format(\"    appservers:\\n      hosts:\\n%s\", join(\"\", formatlist(\"        %s:\\n\", ibm_is_floating_ip.iac_app_floating_ip[*].address)))\n  }\n}\n\n\ndata \"local_file\" \"inventory\" {\n  filename = \"${path.module}/inventory.yaml\"\n  content  = data.template_file.inventory.rendered\n}\n```\n\nNotice the use of `format` and `formatlist` to render from the list of Floating IP addresses `ibm_is_floating_ip.iac_app_floating_ip[*].address` output like this:\n\n```yaml\n    appservers:\n      hosts:\n        52.116.136.206:\n        52.116.132.187:\n        52.116.136.220:\n```\n\nThe template also has additional input variables for the Ansible playbook that are not created by Terraform code so they are required as Terraform user input. Add them to the `variables.tf` file and the values to the `terraform.tfvars` file (which is or should be in `.gitignore`)\n\n```hcl path=variables.tf\nvariable \"logdna_ingestion_key\" {}\nvariable \"logdna_api_host\" {\n  default = \"api.us-south.logging.cloud.ibm.com\"\n}\nvariable \"logdna_log_host\" {\n  default = \"logs.us-south.logging.cloud.ibm.com\"\n}\nvariable \"private_key_file\" {\n  default = \"~/.ssh/id_rsa\"\n}\n```\n\n```hcl path=terraform.tfvars\nlogdna_ingestion_key = \"e86611977....\"\nlogdna_api_host = \"api.us-south.logging.cloud.ibm.com\"\nlogdna_log_host = \"logs.us-south.logging.cloud.ibm.com\"\n```\n\nExecuting this code will generate a `inventory.yaml` file which should be ignored in the GitHub repository as it's a dynamically generated file, so add the filename to the `.gitignore` file.\n\n```bash\necho 'inventory.yaml' >> .gitignore\n```\n\nHaving the inventory ready everything ready to execute `ansible-playbook` but the idea is to automate everything so we are going to make Terraform execute Ansible for us with the following `local-exec` provisioner inside a `null_resource` block in the `configuration.tf` file.\n\n```hcl\nresource \"null_resource\" \"ansible\" {\n  depends_on = [local_file.inventory]\n\n  provisioner \"local-exec\" {\n    command = \"ansible-playbook -i ${path.module}/inventory.yaml ${path.module}/playbook.yml\"\n  }\n}\n```\n\nThis is fine but the Ansible command has to be executed when the instance is up and running and as we know Terraform (with this example) doesn't know when the instance is ready. For example, the Cloud-Init code may be still running when Terraform ends. There is a trick to overcome this. Running a `remote_exec` to execute any command in the instance will make Terraform to wait for the Floating IP and the instance to be ready.\n\nAdd the following `null_resource.waiter` block into the `configuration.tf` file and make the `template_file.inventory` to depend on it.\n\n```hcl\nresource \"null_resource\" \"waiter\" {\n  depends_on = [ibm_is_instance.iac_app_instance, ibm_is_floating_ip.iac_app_floating_ip]\n\n  count  = var.max_size\n\n  provisioner \"remote_exec\" {\n    inline = [\"hostname\"]\n\n    connection {\n      user = \"ubuntu\"\n      host = ibm_is_floating_ip.iac_app_floating_ip[count.index].address\n      private_key = file(pathexpand(var.private_key_file))\n      timeout = \"5m\"\n    }\n  }\n}\n\ndata \"template_file\" \"inventory\" {\n  depends_on = [null_resource.waiter]\n  ...\n}\n```\n\nThe `null_resource.waiter` resource makes a connection to every provisioned instance to execute any command, i.e. `hostname`, when the connection and the command is successfully completed to every instance then Terraform will proceed to generate the inventory and execute ansible (remember ansible execution depends on the inventory).\n\nAs we are using a new providers (`templates` and `null`), we need to execute `terraform init`, then we can plan and apply the code changes\n\n```bash\nterraform init\nterraform plan\nterraform apply\n```\n\nTerraform will do the provisioning and generate the inventory file, when all the instances are up and ready it executes Ansible to install and configure LogDNA on every provisioned instance.\n\nNotice that the Ansible logs went to the Terraform logs, if you'd like to have them in a different file as well for better analysis (i.e. if the playbook fails) modify the Ansible execution in the `local_exec` provider to the following line and add the `ansible.log` file to the `.gitignore` file.\n\n```hcl\n  command = \"ansible-playbook -i ${path.module}/inventory.yaml ${path.module}/playbook.yaml | tee --append ${path.module}/ansible.log\"\n```\n\nTo test your code, as before, execute the following `curl` command to check the API but now you can go to the LogDNA Web Console and review the log analysis from the nodes.\n\n```bash\ncurl $(terraform output entrypoint)/movies/675\n```\n\n## Final Terraform and Ansible Code\n\nAll the developed code can be downloaded or cloned from the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [12-ansible](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/12-ansible) where the new or modified files are:\n\n```hcl path=configuration.tf\nresource \"null_resource\" \"waiter\" {\n  depends_on = [ibm_is_instance.iac_app_instance, ibm_is_floating_ip.iac_app_floating_ip]\n\n  count = var.max_size\n\n  provisioner \"remote-exec\" {\n    inline = [\"hostname\"]\n\n    connection {\n      user        = \"ubuntu\"\n      host        = ibm_is_floating_ip.iac_app_floating_ip[count.index].address\n      private_key = file(pathexpand(var.private_key_file))\n      timeout     = \"5m\"\n    }\n  }\n}\n\ndata \"template_file\" \"inventory\" {\n  depends_on = [null_resource.waiter]\n\n  template = file(\"${path.module}/templates/inventory.tmpl.yaml\")\n  vars = {\n    private_key_file     = pathexpand(var.private_key_file)\n    logdna_ingestion_key = var.logdna_ingestion_key\n    project_name         = var.project_name\n    environment          = var.environment\n    logdna_api_host      = var.logdna_api_host\n    logdna_log_host      = var.logdna_log_host\n    appservers           = format(\"    appservers:\\n      hosts:\\n%s\", join(\"\", formatlist(\"        %s:\\n\", ibm_is_floating_ip.iac_app_floating_ip[*].address)))\n  }\n}\n\nresource \"local_file\" \"inventory\" {\n  filename = \"${path.module}/inventory.yaml\"\n  content  = data.template_file.inventory.rendered\n}\n\nresource \"null_resource\" \"ansible\" {\n  depends_on = [local_file.inventory]\n\n  provisioner \"local-exec\" {\n    command = \"ansible-playbook -i ${path.module}/inventory.yaml ${path.module}/playbook.yaml | tee --append ${path.module}/ansible.log\"\n  }\n}\n```\n\n```yaml path=templates/inventory.tmpl.yaml\nall:\n  vars:\n    ansible_user: ubuntu\n    ansible_ssh_private_key_file: ${private_key_file}\n    ansible_ssh_common_args: \"-o StrictHostKeyChecking=no\"\n\n    # Input Variables:\n    conf_key: ${logdna_ingestion_key}\n    conf_logdir: /var/log\n    conf_tags: app=${project_name},environment=${environment}\n    logdna_api_host: ${logdna_api_host}\n    logdna_log_host: ${logdna_log_host}\n\n  children:\n${appservers}\n```\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"public_key\" {}\n\nvariable \"logdna_ingestion_key\" {}\nvariable \"logdna_api_host\" {\n  default = \"api.us-south.logging.cloud.ibm.com\"\n}\nvariable \"logdna_log_host\" {\n  default = \"logs.us-south.logging.cloud.ibm.com\"\n}\nvariable \"private_key_file\" {\n  default = \"~/.ssh/id_rsa\"\n}\n\nvariable \"port\" {\n  default = 8080\n}\nvariable \"max_size\" {\n  default = 3\n}\n```\n\n```yaml path=playbook.yaml\n- hosts: all\n  become: yes\n  roles:\n    - role: logdna\n```\n\n```yaml path=roles/logdna/tasks/main.yaml\n- include_tasks: ./install.yaml\n\n- include_tasks: ./configure.yaml\n\n- include_tasks: ./service.yaml\n```\n\n```yaml path=roles/logdna/tasks/install.yaml\n- name: LogDNA Agent Package Public Key\n  apt_key:\n    url: https://repo.logdna.com/logdna.gpg\n\n- name: LogDNA Agent Repository\n  apt_repository:\n    repo: deb https://repo.logdna.com stable main\n\n- name: Install LogDNA Agent\n  apt:\n    state: present\n    name: logdna-agent\n    allow_unauthenticated: true\n    update_cache: true\n```\n\n```yaml path=roles/logdna/tasks/configure.yaml\n- name: Setting LogDNA Ingestion Key\n  shell: \"logdna-agent -k {{ conf_key }}\"\n  when: conf_key != \"\"\n\n- name: Adding Log Directories\n  shell: \"logdna-agent -d {{ conf_logdir }}\"\n  when: conf_logdir != \"\"\n\n- name: Adding API Host\n  shell: \"logdna-agent -s LOGDNA_APIHOST={{ ubuntu_api_host }}\"\n  when: ubuntu_api_host != \"\"\n\n- name: Adding Log Host\n  shell: \"logdna-agent -s LOGDNA_LOGHOST={{ ubuntu_log_host }}\"\n  when: ubuntu_log_host != \"\"\n\n- name: Setting Tags for This Host\n  shell: \"logdna-agent -t {{ conf_tags }}\"\n  when: conf_tags != \"\"\n```\n\n```yaml path=roles/logdna/tasks/service.yaml\n- name: Activating LogDNA Agent Service\n  shell: \"update-rc.d logdna-agent defaults\"\n\n- name: Start LogDNA agent\n  service:\n    name: logdna-agent\n    enabled: true\n    state: \"{{ agent_service }}\"\n  when: (conf_key != \"\" and agent_service) or agent_service == \"stopped\"\n\n- debug:\n    msg: \"LogDNA Agent has {{ agent_service }}!\"\n  when: (conf_key != \"\" and agent_service) or agent_service == \"stopped\"\n```\n\n```yaml path=roles/logdna/defaults/main.yaml\nconf_tags: \"\"\nconf_logdir: \"\" # default to /var/log\n\nubuntu_api_host: api.us-south.logging.cloud.ibm.com\nubuntu_log_host: logs.us-south.logging.cloud.ibm.com\nagent_service: started\n```\n\n```hcl path=network.tf\nresource \"ibm_is_vpc\" \"iac_app_vpc\" {\n  name = \"${var.project_name}-${var.environment}-vpc\"\n}\n\nresource \"ibm_is_subnet\" \"iac_app_subnet\" {\n  name            = \"${var.project_name}-${var.environment}-subnet\"\n  vpc             = ibm_is_vpc.iac_app_vpc.id\n  zone            = \"us-south-1\"\n  ipv4_cidr_block = \"10.240.0.0/24\"\n}\n\nresource \"ibm_is_security_group\" \"iac_app_security_group\" {\n  name = \"${var.project_name}-${var.environment}-sg-public\"\n  vpc  = ibm_is_vpc.iac_app_vpc.id\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_app_security_group_rule_all_outbound\" {\n  group     = ibm_is_security_group.iac_app_security_group.id\n  direction = \"outbound\"\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_app_security_group_rule_tcp_http\" {\n  group     = ibm_is_security_group.iac_app_security_group.id\n  direction = \"inbound\"\n  tcp {\n    port_min = var.port\n    port_max = var.port\n  }\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_app_security_group_rule_tcp_ssh\" {\n  group     = ibm_is_security_group.iac_app_security_group.id\n  direction = \"inbound\"\n  tcp {\n    port_min = 22\n    port_max = 22\n  }\n}\n\nresource \"ibm_is_floating_ip\" \"iac_app_floating_ip\" {\n  name   = \"${var.project_name}-${var.environment}-ip-${format(\"%02s\", count.index)}\"\n  target = ibm_is_instance.iac_app_instance[count.index].primary_network_interface.0.id\n  count  = var.max_size\n}\n```\n\nOptionally, you can have a `ansible.cfg` to add more settings, if required.\n\n```ini\n[defaults]\nhost_key_checking = False\n```\n\nIt's required to have a `terraform.tfvars` file with the variables values or set them in environment variables. Use this example to create your `terraform.tfvars` file.\n\n```hcl path=terraform.tfvars\nproject_name = \"iac-ansible-test\"\nenvironment  = \"dev\"\nport         = \"8080\"\npublic_key   = \"ssh-rsa AAAAB3....\"\n\nlogdna_ingestion_key = \"e86611977....\"\nlogdna_api_host      = \"api.us-south.logging.cloud.ibm.com\"\nlogdna_log_host      = \"logs.us-south.logging.cloud.ibm.com\"\n```\n\n## Ansible from a Bastion Host\n\nThe previous design is functional but there is a dimension that may not be preferred in all cases. The Floating IPs and the open access to all the nodes from a single host, namely the workstation running Terraform and Ansible. A solution to avoid a requirement for Floating IPs is to have a Bastion Host. Access to the instances for configuration management can be done from a single host, the Bastion Host, and access controls are placed around this host which is used from the perimeter of the VPC to perform Ansible configuration management.\n\n![Ansible at Bastion Host](./images/IaC-Ansible_Design_2.png \"Running Ansible from Bastion Host\")\n\nThe Terraform and Ansible code for this has not been developed but it can be done in a later release to this pattern guide. Terraform first provisions the environment and also the Bastion Host with Ansible and send all playbooks to the host (if they are not already present). Then Terraform generates the inventory and uploads it to the host to finally execute Ansible remotely to configure all the hosts.\n\nAnother scenario for using a Bastion Host is in the use of IBM Cloud Schematics. In this example, we are executing Ansible from our host but with Schematics there is no host where it can install and execute Ansible, unless you create a instance or docker container to help you this task. Having a Bastion Host, Schematics can use it to execute Ansible and configure all the provisioned instances.\n\n## Clean up\n\nTo clean up everything just execute `terraform destroy`. As we are not using Schematics there is no need to run `ibmcloud` to make Schematics destroy the resources and the workspace.\n\n```bash\nterraform destroy\n```\n","type":"Mdx","contentDigest":"4d10cf8f841ade65c1561295f2f69ce2","counter":592,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"IaC Configuration Management with Ansible","description":"Use Ansible to do configuration management of the provisioned instances","keywords":"terraform,ibm cloud,config management,ansible"},"exports":{},"rawBody":"---\ntitle: IaC Configuration Management with Ansible\ndescription: Use Ansible to do configuration management of the provisioned instances\nkeywords: \"terraform,ibm cloud,config management,ansible\"\n---\n\n<PageDescription>\n\nConfigure the provisioned instance using Ansible\n\n</PageDescription>\n\n<AnchorLinks small>\n  <AnchorLink>Ansible Introduction</AnchorLink>\n  <AnchorLink>Ansible and Terraform Integration</AnchorLink>\n  <AnchorLink>Final Terraform and Ansible Code</AnchorLink>\n  <AnchorLink>Ansible from a Bastion Host</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\nTerraform is great for infrastructure provisioning but it's not a configuration management tool. That's where Ansible comes in. After Terraform finishes the provisioning of an instance, control goes to Ansible to finish up the configuration of the instance system and applications.\n\n<InlineNotification>\n\n**Other Configuration Management Tools**\n\nAnsible is a Configuration Management Tool but is not the only one. There are others such as [Puppet](https://puppet.com), [Chef](https://www.chef.io) and [SaltStack](https://www.saltstack.com). This pattern explains how to use Ansible for Configuration Management after Terraform finishes the Provisioning and creates the input data for Ansible. In a similar way you can setup the input data for Puppet, Chef, SaltStack or other configuration management tools.\n\n</InlineNotification>\n\n## Ansible Introduction\n\nAnsible is one of the most used Configuration Management tools, it's simple and easy to learn. Ansible use SSH to connect to the servers or instances to execute the configuration tasks defined in YAML files or playbooks. Ansible tasks are idempotent, this means that they can be applied multiple times without changing the result beyond the initial application, this is what makes Ansible reliable and differentiates it from some other configuration management tools.\n\nTo use Ansible we need one host with Ansible, the playbooks and all the Ansible configuration files. There are different designs, the one shown in this example is to use Ansible from the same host where Terraform is executed. The Terraform and Ansible example code is in the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [12-ansible](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/12-ansible).\n\n![Ansible at Local Host](./images/IaC-Ansible_Design_1.png \"Running Ansible from Local Host\")\n\n### Install Ansible\n\nInstall Ansible in the same host where Terraform is installed. Follow these [instructions](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html) to install Ansible on your platform. However, the most generic way to install Ansible is using `pip` (Python package manager).\n\n```bash\npip install --user ansible\n```\n\nIf you are on MacOS, other option to install Ansible is using `brew`, it'll install Ansible and its dependencies.\n\n```bash\nbrew install ansible\n```\n\nTo verify Ansible is correctly installed, just execute `ansible --version` and you should get the latest version, at the time this document is written, it's version `2.9.7`.\n\nThe use case exposed in this section is to install to the LogDNA agent on the instance for log analysis. All the hosts require the LogDNA agent to send the logs to the LogDNA service.\n\n### Infrastructure Setup\n\nClone the example project and go to the `12-user-data/start` folder. You will already need to have set up the terraform cli and the API key for terraform as described in [Setup Environment](/iac/setup-environment). Run these commands to initially deploy the virtual machines for the Ansible examples.\n\n```bash\nterraform init\nterraform plan\nterraform apply\n```\n\n### Inventory File\n\nThe first file used by Ansible is the inventory file with the list of hosts to configure. The [inventory file documentation](https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html) shows all the possible parameters and variables you can set. Create a `inventory.yaml` file in the `12-ansible/start` folder with the content below, but update with the IP addresses output from `terraform apply`.\n\n```yaml path=inventory.yaml\nall:\n  vars:\n    ansible_user: ubuntu\n    ansible_ssh_private_key_file: ~/.ssh/id_rsa\n    ansible_ssh_common_args: \"-o StrictHostKeyChecking=no\"\n  children:\n    appservers:\n      hosts:\n        52.116.136.206:\n        52.116.132.187:\n        52.116.136.220:\n\n```\n\nAs the example progresses, we will add more information into this file such as variables and the hostnames, for now we just need the IP addresses grouped by server tier (`appserver`) and the following ansible variables:\n\n- `ansible_user` defines the user to login into the hosts, in this case is `ubuntu` user\n- `ansible_ssh_private_key_file` path to the private SSH key used to login to the hosts, if you [used the public key](/iac-resources/compute) `~/.ssh/id_rsa.pub` then provide the private key `~/.ssh/id_rsa` pair\n- `ansible_ssh_common_args` this variable set extra parameters to the SSH command used by Ansible to connect to the hosts. With no extra arguments SSH will ask confirmation to the user to add the keys to the `known_hosts` file. Add the parameter `-o StrictHostKeyChecking=no` to make this process non-interactive. This can also be achieved with the setting `host_key_checking = False` into the `ansible.cfg` file in the same directory.\n\nTo verify we can reach the hosts we execute Ansible using the `ping` module.\n\n```bash\nansible all -m ping -i inventory.yaml\n```\n\n### Playbook and Roles\n\nA [Playbook](http://docs.ansible.com/playbooks_intro.html) is yaml files with a set of tasks or roles to be executed. A [Role](https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html) is a collection of related tasks and handlers, to encapsulate data such as variables, files, templates and metadata, to execute one specific macro-task. For example, our (macro) task is to get LogDNA up and running on a given host, to execute this task we need to execute (atomic) tasks to install packages, execute configuration commands and start services, using the given configuration parameters (data in form of variables). We can use any of the open sourced [Ansible Galaxy Roles](https://galaxy.ansible.com) but in this example we'll create a simple role.\n\nStill working in the `start` directory, create the directory path `roles/logdna` and inside the directory a `tasks` sub-directory with the file `main.yaml`. Normal convention with Ansible will often add directories such as `handlers`, `defaults`, `vars`, `files`, `templates` and `meta` but they are not needed and are out of the scope of this article.\n\n```bash\nmkdir -p roles/logdna/tasks\n```\n\nThis example only covers the installation of LogDNA. If you'd like to install or configure something else such as deploing the API application or deploying the initial database, then you would create more roles under the directory `roles` for those tasks.\n\nInside the file `roles/logdna/tasks/main.yaml`\n\n```yaml path=roles/logdna/tasks/main.yaml\n- debug:\n    msg: \"Checking Ansible is working!\"\n```\n\nWe'll develop the tasks role more, but for now let's have a debug message just to verify it's working.\n\nTo tell Ansible which roles to execute, create the following playbook file named `playbook.yaml`.\n\n```yaml path=playbook.yaml\n- hosts: all\n  become: yes\n  roles:\n    - role: logdna\n```\n\nThis playbook file instructs Ansible to execute the instructions on `all` hosts. The instructions should be executed as root, so the `become: yes` is used. And finally, we list all the roles to run on every host. You may have a list of roles per host groups such as `appservers` or `dbservers`. Then execute the playbook with the `ansible-playbook` command, like so.\n\n```bash\nansible-playbook -i inventory.yaml ./playbook.yaml\n```\n\nYou should see, among a bunch of lines, the following lines starting with `ok` and at the end a report summary like the following.\n\n```text\nok: [52.116.136.206] => {\n    \"msg\": \"Checking Ansible is working!\"\n}\nok: [52.116.132.187] => {\n    \"msg\": \"Checking Ansible is working!\"\n}\nok: [52.116.136.220] => {\n    \"msg\": \"Checking Ansible is working!\"\n}\n\n52.116.132.187             : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n52.116.136.206             : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n52.116.136.220             : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n```\n\n### Modules\n\nAnsible uses [modules](http://docs.ansible.com/modules_by_category.html) to perform most tasks. In this example we'll use a few modules to install packages for Ubuntu, executing commands, writing files, and handling the services.\n\n<InlineNotification>\n\n**Running a single module**\n\nIt's possible to just execute a single module or task with the `ansible` command. This is very handy when you need to execute a command for a group of hosts. However, excessive use of this approach is a bad practice, if this command is required frequently then it should be in a role or playbook.\n\nAn example of a single task or module execution is the previous ansible line using `ping` to verify the hosts are accessible. Another example may be using the `apt` module to install a package:\n\n```bash\nansible all -s -m apt -a 'pkg=nginx state=installed update_cache=true'\n```\n\n</InlineNotification>\n\nIn the `logdna/tasks/install.yaml` file use the following modules to install the LogDNA packages.\n\n```yaml path=logdna/tasks/install.yaml\n- name: LogDNA Agent Package Public Key\n  apt_key:\n    url: https://repo.logdna.com/logdna.gpg\n\n- name: LogDNA Agent Repository\n  apt_repository:\n    repo: deb https://repo.logdna.com stable main\n\n- name: Install LogDNA Agent\n  apt:\n    state: present\n    name: logdna-agent\n    allow_unauthenticated: true\n    update_cache: true\n```\n\nEvery task can start with the key `name` to document or describe the task, this text is printed on the console when it's executed. All the modules in this file deal with `apt`, the package manager of Debian and Ubuntu. Here are links to the documentation of each module to know the different parameters that can be used.\n\n- [apt_key](https://docs.ansible.com/ansible/latest/modules/apt_key_module.html) is used to add, remove or download apt keys\n- [apt_repository](https://docs.ansible.com/ansible/latest/modules/apt_repository_module.html) to add or remove APT repositories\n- [apt](https://docs.ansible.com/ansible/latest/modules/apt_module.html) to install, remove, upgrade, update and manage in general APT packages on Debian and Ubuntu\n\nCreate the file `logdna/tasks/configure.yaml` to execute the tasks to configure LogDNA.\n\n```yaml path=logdna/tasks/configure.yaml\n- name: Setting LogDNA Ingestion Key\n  shell: \"logdna-agent -k {{ conf_key }}\"\n  when: conf_key != \"\"\n\n- name: Adding Log Directories\n  shell: \"logdna-agent -d {{ conf_logdir }}\"\n  when: conf_logdir != \"\"\n\n- name: Adding API Host\n  shell: \"logdna-agent -s LOGDNA_APIHOST={{ logdna_api_host }}\"\n  when: logdna_api_host != \"\"\n\n- name: Adding Log Host\n  shell: \"logdna-agent -s LOGDNA_LOGHOST={{ logdna_log_host }}\"\n  when: logdna_log_host != \"\"\n\n- name: Setting Tags for This Host\n  shell: \"logdna-agent -t {{ conf_tags }}\"\n  when: conf_tags != \"\"\n```\n\nIn this file we only use the [shell](https://docs.ansible.com/ansible/latest/modules/shell_module.html) module to execute commands on the host. This module is useful when there is no module to do the tasks we want or need. However, exercise caution with the shell module because the outcome of the execution of these commands may not be idempotent, breaking the main characteristics of Ansible.\n\nHere we use the `when` [conditional directive](https://docs.ansible.com/ansible/latest/user_guide/playbooks_conditionals.html) to execute a task only if the output of the condition is True.\n\nIn this file we also use [Ansible variables](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html) enclosed with `{{ }}`, these variables can be set in different forms, in our case we'll set them in the inventory and the default values in the role.\n\nThe default variable values or the role variable values are important in case you execute this role alone on any environment. For example to test the role or playbook with Vagrant or Docker and the default variable values will be used. Other sources of the role variables is they can be local variables, you may have variables that are set from other values provided by the user or constant values. Define some variable default values in the role file `roles/logdna/defaults/main.yaml`:\n\n<!-- TODO: find default values for the following variables -->\n\n```yaml path=logdna/defaults/main.yaml\nconf_tags: \"\"\nconf_logdir: \"\" # default to /var/log\n\nlogdna_api_host: api.us-south.logging.cloud.ibm.com\nlogdna_log_host: logs.us-south.logging.cloud.ibm.com\n```\n\nWe are not setting a default for `conf_key` because the log ingestion key is only provided by the user and associated with a specific service instance. Values for variables provided by the user come from the inventory file `inventory.yaml` so we add the following variables in the `vars:` section (update with values from your LogDNA instance, the api and log host may be the same if you are using us-south).\n\n```yaml\nall:\n  vars:\n    ansible_user: ubuntu\n    ansible_ssh_private_key_file: ~/.ssh/id_rsa\n    ansible_ssh_common_args: \"-o StrictHostKeyChecking=no\"\n\n    # Input Variables:\n    conf_key: e86611977....\n    conf_logdir: /var/log\n    conf_tags: app=movies,environment=dev\n    logdna_api_host: api.us-south.logging.cloud.ibm.com\n    logdna_log_host: logs.us-south.logging.cloud.ibm.com\n\n  children:\n    appservers:\n      hosts:\n        52.116.136.206:\n        52.116.132.187:\n        52.116.136.220:\n```\n\nThe last step in the LogDNA install is to startup the LogDNA agent service, let's do that in the file `logdna/tasks/service.yaml`, like so.\n\n```yaml path=logdna/tasks/service.yaml\n- name: Activating LogDNA Agent Service\n  shell: \"update-rc.d logdna-agent defaults\"\n\n- name: Start LogDNA agent\n  service:\n    name: logdna-agent\n    enabled: true\n    state: \"{{ agent_service }}\"\n  when: (conf_key != \"\" and agent_service) or agent_service == \"stopped\"\n\n- debug:\n    msg: \"LogDNA Agent has {{ agent_service }}!\"\n  when: (conf_key != \"\" and agent_service) or agent_service == \"stopped\"\n```\n\nHere we again use the `shell` module to activate the service and the module [service](https://docs.ansible.com/ansible/latest/modules/service_module.html) to set the state of the service to the value of the variable `agent_service`. The possible values, according to the documentation, can be `started`, `stopped`, `restarted` and `reloaded`. Again, we use the directive `when` to print the message notifying the state of the agent.\n\nThis also requires to add the variable `agent_service` to the default role variables, not to the inventory because in this user case the required action is to have the service up and running.\n\n```yaml path=logdna/defaults/main.yaml\nagent_service: started\nlogdna_api_host: api.us-south.logging.cloud.ibm.com\nlogdna_log_host: logs.us-south.logging.cloud.ibm.com\n```\n\nTo link all these files to the playbook we call them from the `main.yaml` file of the role, replace the content of the file for the following lines.\n\n```yaml path=roles/logdna/tasks/main.yaml\n- include_tasks: ./install.yaml\n\n- include_tasks: ./configure.yml\n\n- include_tasks: ./service.yaml\n```\n\nTo run this code to let Ansible install and configure LogDNA on every host created we just need to execute the `ansible-playbook` command like before.\n\n```bash\nansible-playbook -i inventory.yaml ./playbook.yaml\n```\n\nAfter this has completed running, you can open the LogDNA dashboard and then connect to one of the virtual machines over ssh.\n\n```bash\nIPADDR=$(terraform output -json ip_address | jq .[0] | tr -d \\\")\nssh ubuntu@$IPADDR\n```\n\nOver in LogDNA output similar to the following will appear as a result of the login to the system.\n\n![Screenshot from LogDNA](./images/LogDNA_log.png \"Login information from instance in LogDNA\")\n\nThis completes the walk through of the general Ansible flow. Next, this example will show how to integrate Terraform with Ansible working from the `12-ansible` folder where the final code resides. To clean up from the `start` directory, execute `terraform destroy` before moving on to the next section.\n\n```bash\nterraform destroy\n```\n\n## Ansible and Terraform Integration\n\nEverything to this point of the example still requires the user to manually configure the values in the inventory and then execute Ansible manually. To automate this process we use Terraform to manage all of the steps, chaining to the Ansible execution. After the changes shown here, `terraform` it will be the only command to execute.\n\nChange to the `12-ansible` folder to review the final code and create the `terraform.tfvars` file specified below.\n\nThe first part is to automatically populate the Ansible inventory file. This is done with Terraform template files. Move the `inventory.yaml` file to a new `templates` directory and rename it to `templates/inventory.tmpl.yaml` with the following content.\n\n```yaml path=templates/inventory.tmpl.yaml\nall:\n  vars:\n    ansible_user: ubuntu\n    ansible_ssh_private_key_file: ${private_key_file}\n    ansible_ssh_common_args: \"-o StrictHostKeyChecking=no\"\n\n    # Input Variables:\n    conf_key: ${logdna_ingestion_key}\n    conf_logdir: /var/log\n    conf_tags: app=${project_name},environment=${environment}\n    logdna_api_host: ${logdna_api_host}\n    logdna_log_host: ${logdna_log_host}\n\n  children:\n${appservers}\n```\n\n<InlineNotification>\n\n**LogDNA ingestion key**\n\nThis Terraform code does not include the provisioning or configuration of LogDNA on IBM Cloud. This has to be done manually following the instructions from the [Logging pattern](logging/content-overview). Once the instance is created, get the service instance Ingestion Key and the API and Log hostnames to set them in the terraform variables as explained below.\n\n</InlineNotification>\n\nThis architecture design requires direct access from your host to the provisioned instances, therefore every instance requires a public Floating IP, enable the Floating IPs by adding to the `network.tf`.\n\n```hcl path=network.tf\nresource \"ibm_is_floating_ip\" \"iac_app_floating_ip\" {\n  name   = \"${var.project_name}-${var.environment}-ip-${format(\"%02s\", count.index)}\"\n  target = ibm_is_instance.iac_app_instance[count.index].primary_network_interface.0.id\n  count  = var.max_size\n}\n```\n\nThe Terraform code to generate the template is as follows, in a new file called `configuration.tf` to handle all the Ansible configuration.\n\n```hcl path=configuration.tf\ndata \"template_file\" \"inventory\" {\n  depends_on = ibm_is_instance.iac_app_instance\n  template = file(\"${path.module}/inventory.yaml\")\n  vars = {\n    private_key_file = pathexpand(var.private_key_file)\n    logdna_ingestion_key = var.logdna_ingestion_key\n    project_name = var.project_name\n    environment = var.environment\n    logdna_api_host = var.logdna_api_host\n    logdna_log_host = var.logdna_log_host\n    appservers = format(\"    appservers:\\n      hosts:\\n%s\", join(\"\", formatlist(\"        %s:\\n\", ibm_is_floating_ip.iac_app_floating_ip[*].address)))\n  }\n}\n\n\ndata \"local_file\" \"inventory\" {\n  filename = \"${path.module}/inventory.yaml\"\n  content  = data.template_file.inventory.rendered\n}\n```\n\nNotice the use of `format` and `formatlist` to render from the list of Floating IP addresses `ibm_is_floating_ip.iac_app_floating_ip[*].address` output like this:\n\n```yaml\n    appservers:\n      hosts:\n        52.116.136.206:\n        52.116.132.187:\n        52.116.136.220:\n```\n\nThe template also has additional input variables for the Ansible playbook that are not created by Terraform code so they are required as Terraform user input. Add them to the `variables.tf` file and the values to the `terraform.tfvars` file (which is or should be in `.gitignore`)\n\n```hcl path=variables.tf\nvariable \"logdna_ingestion_key\" {}\nvariable \"logdna_api_host\" {\n  default = \"api.us-south.logging.cloud.ibm.com\"\n}\nvariable \"logdna_log_host\" {\n  default = \"logs.us-south.logging.cloud.ibm.com\"\n}\nvariable \"private_key_file\" {\n  default = \"~/.ssh/id_rsa\"\n}\n```\n\n```hcl path=terraform.tfvars\nlogdna_ingestion_key = \"e86611977....\"\nlogdna_api_host = \"api.us-south.logging.cloud.ibm.com\"\nlogdna_log_host = \"logs.us-south.logging.cloud.ibm.com\"\n```\n\nExecuting this code will generate a `inventory.yaml` file which should be ignored in the GitHub repository as it's a dynamically generated file, so add the filename to the `.gitignore` file.\n\n```bash\necho 'inventory.yaml' >> .gitignore\n```\n\nHaving the inventory ready everything ready to execute `ansible-playbook` but the idea is to automate everything so we are going to make Terraform execute Ansible for us with the following `local-exec` provisioner inside a `null_resource` block in the `configuration.tf` file.\n\n```hcl\nresource \"null_resource\" \"ansible\" {\n  depends_on = [local_file.inventory]\n\n  provisioner \"local-exec\" {\n    command = \"ansible-playbook -i ${path.module}/inventory.yaml ${path.module}/playbook.yml\"\n  }\n}\n```\n\nThis is fine but the Ansible command has to be executed when the instance is up and running and as we know Terraform (with this example) doesn't know when the instance is ready. For example, the Cloud-Init code may be still running when Terraform ends. There is a trick to overcome this. Running a `remote_exec` to execute any command in the instance will make Terraform to wait for the Floating IP and the instance to be ready.\n\nAdd the following `null_resource.waiter` block into the `configuration.tf` file and make the `template_file.inventory` to depend on it.\n\n```hcl\nresource \"null_resource\" \"waiter\" {\n  depends_on = [ibm_is_instance.iac_app_instance, ibm_is_floating_ip.iac_app_floating_ip]\n\n  count  = var.max_size\n\n  provisioner \"remote_exec\" {\n    inline = [\"hostname\"]\n\n    connection {\n      user = \"ubuntu\"\n      host = ibm_is_floating_ip.iac_app_floating_ip[count.index].address\n      private_key = file(pathexpand(var.private_key_file))\n      timeout = \"5m\"\n    }\n  }\n}\n\ndata \"template_file\" \"inventory\" {\n  depends_on = [null_resource.waiter]\n  ...\n}\n```\n\nThe `null_resource.waiter` resource makes a connection to every provisioned instance to execute any command, i.e. `hostname`, when the connection and the command is successfully completed to every instance then Terraform will proceed to generate the inventory and execute ansible (remember ansible execution depends on the inventory).\n\nAs we are using a new providers (`templates` and `null`), we need to execute `terraform init`, then we can plan and apply the code changes\n\n```bash\nterraform init\nterraform plan\nterraform apply\n```\n\nTerraform will do the provisioning and generate the inventory file, when all the instances are up and ready it executes Ansible to install and configure LogDNA on every provisioned instance.\n\nNotice that the Ansible logs went to the Terraform logs, if you'd like to have them in a different file as well for better analysis (i.e. if the playbook fails) modify the Ansible execution in the `local_exec` provider to the following line and add the `ansible.log` file to the `.gitignore` file.\n\n```hcl\n  command = \"ansible-playbook -i ${path.module}/inventory.yaml ${path.module}/playbook.yaml | tee --append ${path.module}/ansible.log\"\n```\n\nTo test your code, as before, execute the following `curl` command to check the API but now you can go to the LogDNA Web Console and review the log analysis from the nodes.\n\n```bash\ncurl $(terraform output entrypoint)/movies/675\n```\n\n## Final Terraform and Ansible Code\n\nAll the developed code can be downloaded or cloned from the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [12-ansible](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/12-ansible) where the new or modified files are:\n\n```hcl path=configuration.tf\nresource \"null_resource\" \"waiter\" {\n  depends_on = [ibm_is_instance.iac_app_instance, ibm_is_floating_ip.iac_app_floating_ip]\n\n  count = var.max_size\n\n  provisioner \"remote-exec\" {\n    inline = [\"hostname\"]\n\n    connection {\n      user        = \"ubuntu\"\n      host        = ibm_is_floating_ip.iac_app_floating_ip[count.index].address\n      private_key = file(pathexpand(var.private_key_file))\n      timeout     = \"5m\"\n    }\n  }\n}\n\ndata \"template_file\" \"inventory\" {\n  depends_on = [null_resource.waiter]\n\n  template = file(\"${path.module}/templates/inventory.tmpl.yaml\")\n  vars = {\n    private_key_file     = pathexpand(var.private_key_file)\n    logdna_ingestion_key = var.logdna_ingestion_key\n    project_name         = var.project_name\n    environment          = var.environment\n    logdna_api_host      = var.logdna_api_host\n    logdna_log_host      = var.logdna_log_host\n    appservers           = format(\"    appservers:\\n      hosts:\\n%s\", join(\"\", formatlist(\"        %s:\\n\", ibm_is_floating_ip.iac_app_floating_ip[*].address)))\n  }\n}\n\nresource \"local_file\" \"inventory\" {\n  filename = \"${path.module}/inventory.yaml\"\n  content  = data.template_file.inventory.rendered\n}\n\nresource \"null_resource\" \"ansible\" {\n  depends_on = [local_file.inventory]\n\n  provisioner \"local-exec\" {\n    command = \"ansible-playbook -i ${path.module}/inventory.yaml ${path.module}/playbook.yaml | tee --append ${path.module}/ansible.log\"\n  }\n}\n```\n\n```yaml path=templates/inventory.tmpl.yaml\nall:\n  vars:\n    ansible_user: ubuntu\n    ansible_ssh_private_key_file: ${private_key_file}\n    ansible_ssh_common_args: \"-o StrictHostKeyChecking=no\"\n\n    # Input Variables:\n    conf_key: ${logdna_ingestion_key}\n    conf_logdir: /var/log\n    conf_tags: app=${project_name},environment=${environment}\n    logdna_api_host: ${logdna_api_host}\n    logdna_log_host: ${logdna_log_host}\n\n  children:\n${appservers}\n```\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"public_key\" {}\n\nvariable \"logdna_ingestion_key\" {}\nvariable \"logdna_api_host\" {\n  default = \"api.us-south.logging.cloud.ibm.com\"\n}\nvariable \"logdna_log_host\" {\n  default = \"logs.us-south.logging.cloud.ibm.com\"\n}\nvariable \"private_key_file\" {\n  default = \"~/.ssh/id_rsa\"\n}\n\nvariable \"port\" {\n  default = 8080\n}\nvariable \"max_size\" {\n  default = 3\n}\n```\n\n```yaml path=playbook.yaml\n- hosts: all\n  become: yes\n  roles:\n    - role: logdna\n```\n\n```yaml path=roles/logdna/tasks/main.yaml\n- include_tasks: ./install.yaml\n\n- include_tasks: ./configure.yaml\n\n- include_tasks: ./service.yaml\n```\n\n```yaml path=roles/logdna/tasks/install.yaml\n- name: LogDNA Agent Package Public Key\n  apt_key:\n    url: https://repo.logdna.com/logdna.gpg\n\n- name: LogDNA Agent Repository\n  apt_repository:\n    repo: deb https://repo.logdna.com stable main\n\n- name: Install LogDNA Agent\n  apt:\n    state: present\n    name: logdna-agent\n    allow_unauthenticated: true\n    update_cache: true\n```\n\n```yaml path=roles/logdna/tasks/configure.yaml\n- name: Setting LogDNA Ingestion Key\n  shell: \"logdna-agent -k {{ conf_key }}\"\n  when: conf_key != \"\"\n\n- name: Adding Log Directories\n  shell: \"logdna-agent -d {{ conf_logdir }}\"\n  when: conf_logdir != \"\"\n\n- name: Adding API Host\n  shell: \"logdna-agent -s LOGDNA_APIHOST={{ ubuntu_api_host }}\"\n  when: ubuntu_api_host != \"\"\n\n- name: Adding Log Host\n  shell: \"logdna-agent -s LOGDNA_LOGHOST={{ ubuntu_log_host }}\"\n  when: ubuntu_log_host != \"\"\n\n- name: Setting Tags for This Host\n  shell: \"logdna-agent -t {{ conf_tags }}\"\n  when: conf_tags != \"\"\n```\n\n```yaml path=roles/logdna/tasks/service.yaml\n- name: Activating LogDNA Agent Service\n  shell: \"update-rc.d logdna-agent defaults\"\n\n- name: Start LogDNA agent\n  service:\n    name: logdna-agent\n    enabled: true\n    state: \"{{ agent_service }}\"\n  when: (conf_key != \"\" and agent_service) or agent_service == \"stopped\"\n\n- debug:\n    msg: \"LogDNA Agent has {{ agent_service }}!\"\n  when: (conf_key != \"\" and agent_service) or agent_service == \"stopped\"\n```\n\n```yaml path=roles/logdna/defaults/main.yaml\nconf_tags: \"\"\nconf_logdir: \"\" # default to /var/log\n\nubuntu_api_host: api.us-south.logging.cloud.ibm.com\nubuntu_log_host: logs.us-south.logging.cloud.ibm.com\nagent_service: started\n```\n\n```hcl path=network.tf\nresource \"ibm_is_vpc\" \"iac_app_vpc\" {\n  name = \"${var.project_name}-${var.environment}-vpc\"\n}\n\nresource \"ibm_is_subnet\" \"iac_app_subnet\" {\n  name            = \"${var.project_name}-${var.environment}-subnet\"\n  vpc             = ibm_is_vpc.iac_app_vpc.id\n  zone            = \"us-south-1\"\n  ipv4_cidr_block = \"10.240.0.0/24\"\n}\n\nresource \"ibm_is_security_group\" \"iac_app_security_group\" {\n  name = \"${var.project_name}-${var.environment}-sg-public\"\n  vpc  = ibm_is_vpc.iac_app_vpc.id\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_app_security_group_rule_all_outbound\" {\n  group     = ibm_is_security_group.iac_app_security_group.id\n  direction = \"outbound\"\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_app_security_group_rule_tcp_http\" {\n  group     = ibm_is_security_group.iac_app_security_group.id\n  direction = \"inbound\"\n  tcp {\n    port_min = var.port\n    port_max = var.port\n  }\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_app_security_group_rule_tcp_ssh\" {\n  group     = ibm_is_security_group.iac_app_security_group.id\n  direction = \"inbound\"\n  tcp {\n    port_min = 22\n    port_max = 22\n  }\n}\n\nresource \"ibm_is_floating_ip\" \"iac_app_floating_ip\" {\n  name   = \"${var.project_name}-${var.environment}-ip-${format(\"%02s\", count.index)}\"\n  target = ibm_is_instance.iac_app_instance[count.index].primary_network_interface.0.id\n  count  = var.max_size\n}\n```\n\nOptionally, you can have a `ansible.cfg` to add more settings, if required.\n\n```ini\n[defaults]\nhost_key_checking = False\n```\n\nIt's required to have a `terraform.tfvars` file with the variables values or set them in environment variables. Use this example to create your `terraform.tfvars` file.\n\n```hcl path=terraform.tfvars\nproject_name = \"iac-ansible-test\"\nenvironment  = \"dev\"\nport         = \"8080\"\npublic_key   = \"ssh-rsa AAAAB3....\"\n\nlogdna_ingestion_key = \"e86611977....\"\nlogdna_api_host      = \"api.us-south.logging.cloud.ibm.com\"\nlogdna_log_host      = \"logs.us-south.logging.cloud.ibm.com\"\n```\n\n## Ansible from a Bastion Host\n\nThe previous design is functional but there is a dimension that may not be preferred in all cases. The Floating IPs and the open access to all the nodes from a single host, namely the workstation running Terraform and Ansible. A solution to avoid a requirement for Floating IPs is to have a Bastion Host. Access to the instances for configuration management can be done from a single host, the Bastion Host, and access controls are placed around this host which is used from the perimeter of the VPC to perform Ansible configuration management.\n\n![Ansible at Bastion Host](./images/IaC-Ansible_Design_2.png \"Running Ansible from Bastion Host\")\n\nThe Terraform and Ansible code for this has not been developed but it can be done in a later release to this pattern guide. Terraform first provisions the environment and also the Bastion Host with Ansible and send all playbooks to the host (if they are not already present). Then Terraform generates the inventory and uploads it to the host to finally execute Ansible remotely to configure all the hosts.\n\nAnother scenario for using a Bastion Host is in the use of IBM Cloud Schematics. In this example, we are executing Ansible from our host but with Schematics there is no host where it can install and execute Ansible, unless you create a instance or docker container to help you this task. Having a Bastion Host, Schematics can use it to execute Ansible and configure all the provisioned instances.\n\n## Clean up\n\nTo clean up everything just execute `terraform destroy`. As we are not using Schematics there is no need to run `ibmcloud` to make Schematics destroy the resources and the workspace.\n\n```bash\nterraform destroy\n```\n","fileAbsolutePath":"/Users/johandry/Workspace/ibm/att-cloudnative/ibmcloud-pattern-guide/src/pages/iac-conf-mgmt/ansible/index.mdx"}}}}