{"componentChunkName":"component---src-pages-iac-resources-container-index-mdx","path":"/iac-resources/container/","result":{"pageContext":{"frontmatter":{"title":"IaC for Containers Registry","description":"Use IaC to create and modify the IBM Container Registry","keywords":"terraform,ibm cloud,containers,registry,docker,images,icr,iks,k8s,kubernetes"},"relativePagePath":"/iac-resources/container/index.mdx","titleType":"page","MdxNode":{"id":"80e366d6-308f-5976-b4ab-2b6edc01d6a4","children":[],"parent":"966f71ce-6345-5f2f-949b-a5aaca8259d4","internal":{"content":"---\ntitle: IaC for Containers Registry\ndescription: Use IaC to create and modify the IBM Container Registry\nkeywords: 'terraform,ibm cloud,containers,registry,docker,images,icr,iks,k8s,kubernetes'\n---\n\n<PageDescription>\n\nAutomating the management of container services on IBM Cloud including the Container Registry and Kubernetes Services (IKS)\n\n</PageDescription>\n\n<AnchorLinks small>\n  <AnchorLink>Prerequisites</AnchorLink>\n  <AnchorLink>IBM Cloud Container Registry</AnchorLink>\n  <AnchorLink>IBM Cloud Kubernetes Service</AnchorLink>\n  <AnchorLink>IKS with Terraform</AnchorLink>\n  <AnchorLink>IKS with IBM Cloud Schematics</AnchorLink>\n  <AnchorLink>Deploy the Application</AnchorLink>\n  <AnchorLink>Persistent Volumes</AnchorLink>\n  <AnchorLink>External IBM Cloud Database</AnchorLink>\n  <AnchorLink>Deployment Troubleshooting</AnchorLink>\n  <AnchorLink>Final Code</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\n## Prerequisites\n\nThe steps in this pattern require the local workstation to be configured with the IBM Cloud CLI, CLI plugins for `container-service`, `container-registry` & `schematics`, the Terraform CLI, IBM Terrraform provider and a local installation of [Docker](https://docs.docker.com/get-docker/) . For more details on setting up the various CLI environments, see the [Setup Environment](/iac/setup-environment) chapter.\n\n## IBM Cloud Container Registry\n\nIBM Cloud Container Registry (ICR) is used to store, manage and deploy private container images in a highly available and scalable architecture. You can also set up your own image namespace and push container images to them. To learn more, see the [Container Registry](https://cloud.ibm.com/docs/Registry?topic=Registry-getting-started) documentation. There are no specific IaC steps required to enable the Container Registry, this is a capablity that is available to an IBM Cloud account without performing a service creation task.\n\nContainer images for IBM Cloud follow the [Open Container Initiative](https://www.opencontainers.org/) (OCI) standards to provide interoperability and flexibility in tooling for the container lifecycle. One well known tool for createing OCI-compliant images is `docker` which will be used for the examples in this pattern.\n\nThe `docker` command creates an image from a `Dockerfile`, which contains instructions to build the image. A `Dockerfile` might reference build artifacts in its instructions that are stored separately, such as an app, the app's configuration, and its dependencies. Images are typically stored in a registry that can either be accessible by the public (public registry) or set up with limited access for a small group of users (private registry). By using IBM Cloud Container Registry, only users with access to your IBM Cloud account through IAM can access your images.\n\nContinue using the same application from the previous patterns in order to have a simple container image that can be used with the IBM Container Registry and Kubernetes service. Create the first version of a `Dockerfile` with the following content in the directory `docker/1.0/` on this project.\n\n```Dockerfile path=docker/1.0/Dockerfile\nFROM node:13\n\nCOPY ./data/v1 /data\nRUN npm install -g json-server\n\nWORKDIR /app\n\nEXPOSE 8080\n\nCMD [ \"json-server\", \"--watch\", \"/data/db.min.json\", \"--port\", \"8080\", \"--host\", \"0.0.0.0\" ]\n```\n\nCopy to the `data/v1` folder the JSON database file `db.min.json` from the previous patterns. Now, build and test the container image locally using the `docker` command, the Dockerfile in `docker/1.0/` using the current directory as context because it needs the `data/v1/` directory with the JSON database.\n\n```bash\ndocker build -t movies:1.0 -f docker/1.0/Dockerfile .\ndocker images\n\ndocker run --name movies -d --rm -p 80:8080 -v $PWD/data/v1:/data movies:1.0\n\ncurl http://localhost/movies/675\n\ndocker stop $(docker ps -q --filter name=movies)\n```\n\nTo create an Container Registry namespace, use the IBM Cloud CLI with the `container-registry` plugin. Make sure you have the latest version installed and you have [setup the environment](/iac/setup-environment) correctly. Namespace names (like Docker Hub and other container repositories) must be unique for a container registry region, so substitute the name shown here with a unique one of your choosing.\n\nThe sub-command `namespace-add` will create the new namespace. The examples that follow will use `iac-registry` as the namespace:\n\n```bash\nibmcloud cr namespace-list\nibmcloud cr namespace-add iac-registry\n```\n\nIn order to push your local OCI image to the namespace registry, it must be tagged as: `REGION.icr.io/NAMESPACE/IMAGE:TAG`. Use the sub-command `region` to find the registry region you are targeting:\n\n```bash\nibmcloud cr region\n```\n\nContinuing with the example, the region is `us` so the registry is `us.icr.io`. The namespace is `iac-registry`, the image name is `movies` and the version tag `1.0`. The full tag would be: `us.icr.io/iac-registry/movies:1.0`. The image has already been created with the tag `movies` so to update, use the docker `tag` command:\n\n```bash\ndocker images\ndocker tag movies us.icr.io/iac-registry/movies:1.0\n```\n\n<InlineNotification>\n\n**Image tags**\n\nVisit the [Docker tag](https://docs.docker.com/engine/reference/commandline/tag/) documentation to find our more about image tags\n\n</InlineNotification>\n\nBefore pushing the image to the registry it's required to login with the IBM Cloud CLI `login` sub-command:\n\n```bash\nibmcloud cr login\n```\n\nThis command will set up the local `docker` CLI with a credentials object that allows it to communicate to the namespaces defined for your account in the current container registry region. After logging in, push the image with the Docker command `push`:\n\n```bash\ndocker push us.icr.io/iac-registry/movies:1.0\n```\n\nYou can check the image in the registry in different ways: (1) listing the images in the registry with the `ibmcloud cr images` command, or (2) using the `docker` command to pull the image, either from a different computer or locally deleting the image and pulling it down from the registry:\n\n```bash\n# Option 1:\nibmcloud cr images --restrict iac-registry\n\n# Option 2:\ndocker rmi us.icr.io/iac-registry/movies:1.0\ndocker pull us.icr.io/iac-registry/movies:1.0\ndocker images\n```\n\nWith the container image uploaded to the IBM Container Registry, you will be able to create Kubernetes deployments of the image by specifying the path to the fully qualified tag name `us.icr.io/iac-registry/movies:1.0` . Before doing this, you will need to create an IKS cluster.\n\n## IBM Cloud Kubernetes Service\n\nIBM Cloud Kubernetes Service (IKS) is a managed offering providing dedicated Kubernetes clusters to deploy and manage containerized apps. In this section you will create a Kubernetes cluster and deploy a simple API application. Examples will be provided using IBM Cloud CLI, Terraform and Schematics. The scope of this section is to cover creation of clusters and simple application deployment using IaC techniques. It will not cover deeper details for managing Kubernetes resources in general or broadly managing Kubernetes and deployments.\n\nTo create a Kubernetes cluster using the IBM Cloud CLI you need to specify parameters such as zone and worker node flavor. Discover these using the following commands. In this example, we are using Zone `us-south-1` and worker node flavor `mx2.4x32`.\n\n```bash\nibmcloud ks zone ls --provider vpc-gen2 --show-flavors\nZONE=us-south-1\n\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE\nFLAVOR=mx2.4x32\n```\n\nYou also need a VPC and Subnet for the Kubernetes cluster. If they do not yet exist, they may be created using the IBM Cloud CLI:\n\n```bash\n# VPC Name: iac-iks-vpc\nibmcloud is vpc-create iac-iks-vpc\nVPC_ID=$(ibmcloud is vpcs --json | jq -r \".[] | select(.name==\\\"iac-iks-vpc\\\").id\")\n\n# Subnet Name: iac-iks-subnet with 16 IP addresses.\nibmcloud is subnet-create iac-iks-subnet $VPC_ID --zone $ZONE --ipv4-address-count 16\nSUBNET_ID=$(ibmcloud is subnets --json | jq -r \".[] | select(.name==\\\"iac-iks-subnet\\\").id\")\n```\n\nAfter the VPC is created, the default security group will not have network access rules needed by the load balancers of the Kubernetes service to talk to the ingress controllers or other applications deployed as NodePort services. Update the default security group by adding the following rule.\n\n```bash\nDEFAULT_SG_ID=$(ibmcloud is vpc-default-security-group $VPC_ID --json | jq -r \".id\")\n\nibmcloud is security-group-rule-add $DEFAULT_SG_ID inbound tcp --port-min 30000 --port-max 32767\n```\n\nIf you already have a VPC and Subnets, get their IDs with the following `ibmcloud ks` sub-commands:\n\n```bash\nibmcloud ks vpcs --provider vpc-gen2        # VPC Name: iac-iks-vpc\nVPC_ID=$(ibmcloud ks vpcs --provider vpc-gen2 --json | jq -r '.[] | select(.name==\"iac-iks-vpc\").id')\n\nibmcloud ks subnets --provider vpc-gen2 --vpc-id $VPC_ID --zone $ZONE   # Subnet Name: iac-iks-subnet\nSUBNET_ID=$(ibmcloud ks subnets --provider vpc-gen2 --vpc-id $VPC_ID --zone $ZONE --json | jq -r '.[] | select(.name==\"iac-iks-subnet\").id')\n```\n\nThe available Kubernetes versions to install are listed with the command `ibmcloud ks versions`. For IKS on Gen2, use a kubernetes cluster version > `1.18`. With all input parameters defined, including a name and Kubernetes veyou are ready to create the cluster using the `cluster create` sub-command, like this:\n\n```bash\nNAME=iac-iks-cluster\nVERSION=1.18.3\n\nibmcloud ks cluster create vpc-gen2 \\\n  --name $NAME \\\n  --zone $ZONE \\\n  --vpc-id $VPC_ID \\\n  --subnet-id $SUBNET_ID \\\n  --flavor $FLAVOR \\\n  --version $VERSION \\\n  # --workers $N \\\n  # --entitlement cloud_pak \\\n  # --service-subnet $SUBNET_CIDR \\\n  # --pod-subnet $POD_CIDR \\\n  # --disable-public-service-endpoint \\\n```\n\nThe default values for the optional parameters are:\n\n- `N`: 1, this is a one worker node cluster.\n- `SUBNET_CIDR`: 172.21.0.0/16\n- `POD_CIDR`: 172.30.0.0/16\n- `disable-public-service-endpoint`: false\n\nTo identify your Kubernetes cluster status use the command `ibmcloud ks clusters`, wait a few minutes to have it up and running.\n\nWhen the Kubernetes cluster state is `normal` get the configuration to access the cluster using the following command:\n\n```bash\nibmcloud ks cluster config --cluster $NAME\n```\n\nNow you are ready to use the `kubectl` command, these are some initial commands:\n\n```bash\nkubectl cluster-info\nkubectl get nodes\n```\n\nYou can obtain more information of the cluster with the commands:\n\n```bash\nibmcloud ks worker ls --cluster $NAME\nibmcloud ks cluster get --cluster $NAME\n```\n\nTo know more read the [Kubernetes Service (IKS)](https://cloud.ibm.com/docs/containers?topic=containers-getting-started) documentation.\n\n## IKS with Terraform\n\nAll the same actions executed with the IBM Cloud CLI has to be done with Terraform, lets create a new `main.tf` file with the IBM Provisioner using Gen 2, the given region and the data source to get the info of the user selected resource group.\n\n```hcl path=main.tf\nprovider \"ibm\" {\n  generation = 2\n  region     = var.region\n}\n\ndata \"ibm_resource_group\" \"group\" {\n  name = var.resource_group\n}\n```\n\nThe `variables.tf` file defines the required variables above, the project name and environment to use them as prefix to name the resources, the code would be like this:\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"resource_group\" {\n  default = \"Default\"\n}\nvariable \"region\" {\n  default = \"us-south\"\n}\n```\n\nTo not have to enter the variables every time we execute terraform, lets add some variables value to the `terraform.tfvars` file. Make sure this file is appended to the `.gitignore` file.\n\n```hcl path=terraform.tfvars\nproject_name = \"iac-iks-test\"\nenvironment  = \"dev\"\n\n# Optional variables\nresource_group = \"Default\"\nregion         = \"us-south\"\n```\n\nThe IKS clusters needs a VPC, Subnet(s) and Security Group Rules(s) added to the default security group of the VPC. Just like we did using the IBM Cloud CLI let's create them allowing inbound traffic to ports 30000 - 32767 for the security group rules. Same as you did on [Network](/iac-resources/network) and [Compute](/iac-resources/compute) the number of subnets is defined by the number of zones provided by the user. Lets code this in the `network.tf` file and append the following variables to `variables.tf`.\n\n```hcl path=network.tf\nresource \"ibm_is_vpc\" \"iac_iks_vpc\" {\n  name = \"${var.project_name}-${var.environment}-vpc\"\n}\n\nresource \"ibm_is_subnet\" \"iac_iks_subnet\" {\n  count                    = local.max_size\n  name                     = \"${var.project_name}-${var.environment}-subnet-${format(\"%02s\", count.index)}\"\n  zone                     = var.vpc_zone_names[count.index]\n  vpc                      = ibm_is_vpc.iac_iks_vpc.id\n  total_ipv4_address_count = 256\n  resource_group           = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_iks_security_group_rule_tcp_k8s\" {\n  count     = local.max_size\n  group     = ibm_is_vpc.iac_iks_vpc.default_security_group\n  direction = \"inbound\"\n  remote    = ibm_is_subnet.iac_iks_subnet[count.index].ipv4_cidr_block\n\n  tcp {\n    port_min = 30000\n    port_max = 32767\n  }\n}\n```\n\n```hcl path=variables.tf\n  ...\nvariable \"vpc_zone_names\" {\n  type    = list(string)\n  default = [\"us-south-1\", \"us-south-2\", \"us-south-3\"]\n}\n\nlocals {\n  max_size = length(var.vpc_zone_names)\n}\n```\n\nLast but not least, create the `iks.tf` file to define the IKS cluster using the `ibm_container_vpc_cluster` resource.\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavor\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n```\n\nThe above code also takes the Kubernetes version, worker nodes flavor and number from the variables `k8s_version`, `flavor` and `workers_count` respectively, so lets add them to the `variables.tf` file.\n\n```hcl path=variables.tf\n  ...\nvariable \"flavor\" {\n  default = \"mx2.4x32\"\n}\nvariable \"workers_count\" {\n  default = 3\n}\nvariable \"k8s_version\" {\n  default = \"1.18.3\"\n}\n```\n\nThis will create a Kubernetes cluster of 3 worker nodes with 4 CPU and 32 Gb Memory. To know the available flavors in the zone, use the following IBM Cloud CLI command:\n\n```bash\nibmcloud ks zone ls --provider vpc-gen2 --show-flavors\n\n# Or\nZONE=us-south-1\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE\n```\n\nTo sort them by CPU and memory, use the same command with `sort`:\n\n```bash\nZONE=us-south-1\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE -s | sort -k2 -k3 -n\n```\n\nThe main input parameters of the `ibm_container_vpc_cluster` resource are listed in the following table:\n\n| Input parameter | Description |\n|---|---|\n| `name` | name of the cluster |\n| `vpc_id` | ID of the VPC that you want to use for your cluster |\n| `flavor` | flavor of the VPC worker node |\n| `zones` | nested block describing the zones of this VPC cluster |\n| `zones.name` | name of the zone |\n| `zones.subnet_id` | subnet in the zone to assign the cluster |\n| `worker_count` | (optional) number of worker nodes per zone in the default worker pool. Default value `1` |\n| `kube_version` | (optional) Kubernetes version, including the major.minor version. If not set, the default version from `ibmcloud ks versions` is used |\n| `resource_group_id` | (optional) ID of the resource group. Defaults to `default` |\n| `wait_till` | (optional) marks the creation of your cluster complete when the given stage is achieved, read below to know the available stages and how this can help you speed up the terraform execution |\n| `disable_public_service_endpoint` | (optional) disable the master public service endpoint to prevent public access. Defaults to `true` |\n| `pod_subnet` | (optional) subnet CIDR to provide private IP addresses for pods. Defaults to `172.30.0.0/16` |\n| `service_subnet` | (optional) subnet CIDR to provide private IP addresses for services. Defaults to `172.21.0.0/16` |\n| `tags` | (optional) list of tags to associate with your cluster |\n\nThe creation of a cluster can take some minutes to complete. To avoid long wait times, you can specify the stage when you want Terraform to mark the cluster resource creation as completed. The cluster creation might not be fully completed and continues to run in the background, however this can help you to continue with the code execution without waiting for the cluster to be fully created.\n\nTo set the waiting stage, use the `wait_till` with one of the following stages:\n\n- **MasterNodeReady**: Terraform marks the creation of your cluster complete when the cluster master is in a ready state.\n- **OneWorkerNodeReady**: Waits until the master and at least one worker node are in a ready state.\n- **IngressReady**: Waits until the cluster master and all worker nodes are in a ready state, and the Ingress subdomain is fully set up. This is the default value.\n\nThis would be enough to have an IKS cluster running. Just need to execute `terraform apply`, however lets create workers pools, one in each subnet or zone, using the resource `ibm_container_vpc_worker_pool`. Replace the code in `iks.tf` file for the following code and modify the variables used for the number of workers and its flavor.\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavors[0]\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  wait_till         = \"OneWorkerNodeReady\"\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n\nresource \"ibm_container_vpc_worker_pool\" \"iac_iks_cluster_pool\" {\n  count             = local.max_size - 1\n  cluster           = ibm_container_vpc_cluster.iac_iks_cluster.id\n  worker_pool_name  = \"${var.project_name}-${var.environment}-wp-${format(\"%02s\", count.index + 1)}\"\n  flavor            = var.flavors[count.index + 1]\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  worker_count      = var.workers_count[count.index + 1]\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[count.index + 1]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[count.index + 1].id\n  }\n}\n```\n\n```hcl path=variables.tf\nvariable \"flavors\" {\n  type    = list(string)\n  default = [\"mx2.4x32\", \"cx2.2x4\", \"cx2.4x8\"]\n}\nvariable \"workers_count\" {\n  type    = list(number)\n  default = [3, 2, 1]\n}\n```\n\nThe main input parameters for the `ibm_container_vpc_worker_pool` resource are similar to the parameters for `ibm_container_vpc_cluster` except for `worker_pool_name` which is used to name the pool, and `cluster` with the name or ID of the cluster set this pool.\n\nUsing a file `output.tf` helps us to get some useful information about the cluster through output variables, like so.\n\n```hcl path=output.tf\noutput \"cluster_id\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.id\n}\n\noutput \"cluster_name\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.name\n}\n\noutput \"entrypoint\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.public_service_endpoint_url\n}\n```\n\nNow everything is ready to create the cluster with the wellknown Terraform commands:\n\n```bash\nterraform plan\nterraform apply\n```\n\nAfter having the cluster ready, you can use the IBM Cloud CLI to get the cluster configuration to setup `kubectl`, like so:\n\n```bash\nibmcloud ks cluster config --cluster $(terraform output cluster_id)\n```\n\nEnjoy the new cluster, here are some basic initial commands to verify the cluster is working\n\n```bash\nkubectl cluster-info\nkubectl get nodes\nkubectl get pods -A\n```\n\n### A simpler IKS cluster\n\nFor simplicity and creation speed, lets modify the `terraform.tfvars` to have a simpler cluster with one single node. This will help us to have the cluster quicker.\n\n```hcl path=terraform.tfvars\nproject_name = \"iac-iks-small-OWNER\"\nenvironment  = \"dev\"\n\n# Optional variables\nresource_group = \"Default\"\nregion         = \"us-south\"\nvpc_zone_names = [\"us-south-1\"]\nflavors        = [\"mx2.4x32\"]\nworkers_count  = [1]\nk8s_version    = \"1.18.3\"\n```\n\nRemember to get a supported and the latest Kubernetes version from the output of the command `ibmcloud ks versions`, otherwise you may get an error like this one:\n\n```\nError: Request failed with status code: 400, ServerErrorResponse: {\"incidentID\":\"5a4a1a08a275eb6d-LAX\",\"code\":\"E0156\",\"description\":\"A previous patch was specified. Only the most recent patch for a particular minor version can be specified during cluster create.\",\"type\":\"Versions\",\"recoveryCLI\":\"To list supported versions, run 'ibmcloud ks versions'.\"}\n```\n\nExecuting `terraform plan & terraform apply` will get an IKS cluster up and running quicker than before.\n\n## IKS with IBM Cloud Schematics\n\nRunning this code with IBM Cloud Schematics is the same as with the other patterns. Create the `workspace.json` file adding the variables required for this code and replacing `OWNER` for your username or id, like this one:\n\n```json path=workspace.json\n{\n  \"name\": \"iac_iks_test\",\n  \"type\": [\n    \"terraform_v0.12\"\n  ],\n  \"description\": \"Sample workspace to test IBM Cloud Schematics. Deploys an web server on a VSI with a Hello World response\",\n  \"tags\": [\n    \"app:iac_iks_test\",\n    \"owner:OWNER\",\n    \"env:dev\"\n  ],\n  \"template_repo\": {\n    \"url\": \"https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/09-containers\"\n  },\n  \"template_data\": [{\n    \"folder\": \".\",\n    \"type\": \"terraform_v0.12\",\n    \"variablestore\": [{\n        \"name\": \"project_name\",\n        \"value\": \"iac-iks-test-OWNER\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"environment\",\n        \"value\": \"dev\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"region\",\n        \"value\": \"us-south\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"vpc_zone_names\",\n        \"value\": [\"us-south-1\", \"us-south-2\", \"us-south-3\"],\n        \"type\": \"list(string)\"\n      },\n      {\n        \"name\": \"flavors\",\n        \"value\": [\"mx2.4x32\", \"cx2.2x4\", \"cx2.4x8\"],\n        \"type\": \"list(string)\"\n      },\n      {\n        \"name\": \"workers_count\",\n        \"value\": [3, 2, 1],\n        \"type\": \"list(number)\"\n      },\n      {\n        \"name\": \"k8s_version\",\n        \"value\": \"1.18.3\",\n        \"type\": \"string\"\n      }\n    ]\n  }]\n}\n```\n\nTo create the workspace using the IBM Cloud CLI execute the following commands:\n\n```bash\nibmcloud schematics workspace new --file workspace.json\nibmcloud schematics workspace list          # Identify the WORKSPACE_ID\nWORKSPACE_ID=\n```\n\nSet the variable `WORKSPACE_ID` because it'll be used several times. Then plan and apply the code like so.\n\n```bash\nibmcloud schematics plan --id $WORKSPACE_ID  # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n\nibmcloud schematics apply --id $WORKSPACE_ID # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n```\n\nNote the execution of apply will take some time, so check the logs either with the IBM Cloud CLI command or using the IBM Cloud Web Console. When the cluster is ready, you can use the IBM Cloud CLI to get the cluster configuration to setup `kubectl` and validate the cluster is accesible:\n\n```bash\nCLUSTER_ID=$(ibmcloud schematics workspace output --id $WORKSPACE_ID --json | jq -r '.[].output_values[].cluster_id.value')\nibmcloud ks cluster config --cluster $CLUSTER_ID\n\nkubectl cluster-info\nkubectl get nodes\nkubectl get pods -A\n```\n\n## Deploy the Application\n\nTo deploy the previously built Docker image version 1.0 we use the Kubernetes API and resources. Lets create a deployment file either by getting it from the following example or generating it with kubectl generators, like so:\n\n```bash\nmkdir kubernetes\nkubectl create deployment movies --image=us.icr.io/iac-registry/movies:1.0 --dry-run=client -o yaml > kubernetes/deployment.yaml\nkubectl expose deployment movies --port=80 --target-port=8080 --type=LoadBalancer --dry-run=client -o yaml > kubernetes/service.yaml\n```\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      containers:\n        - image: us.icr.io/iac-registry/movies:1.0\n          name: movies\n```\n\n```yaml path=kubernetes/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  ports:\n    - name: \"http\"\n      port: 80\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    app: movies\n  type: LoadBalancer\n```\n\nTo deploy the application execute the `kubectl apply` command like this:\n\n```bash\nkubectl apply -f kubernetes/deployment.yaml\nkubectl apply -f kubernetes/service.yaml\n\nkubectl get deployment movies\nkubectl get svc movies\n```\n\nTo validate the application you need to get the external IP or DNS to access the application executing the following code. You may have to wait a few minutes until the Load Balancer is ready. You can checkt thest status again using `kubectl get svc movies`.\n\n```bash\nwatch kubectl get svc movies\n\nADDRESS=$(kubectl get svc movies -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n\ncurl $ADDRESS/movies/675\n```\n\nIn a real application, it's quite common to have new or changing data. In this example, such a change to the JSON database would require a new image. If this happens very often it becomes very efficient. To address this inflexible model, you can put the JSON database in a ConfigMap. Create the `cm.yaml` file to define the ConfigMap with the content of the JSON file `data/v1/db.min.json` using this command\n\n```bash\nkubectl create configmap movies-db --from-file=./data/v1/db.min.json --dry-run=client -o yaml > kubernetes/cm.yaml\n```\n\nOr, edit the file yourself with the following content.\n\n```yaml path=kubernetes/cm.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: movies-db\ndata:\n  db.min.json: |\n    {\"movies\":[ ... HERE GOES THE JSON FILE ... ]}\n```\n\nAnd apply the code to the cluster using `kubectl`, like this.\n\n```bash\nkubectl apply -f kubernetes/cm.yaml\nkubectl get cm\n```\n\nTo make the pod access the JSON file you need to modify the Pod definition inside the deployment. Modify the `deployment.yaml` file to add the `volumes` and `volumeMounts` specifications, like so.\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\n  ...\n    spec:\n      volumes:\n        - name: db-volume\n          configMap:\n            name: movies-db\n      containers:\n        - image: us.icr.io/iac-registry/movies:1.0\n          name: movies\n          volumeMounts:\n            - name: db-volume\n              mountPath: /data\n```\n\nUpdate the new Pod applying the code, then verify it was sucessfuly applied using these commands.\n\n```bash\nkubectl apply -f kubernetes/v1.0/deployment.yaml\nkubectl get deployments,pods\n```\n\nThe application should be running as usual:\n\n```bash\nADDRESS=$(kubectl get svc movies -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\ncurl $ADDRESS/movies/675\n```\n\n<InlineNotification>\n\n**What about the JSON DB in the container?**\n\nIdeally you need to remove the line that copy the JSON database file into the image from the Dockerfile (`COPY ./data/v1 /data`), then re-build and push the image to ICR. However, this is a simple example and we will modify this application docker image even more.\n\nWith the new ConfigMap the current `/data/db.min.json` file in the container is replaced by the one from the ConfigMap. You can verify it's there with the execution of `kubectl exec --stdin --tty movies-89977dc9-7gdpd -- cat /data/db.min.json`, replacing `movies-89977dc9-7gdpd` for the name of the movies Pod.\n\n</InlineNotification>\n\nTo double check, modify the ConfigMap updating a movie or modifying the database, then access the application using `curl`. The instructions when the ConfigMap is modified are as follows.\n\n1. Modify the ConfigMap in the file `cm.yaml`\n2. Applying the changes with the command `kubectl apply -f kubernetes/cm.yaml`\n3. Delete the running pods so the Replica Set create a new pod using the new JSON database. Identify the Pod name using `kubectl get pods` then use the command `kubectl delete pod <Movies Pod Name>`\n4. Verify the change with `curl $ADDRESS/movies/`.\n\n## Persistent Volumes\n\nIn the version 1.0 of the application, the JSON database was in the ephemeral container, this may not be good practice in general so let's migrate the database to a persistent storage such as IBM Cloud Block Storage for VPC. This storage provides hypervisor-mounted, high-performance data storage for your VSI or IKS nodes that you can provision within a VPC.\n\nLet's start creating the file `pvc.yaml` with the definition of a [Persisten Volume Claim](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim) with 1Gb and the profile `ibmc-vpc-block-5iops-tier`.\n\n```yaml path=kubernetes/pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: movies\nspec:\n  storageClassName: ibmc-vpc-block-general-purpose\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n```\n\nBefore use the Persistent Volume Claim (PVC) apply the changes, it has to be ready before being used.\n\n```bash\nkubectl apply -f kubernetes/pvc.yaml\nkubectl get pvc movies\n```\n\nTo use this volume we need to modify the Pod specification in the deployment, open the `kubernetes/deployment.yaml` file to add the `volumes` and `volumeMounts` specifications.\n\nHowever, these changes don't put the initial JSON database into the volume yet. There may be different ways to do this, a possible option is to the use of [Init Containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) to dump the initial JSON database into the volume, but this option cannot be used because the volume access mode is **ReadWriteOnce** which only allows one container to access the volume at a time. Other option, and the one we will implement, is to make the Docker container copy the initial database into the volumen if there isn't any yet. The initial JSON file is provided with a ConfigMap, let's add it just like we did in the previous section.\n\nThe `deployment.yaml` file will be like this.\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      volumes:\n        - name: movies-volume\n          persistentVolumeClaim:\n            claimName: movies\n        - name: db-volume\n          configMap:\n            name: movies-db\n      containers:\n        - image: us.icr.io/iac-registry/movies:1.1\n          name: movies\n          volumeMounts:\n            - name: db-volume\n              mountPath: /data/init\n            - name: movies-volume\n              mountPath: \"/data\"\n```\n\nAs you can see the image version is different, it is `1.1`. In the new Dockerfile we remove the line `COPY ./data/v1 /data` and add the line `VOLUME /data`. Also, instead of executing the `json-server` command, it runs a script to copy the database to the right location. The new Dockerfile, to be tagged with version `1.1`, is like this.\n\n```dockerfile path=docker/1.1/Dockerfile\nFROM node:13\n\nRUN npm install -g json-server\nADD entrypoint.sh /entrypoint.sh\n\nWORKDIR /app\nVOLUME /data\n\nEXPOSE 8080\n\nENTRYPOINT [ \"/entrypoint.sh\" ]\n\nCMD [ \"\" ]\n```\n\nAnd the script to be executed as entrypoint executes the input with `exec \"$@\"`, however if no command is passed in it'll execute `json-server` after initialize the JSON database file. This script is like follows.\n\n```bash path=docker/1.1/entrypoint.sh\n#!/bin/bash\n\nif [[ -n \"$@\" ]]; then\n  exec \"$@\"\n  exit $?\nfi\n\nport=\"8080\"\nhost=\"0.0.0.0\"\ninit_db='{\"movies\": []}'\ninit_db_file=/data/init/db.min.json\ndb_file=/data/db.json\n\ninit_database() {\n  # If the DB is there and contain movies, there is nothing to do\n  [[ -e $db_file ]] && grep -q '\"movies\"' $db_file && echo \"A database file with movies already exists.\" && return\n\n  # If the init DB file is there, copy it to the DB file and return if succeed\n  [[ -e $init_db_file ]] && cp $init_db_file $db_file && echo \"initialized the database with the database file $init_db_file.\" && return\n\n  # if everything failed, initiallize the DB with empty list of movies\n  echo \"initializing the database with an empty database.\"\n  echo $init_db > $db_file\n}\n\ninit_database\n\necho \"Starting json-server on $host:$port watching for DB file $db_file\"\njson-server --port $port --host $host --watch $db_file\n```\n\nThis new image has to be built, tagged and pushed to ICR very similar to like we did with the initial version and that's what we will do in a moment. This time the context of `docker build` change to `docker/1.1` because we don't use the `./data` directory and we use the `docker/1.1/entrypoint.sh` script.\n\n```bash\ndocker build -t us.icr.io/iac-registry/movies:1.1 -f docker/1.1/Dockerfile docker/1.1\ndocker push us.icr.io/iac-registry/movies:1.1\n\nibmcloud cr images --restrict iac-registry\n```\n\nApply all files and verify the new changes executing the following commands.\n\n```bash\nkubectl apply -f kubernetes/pvc.yaml\nkubectl apply -f kubernetes/cm.yaml\nkubectl apply -f kubernetes/deployment.yaml\n\nkubectl get pvc movies\nkubectl get cm movies\nkubectl get deployment movies\nkubectl get pods\n\nADDRESS=$(kubectl get svc movies -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\ncurl $ADDRESS/movies/83\n```\n\nHaving the JSON Database in a persistent volume we can modify the database and the changes will persist the next time we deploy the application or restart the container. Having the following movie to add:\n\n```json path=data/v1/new_movie.json\n{\n  \"id\": \"32\",\n  \"title\": \"13 Assassins\",\n  \"originalTitle\": \"十三人の刺客\",\n  \"contentRating\": \"R\",\n  \"summary\": \"Cult director Takashi Miike (Ichi the Killer, Audition) delivers a bravado period action film set at the end of Japan’s feudal era. 13 Assassins - a “masterful exercise in cinematic butchery” (New York Post) - is centered around a group of elite samurai who are secretly enlisted to bring down a sadistic lord to prevent him from ascending to the throne and plunging the country into a war torn future.\",\n  \"rating\": \"9.6\",\n  \"audienceRating\": \"8.8\",\n  \"year\": \"2011\",\n  \"tagline\": \"Take up your sword.\",\n  \"duration\": \"7505063\",\n  \"originallyAvailableAt\": \"2011-07-05\",\n  \"addedAt\": \"1351391906\",\n  \"updatedAt\": \"1546942538\",\n  \"audienceRatingImage\": \"rottentomatoes://image.rating.upright\",\n  \"hasPremiumPrimaryExtra\": \"1\",\n  \"ratingImage\": \"rottentomatoes://image.rating.ripe\",\n  \"genre\": \"Action & Adventure\",\n  \"director\": \"Takashi Miike\",\n  \"writer\": [\"Kaneo Ikegami\", \"Daisuke Tengan\"],\n  \"country\": \"Japan\",\n  \"cast\": [\"Yusuke Iseya\", \"Kôji Yakusho\"]\n}\n```\n\nLets add the new movie using `curl`, scale the deployment to zero containers, then back to one and verify the new movie is still there.\n\n```bash\ncurl -X POST -H \"Content-Type: application/json\" -d@data/v1/new_movie.json $ADDRESS/movies\ncurl $ADDRESS/movies/32\n\nkubectl scale deployment movies --replicas=0\nkubectl get deployments movies\nkubectl get pods\nkubectl get pv,pvc\n\nkubectl scale deployment movies --replicas=1\nkubectl get deployments movies\nwatch kubectl get pods\n\n# When the pod is running:\ncurl $ADDRESS/movies/32\n```\n\nTo learn more about the storage provided to the persistent volume claim, see the [Block Storage for VPC](https://cloud.ibm.com/docs/containers?topic=containers-vpc-block) documentation.\n\n## External IBM Cloud Database\n\nThis section provides an example of deploying the Python API application used in the [Cloud Databases](/iac-resources/services) pattern also in the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [08_cloud-services/app](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/08_cloud-services/app).\n\nThis change requires more major changes to the Docker container so it's going to make sense to bump the tag to `2.0`. In the following Dockerfile we use a multi-stage build to reduce the size of the final Docker image. The `build` stage use Python VirtualEnv to install all the required packages then they are copied to the `app` image which is used to execute the API application.\n\n```dockerfile path=docker/2.0/Dockerfile\nFROM python:3.7-slim AS build\n\nRUN apt-get update && \\\n  apt-get install -y --no-install-recommends build-essential gcc && \\\n  pip install --upgrade pip && \\\n  pip install pip-tools\nRUN python -m venv /opt/venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCOPY app.py .\nCOPY import.py .\nCOPY requirements.txt requirements.in\nRUN pip-compile requirements.in > requirements.txt && \\\n  pip-sync && \\\n  pip install -r requirements.txt\n\nFROM python:3.7-slim AS app\nCOPY --from=build /opt/venv /opt/venv\n\nCOPY app.py .\nCOPY import.py .\nRUN chmod +x app.py import.py\n\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCMD [ \"python\", \"app.py\" ]\n```\n\nJust as with the previous versions, let's build and push the container using the following commands:\n\n```bash\ndocker build -t us.icr.io/iac-registry/movies:2.0 -f docker/2.0/Dockerfile docker/2.0\ndocker push us.icr.io/iac-registry/movies:2.0\n\nibmcloud cr images --restrict iac-registry\n```\n\nWe also need the IBM Cloud Database, with the following Terraform code in the `db.tf` file copied from the [Cloud Databases](/iac-resources/services) pattern, like so.\n\n```hcl path=db.tf\nresource \"ibm_database\" \"iac_app_db_instance\" {\n  name              = var.db_name\n  plan              = var.db_plan\n  location          = var.region\n  service           = \"databases-for-mongodb\"\n  resource_group_id = data.ibm_resource_group.group.id\n\n  adminpassword                = var.db_admin_password\n  members_memory_allocation_mb = var.db_memory_allocation\n  members_disk_allocation_mb   = var.db_disk_allocation\n}\n```\n\nThis file also requires addition of the following input variables to the `variables.tf` file and output variables to the `output.tf` file:\n\n```hcl path=variables.tf\nvariable \"db_plan\" {\n  default = \"standard\"\n}\nvariable \"db_name\" {\n  default = \"moviedb\"\n}\nvariable \"db_admin_password\" {\n  default = \"inSecurePa55w0rd\"\n}\nvariable \"db_memory_allocation\" {\n  default = \"3072\"\n}\nvariable \"db_disk_allocation\" {\n  default = \"61440\"\n}\n```\n\n```hcl path=output.tf\noutput \"db_connection_string\" {\n  value = ibm_database.iac_app_db_instance.connectionstrings.0.composed\n}\noutput \"db_connection_certbase64\" {\n  value = ibm_database.iac_app_db_instance.connectionstrings.0.certbase64\n}\noutput \"db_admin_userid\" {\n  value = ibm_database.iac_app_db_instance.adminuser\n}\noutput \"db_id\" {\n  value = ibm_database.iac_app_db_instance.id\n}\noutput \"db_password\" {\n  value = var.db_admin_password\n}\n```\n\nTo get it running, execute the `plan` and `apply` Terraform commands.\n\n```bash\nterrform plan\nterrform apply\n```\n\nBefore deploying the container to our Kubernetes cluster, do some local testing using just Docker. Execute the following commands to run the container locally, mounting the local directory `./data/v2` in a volume on the container directory `/data/init/` so the application can reach the `db.min.json` file with the initial values of the database. The initial database `db.min.json` file is different to the one used for version 1 because the `id` field is not required. Also, to allow the container to reach the IBM Cloud MongoDB Database that was created, populate environment variables with values from the Terraform output variables.\n\n The following commands will: create all the application input data, initialize the database, run the container with the API application and finally query the application with `curl`.\n\n```bash\nmkdir ./secret\nterraform output db_connection_certbase64 | base64 --decode > ./secret/db_ca.crt\n\nexport PASSWORD=$(terraform output db_password)\nexport APP_MONGODB_URI=$(terraform output db_connection_string)\nexport APP_PORT=8080\nexport APP_SSL_CA_CERT=\"/secret/db_ca.crt\"\n\ndocker run --rm \\\n  --name init-movies \\\n  -v $PWD/data/v2:/data/init \\\n  -v $PWD/secret:/secret \\\n  -e APP_SSL_CA_CERT=$APP_SSL_CA_CERT \\\n  -e PASSWORD=$PASSWORD \\\n  -e APP_MONGODB_URI=$APP_MONGODB_URI \\\n  us.icr.io/iac-registry/movies:2.0 python import.py\n\ndocker run --rm \\\n  --name movies \\\n  -p 80:$APP_PORT \\\n  -v $PWD/secret:/secret \\\n  -e APP_SSL_CA_CERT=$APP_SSL_CA_CERT \\\n  -e PASSWORD=$PASSWORD \\\n  -e APP_MONGODB_URI=$APP_MONGODB_URI \\\n  -e APP_PORT=$APP_PORT \\\n  us.icr.io/iac-registry/movies:2.0\n\ncurl http://localhost/api/healthcheck\ncurl http://localhost/api/movies\n\n\nid=$(curl -s \"http://localhost/api/movies\" | jq -r '.[0]._id | .[\"$oid\"]')\ncurl \"http://localhost/api/movies/$id\"\n```\n\nTo wipe out the database use the same docker container but instead run the `python import.py` command with the `--empty` parameter, like so:\n\n```bash\ndocker run --rm \\\n  --name drop-movies \\\n  -v $PWD/secret:/secret \\\n  -e APP_SSL_CA_CERT=$APP_SSL_CA_CERT \\\n  -e PASSWORD=$PASSWORD \\\n  -e APP_MONGODB_URI=$APP_MONGODB_URI \\\n  us.icr.io/iac-registry/movies:2.0 python import.py --empty\n```\n\nThe current Database was created with a public endpoint (it's public by default) and considering the current IKS cluster is private you may also want to migrate this database to be private as well. It was not created from the very begining because you may want to test the database from your computer, ilke we did running Docker locally.\n\nTo set this database with a private endpoint add the parameter `service_endpoints = \"private\"` to the `ibm_database.iac_app_db_instance` located in the `db.tf` file, Like so:\n\n```hcl path=db.tf\nresource \"ibm_database\" \"iac_app_db_instance\" {\n  name              = var.db_name\n  plan              = var.db_plan\n  location          = var.region\n  service           = \"databases-for-mongodb\"\n  resource_group_id = data.ibm_resource_group.group.id\n  service_endpoints = \"private\"\n\n  adminpassword                = var.db_admin_password\n  members_memory_allocation_mb = var.db_memory_allocation\n  members_disk_allocation_mb   = var.db_disk_allocation\n}\n```\n\nTo apply this migration the database has to be deleted first, applying this change now with Terrafor will cause an error because it's a parameter that cannot be set and modify the database. So, delete the database using the `destroy` Terraform command targetting the database, then apply the changes.\n\n```bash\nterraform destroy -target ibm_database.iac_app_db_instance\n\nterraform apply\n```\n\nIf the local docker container works, everything is ready to work on the Kubernetes deployment. A new ConfigMap is required with the initial data for MongoDB, another ConfigMap is required with the environment variables to have access to the database, and finally, two Secrets are required, the first [Secret](https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data) is used to store the database CA certificate and the second [Secret](https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables) stores the DB admin password. Create the ConfigMaps and Secrets with the following commands\n\n```bash\nexport PASSWORD=$(terraform output db_password)\nexport APP_MONGODB_URI=$(terraform output db_connection_string)\nexport APP_SSL_CA_CERT=\"/secret/db_ca.crt\"\n\nkubectl create configmap movies-db \\\n  --from-file=./data/v2/db.min.json \\\n  --dry-run=client -o yaml > kubernetes/cm.yaml\n\nkubectl create configmap config \\\n  --from-literal=APP_SSL_CA_CERT=$APP_SSL_CA_CERT \\\n  --from-literal=APP_MONGODB_URI=$APP_MONGODB_URI \\\n  --dry-run=client -o yaml > kubernetes/config.yaml\n\nkubectl create secret generic db-admin-password \\\n  --from-literal=PASSWORD=$PASSWORD \\\n  --dry-run=client -o yaml > kubernetes/db_admin_password.yaml\n\nmkdir ./secret\nterraform output db_connection_certbase64 | base64 --decode > ./secret/db_ca.crt\nkubectl create secret generic db-ssl-ca-cert \\\n  --from-file=./secret/db_ca.crt \\\n  --dry-run=client -o yaml > kubernetes/db_ca_cert.yaml\nrm -rf ./secret\n```\n\nThe new `deployment.yaml` uses the ConfigMap to initialize the database but this time with an [Init Container](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) to execute the `import.py` python script. Both containers get the ConfigMap with the environment variables and the two Secrets.\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      volumes:\n        - name: db-volume\n          configMap:\n            name: movies-db\n        - name: db-ca-cert\n          secret:\n            secretName: db-ssl-ca-cert\n      initContainers:\n        - name: init-db\n          image: us.icr.io/iac-registry/movies:2.0\n          command: [\"python\", \"import.py\"]\n          envFrom:\n            - secretRef:\n                name: db-admin-password\n            - configMapRef:\n                name: config\n          volumeMounts:\n            - name: db-volume\n              mountPath: /data/init\n            - name: db-ca-cert\n              mountPath: \"/secret\"\n              readOnly: true\n      containers:\n        - image: us.icr.io/iac-registry/movies:2.0\n          name: movies\n          envFrom:\n            - secretRef:\n                name: db-admin-password\n            - configMapRef:\n                name: config\n          volumeMounts:\n            - name: db-ca-cert\n              mountPath: \"/secret\"\n              readOnly: true\n```\n\nAll is set to apply the new Deployment, Services, Secrets and ConfigMaps with the following commands.\n\n```bash\nkubectl apply -f kubernetes/cm.yaml\nkubectl apply -f kubernetes/config.yaml\nkubectl apply -f kubernetes/db_admin_password.yaml\nkubectl apply -f kubernetes/db_ca_cert.yaml\nkubectl apply -f kubernetes/deployment.yaml\nkubectl apply -f kubernetes/service.yaml\n```\n\nThe PersistentVolumeClaim is not longer required for this version, so you may delete it with this command.\n\n```bash\nkubectl delete pvc movies\nwatch kubectl get pv,pvc\n```\n\n<InlineNotification>\n\n**VRF enablement**\n\nVRF stands for **Virtual Route Framework**, it's a way for routers to partition route tables logically, sort of like VLANS for L3.\n\nThe new generation of private connectivity for VPC requires that each account switch over to VRF, giving all the networks in the account it's own routing context.\n\nIf at this point you are having troubles to connect to the database from the container chances are you need to enable VRF to your account. Submit a ticket to the IBM Cloud Support team for VRF enablement.\n\nRead this [documentation](https://cloud.ibm.com/docs/resources?topic=resources-private-network-endpoints) to know more.\n</InlineNotification>\n\nOne of the differences with the version 1 is that this new architecture allows us to scale up the replicas of the pods. You can try it with these commands:\n\n```bash\nkubectl scale deployment movies --replicas=5\n\nwatch kubectl get po,deploy,rs\n```\n\nTo verify the application is working, use the same `curl` commands we've been using.\n\n```bash\nADDRESS=$(kubectl get svc movies -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n\n# Get all movies\ncurl $ADDRESS/api/movies\n\n# Get a movie\nid=$(curl -s \"http://$ADDRESS/api/movies\" | jq -r '.[0]._id | .[\"$oid\"]')\ncurl \"http://$ADDRESS/api/movies/$id\" | jq\n\n# Create a movie\nid=$(curl -s -X POST -H \"Content-Type: application/json\" -d@data/v2/new_movie.json $ADDRESS/api/movies | jq -r '.id')\ncurl $ADDRESS/api/movies/$id\n\n# Update a movie\nsed 's/13 Assassins/14 Assassins/' data/v2/new_movie.json > data/v2/update_movie.json\ncurl -s -X PUT -H \"Content-Type: application/json\" -d@data/v2/update_movie.json $ADDRESS/api/movies/$id\ncurl $ADDRESS/api/movies/$id\nrm data/v2/update_movie.json\n\n# Delete a movie\ncurl -X DELETE $ADDRESS/api/movies/$id\ncurl $ADDRESS/api/movies/$id\ncurl -s $ADDRESS/api/movies | grep '14 Assassins'\n```\n\nThere is more that you can do with this sample application, you can:\n\n- Add resource limits to the Pod so is can be scaled up or down automatically\n- Deploy a new Angular, React or Vue application to visualize the movies\n- Deploy a container with your own MongoDB to use it instead of the IBM Cloud MongoDB\n\n## Deployment Troubleshooting\n\nIf you have any problem with the validation and want to [debug or troubleshot it](https://kubernetes.io/docs/tasks/debug-application-cluster/), use the following commands to identify the root cause.\n\n```bash\nkubectl get deploy,po\n\npod_id=$(kubectl get deploy,po | grep pod/movies | head -1 | awk '{print $1}')\n\nkubectl describe pod $pod_id\nkubectl logs $pod_id\nkubectl logs $pod_id init-db\n\nkubectl exec $pod_id --container init-db -- cat /secret/db_ca.crt\nkubectl exec $pod_id --container movies -- env | grep APP\n```\n\nIf you need to login to a container replace the `command` in the deployment for `command: [\"/bin/sh\", \"-c\", \"while true; do sleep 1000;done\"]` so it doesn't fail and you have time to execute a remote bash session.\n\n```bash\nkubectl exec --stdin --tty $pod_id -- /bin/bash\nkubectl exec --stdin --tty $pod_id --container init-db  -- /bin/bash\n```\n\nIf you need to connect to the database and it has a private endpoint, deploy a MongoDB container with the `mongo` client and the required ConfigMaps and Secrets to connect to the database. Push to ICR the official MongoDB image and execute the `kubectl` generator, then modify the output file to include the ConfigMap and Secrets:\n\n```bash\ndocker pull mongo:bionic\ndocker tag mongo:bionic us.icr.io/iac-registry/mongo:bionic\ndocker push us.icr.io/iac-registry/mongo:bionic\n\nkubectl create deployment mongo --image us.icr.io/iac-registry/mongo:bionic --dry-run=client -o yaml > kubernetes/mongo.yaml\n```\n\n```yaml path=kubernetes/mongo.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo\n  name: mongo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n        - image: us.icr.io/iac-registry/mongo:bionic\n          name: mongo\n          envFrom:\n            - secretRef:\n                name: db-admin-password\n            - configMapRef:\n                name: config\n          volumeMounts:\n            - name: db-volume\n              mountPath: /data/init\n            - name: db-ca-cert\n              mountPath: \"/secret\"\n              readOnly: true\n```\n\nThen login into the container and run the `mongo` client like so.\n\n```bash\nkubectl exec --stdin --tty $(kubectl get pods | grep mongo | awk '{print $1}')  -- /bin/bash\n# verify the enviroment variable APP_MONGODB_URI has the password from $PASSWORD\nAPP_MONGODB_URI=$(echo $APP_MONGODB_URI | sed -e \"s/\\$PASSWORD/$PASSWORD/\" -e \"s/ibmclouddb/moviesdb/\")\necho $APP_MONGODB_URI\nmongo $APP_MONGODB_URI --tls --tlsCAFile $APP_SSL_CA_CERT\n```\n\nOr, instead, just execute this one-liner:\n\n```bash\nkubectl exec --stdin --tty $(kubectl get pods | grep mongo | awk '{print $1}')  -- /bin/bash -c 'mongo $(echo $APP_MONGODB_URI | sed -e \"s/\\$PASSWORD/$PASSWORD/\" -e \"s/ibmclouddb/moviesdb/\") --tls --tlsCAFile $APP_SSL_CA_CERT'\n```\n\n## Final Code\n\nAll the code used for this pattern is located and available to download in the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [09-containers](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/09-containers). The main files for the latest version (version 2) of the application are:\n\n```hcl path=network.tf\nresource \"ibm_is_vpc\" \"iac_iks_vpc\" {\n  name = \"${var.project_name}-${var.environment}-vpc\"\n}\n\nresource \"ibm_is_subnet\" \"iac_iks_subnet\" {\n  count                    = local.max_size\n  name                     = \"${var.project_name}-${var.environment}-subnet-${format(\"%02s\", count.index)}\"\n  zone                     = var.vpc_zone_names[count.index]\n  vpc                      = ibm_is_vpc.iac_iks_vpc.id\n  total_ipv4_address_count = 256\n  resource_group           = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_iks_security_group_rule_tcp_k8s\" {\n  count     = local.max_size\n  group     = ibm_is_vpc.iac_iks_vpc.default_security_group\n  direction = \"inbound\"\n  remote    = ibm_is_subnet.iac_iks_subnet[count.index].ipv4_cidr_block\n\n  tcp {\n    port_min = 30000\n    port_max = 32767\n  }\n}\n```\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavors[0]\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  wait_till         = \"OneWorkerNodeReady\"\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n\nresource \"ibm_container_vpc_worker_pool\" \"iac_iks_cluster_pool\" {\n  count             = local.max_size - 1\n  cluster           = ibm_container_vpc_cluster.iac_iks_cluster.id\n  worker_pool_name  = \"${var.project_name}-${var.environment}-wp-${format(\"%02s\", count.index + 1)}\"\n  flavor            = var.flavors[count.index + 1]\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  worker_count      = var.workers_count[count.index + 1]\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[count.index + 1]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[count.index + 1].id\n  }\n}\n```\n\n```hcl path=db.tf\nresource \"ibm_database\" \"iac_app_db_instance\" {\n  name              = var.db_name\n  plan              = var.db_plan\n  location          = var.region\n  service           = \"databases-for-mongodb\"\n  resource_group_id = data.ibm_resource_group.group.id\n  service_endpoints = \"private\"\n\n  adminpassword                = var.db_admin_password\n  members_memory_allocation_mb = var.db_memory_allocation\n  members_disk_allocation_mb   = var.db_disk_allocation\n}\n```\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"resource_group\" {\n  default = \"Default\"\n}\nvariable \"region\" {\n  default = \"us-south\"\n}\nvariable \"vpc_zone_names\" {\n  type    = list(string)\n  default = [\"us-south-1\", \"us-south-2\", \"us-south-3\"]\n}\nvariable \"flavors\" {\n  type    = list(string)\n  default = [\"cx2.2x4\", \"cx2.4x8\", \"cx2.8x16\"]\n}\nvariable \"workers_count\" {\n  type    = list(number)\n  default = [3, 2, 1]\n}\nvariable \"k8s_version\" {\n  default = \"1.18.3\"\n}\n\nlocals {\n  max_size = length(var.vpc_zone_names)\n}\n\nvariable \"db_plan\" {\n  default = \"standard\"\n}\nvariable \"db_name\" {\n  default = \"moviedb\"\n}\nvariable \"db_admin_password\" {\n  default = \"inSecurePa55w0rd\"\n}\nvariable \"db_memory_allocation\" {\n  default = \"3072\"\n}\nvariable \"db_disk_allocation\" {\n  default = \"61440\"\n}\n```\n\n```hcl path=output.tf\noutput \"cluster_id\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.id\n}\noutput \"cluster_name\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.name\n}\noutput \"entrypoint\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.public_service_endpoint_url\n}\n\noutput \"db_connection_string\" {\n  value = ibm_database.iac_app_db_instance.connectionstrings.0.composed\n}\noutput \"db_connection_certbase64\" {\n  value = ibm_database.iac_app_db_instance.connectionstrings.0.certbase64\n}\noutput \"db_admin_userid\" {\n  value = ibm_database.iac_app_db_instance.adminuser\n}\noutput \"db_id\" {\n  value = ibm_database.iac_app_db_instance.id\n}\noutput \"db_password\" {\n  value = var.db_admin_password\n}\n```\n\n```Dockerfile path=docker/2.0/Dockerfile\nFROM python:3.7-slim AS build\n\nRUN apt-get update && \\\n  apt-get install -y --no-install-recommends build-essential gcc && \\\n  pip install --upgrade pip && \\\n  pip install pip-tools\nRUN python -m venv /opt/venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCOPY app.py .\nCOPY import.py .\nCOPY requirements.txt requirements.in\nRUN pip-compile requirements.in > requirements.txt && \\\n  pip-sync && \\\n  pip install -r requirements.txt\n\nFROM python:3.7-slim AS app\nCOPY --from=build /opt/venv /opt/venv\n\nCOPY app.py .\nCOPY import.py .\nRUN chmod +x app.py import.py\n\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCMD [ \"python\", \"app.py\" ]\n\n```\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      volumes:\n        - name: db-volume\n          configMap:\n            name: movies-db\n        - name: db-ca-cert\n          secret:\n            secretName: db-ssl-ca-cert\n      initContainers:\n        - name: init-db\n          image: us.icr.io/iac-registry/movies:2.3\n          command: [\"python\", \"import.py\"]\n          # command: [\"/bin/sh\", \"-c\", \"while true; do sleep 1000;done\"]\n          envFrom:\n            - secretRef:\n                name: db-admin-password\n            - configMapRef:\n                name: config\n          volumeMounts:\n            - name: db-volume\n              mountPath: /data/init\n            - name: db-ca-cert\n              mountPath: \"/secret\"\n              readOnly: true\n      containers:\n        - image: us.icr.io/iac-registry/movies:2.3\n          name: movies\n          envFrom:\n            - secretRef:\n                name: db-admin-password\n            - configMapRef:\n                name: config\n          volumeMounts:\n            - name: db-ca-cert\n              mountPath: \"/secret\"\n              readOnly: true\n```\n\n```yaml path=kubernetes/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  ports:\n    - name: \"http\"\n      port: 80\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    app: movies\n  type: LoadBalancer\n```\n\n## Clean up\n\nWhen you are done with the Kubernetes cluster should destroy it.\n\nIf you want to keep the cluster running but remove everything you have done, you can execute:\n\n```bash\nkubectl delete -f kubernetes/\n\nkubectl get configmap,secret,service,deployment,pod,pvc,pv\n```\n\nIf the cluster was created **using the IBM Cloud CLI**, execute the following commands:\n\n```bash\nNAME=iac-iks-cluster\nibmcloud ks cluster rm --cluster $NAME\n\nSubnet_Name=iac-iks-subnet\nSUBNET_ID=$(ibmcloud is subnets --json | jq -r \".[] | select(.name==\\\"$Subnet_Name\\\").id\")\nibmcloud is subnet-delete $SUBNET_ID\n\nVPC_Name=iac-iks-vpc\nVPC_ID=$(ibmcloud is vpcs --json | jq -r \".[] | select(.name==\\\"$VPC_Name\\\").id\")\nibmcloud is vpc-delete $VPC_Name\n```\n\nIf the cluster was created **using Terraform**, just need to execute the command:\n\n```bash\nterraform destroy\n```\n\nAnd, if the cluster was created **using IBM Cloud Schematics**, execute the following commands:\n\n```bash\nibmcloud schematics workspace list              # Identify the WORKSPACE_ID\nWORKSPACE_ID=\n\nibmcloud schematics destroy --id $WORKSPACE_ID  # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n\n# ... wait until it's done\n\nibmcloud schematics workspace delete --id $WORKSPACE_ID\nibmcloud schematics workspace list\n```\n","type":"Mdx","contentDigest":"71a2941bf55f5615164053443731bc28","counter":702,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"IaC for Containers Registry","description":"Use IaC to create and modify the IBM Container Registry","keywords":"terraform,ibm cloud,containers,registry,docker,images,icr,iks,k8s,kubernetes"},"exports":{},"rawBody":"---\ntitle: IaC for Containers Registry\ndescription: Use IaC to create and modify the IBM Container Registry\nkeywords: 'terraform,ibm cloud,containers,registry,docker,images,icr,iks,k8s,kubernetes'\n---\n\n<PageDescription>\n\nAutomating the management of container services on IBM Cloud including the Container Registry and Kubernetes Services (IKS)\n\n</PageDescription>\n\n<AnchorLinks small>\n  <AnchorLink>Prerequisites</AnchorLink>\n  <AnchorLink>IBM Cloud Container Registry</AnchorLink>\n  <AnchorLink>IBM Cloud Kubernetes Service</AnchorLink>\n  <AnchorLink>IKS with Terraform</AnchorLink>\n  <AnchorLink>IKS with IBM Cloud Schematics</AnchorLink>\n  <AnchorLink>Deploy the Application</AnchorLink>\n  <AnchorLink>Persistent Volumes</AnchorLink>\n  <AnchorLink>External IBM Cloud Database</AnchorLink>\n  <AnchorLink>Deployment Troubleshooting</AnchorLink>\n  <AnchorLink>Final Code</AnchorLink>\n  <AnchorLink>Clean up</AnchorLink>\n</AnchorLinks>\n\n## Prerequisites\n\nThe steps in this pattern require the local workstation to be configured with the IBM Cloud CLI, CLI plugins for `container-service`, `container-registry` & `schematics`, the Terraform CLI, IBM Terrraform provider and a local installation of [Docker](https://docs.docker.com/get-docker/) . For more details on setting up the various CLI environments, see the [Setup Environment](/iac/setup-environment) chapter.\n\n## IBM Cloud Container Registry\n\nIBM Cloud Container Registry (ICR) is used to store, manage and deploy private container images in a highly available and scalable architecture. You can also set up your own image namespace and push container images to them. To learn more, see the [Container Registry](https://cloud.ibm.com/docs/Registry?topic=Registry-getting-started) documentation. There are no specific IaC steps required to enable the Container Registry, this is a capablity that is available to an IBM Cloud account without performing a service creation task.\n\nContainer images for IBM Cloud follow the [Open Container Initiative](https://www.opencontainers.org/) (OCI) standards to provide interoperability and flexibility in tooling for the container lifecycle. One well known tool for createing OCI-compliant images is `docker` which will be used for the examples in this pattern.\n\nThe `docker` command creates an image from a `Dockerfile`, which contains instructions to build the image. A `Dockerfile` might reference build artifacts in its instructions that are stored separately, such as an app, the app's configuration, and its dependencies. Images are typically stored in a registry that can either be accessible by the public (public registry) or set up with limited access for a small group of users (private registry). By using IBM Cloud Container Registry, only users with access to your IBM Cloud account through IAM can access your images.\n\nContinue using the same application from the previous patterns in order to have a simple container image that can be used with the IBM Container Registry and Kubernetes service. Create the first version of a `Dockerfile` with the following content in the directory `docker/1.0/` on this project.\n\n```Dockerfile path=docker/1.0/Dockerfile\nFROM node:13\n\nCOPY ./data/v1 /data\nRUN npm install -g json-server\n\nWORKDIR /app\n\nEXPOSE 8080\n\nCMD [ \"json-server\", \"--watch\", \"/data/db.min.json\", \"--port\", \"8080\", \"--host\", \"0.0.0.0\" ]\n```\n\nCopy to the `data/v1` folder the JSON database file `db.min.json` from the previous patterns. Now, build and test the container image locally using the `docker` command, the Dockerfile in `docker/1.0/` using the current directory as context because it needs the `data/v1/` directory with the JSON database.\n\n```bash\ndocker build -t movies:1.0 -f docker/1.0/Dockerfile .\ndocker images\n\ndocker run --name movies -d --rm -p 80:8080 -v $PWD/data/v1:/data movies:1.0\n\ncurl http://localhost/movies/675\n\ndocker stop $(docker ps -q --filter name=movies)\n```\n\nTo create an Container Registry namespace, use the IBM Cloud CLI with the `container-registry` plugin. Make sure you have the latest version installed and you have [setup the environment](/iac/setup-environment) correctly. Namespace names (like Docker Hub and other container repositories) must be unique for a container registry region, so substitute the name shown here with a unique one of your choosing.\n\nThe sub-command `namespace-add` will create the new namespace. The examples that follow will use `iac-registry` as the namespace:\n\n```bash\nibmcloud cr namespace-list\nibmcloud cr namespace-add iac-registry\n```\n\nIn order to push your local OCI image to the namespace registry, it must be tagged as: `REGION.icr.io/NAMESPACE/IMAGE:TAG`. Use the sub-command `region` to find the registry region you are targeting:\n\n```bash\nibmcloud cr region\n```\n\nContinuing with the example, the region is `us` so the registry is `us.icr.io`. The namespace is `iac-registry`, the image name is `movies` and the version tag `1.0`. The full tag would be: `us.icr.io/iac-registry/movies:1.0`. The image has already been created with the tag `movies` so to update, use the docker `tag` command:\n\n```bash\ndocker images\ndocker tag movies us.icr.io/iac-registry/movies:1.0\n```\n\n<InlineNotification>\n\n**Image tags**\n\nVisit the [Docker tag](https://docs.docker.com/engine/reference/commandline/tag/) documentation to find our more about image tags\n\n</InlineNotification>\n\nBefore pushing the image to the registry it's required to login with the IBM Cloud CLI `login` sub-command:\n\n```bash\nibmcloud cr login\n```\n\nThis command will set up the local `docker` CLI with a credentials object that allows it to communicate to the namespaces defined for your account in the current container registry region. After logging in, push the image with the Docker command `push`:\n\n```bash\ndocker push us.icr.io/iac-registry/movies:1.0\n```\n\nYou can check the image in the registry in different ways: (1) listing the images in the registry with the `ibmcloud cr images` command, or (2) using the `docker` command to pull the image, either from a different computer or locally deleting the image and pulling it down from the registry:\n\n```bash\n# Option 1:\nibmcloud cr images --restrict iac-registry\n\n# Option 2:\ndocker rmi us.icr.io/iac-registry/movies:1.0\ndocker pull us.icr.io/iac-registry/movies:1.0\ndocker images\n```\n\nWith the container image uploaded to the IBM Container Registry, you will be able to create Kubernetes deployments of the image by specifying the path to the fully qualified tag name `us.icr.io/iac-registry/movies:1.0` . Before doing this, you will need to create an IKS cluster.\n\n## IBM Cloud Kubernetes Service\n\nIBM Cloud Kubernetes Service (IKS) is a managed offering providing dedicated Kubernetes clusters to deploy and manage containerized apps. In this section you will create a Kubernetes cluster and deploy a simple API application. Examples will be provided using IBM Cloud CLI, Terraform and Schematics. The scope of this section is to cover creation of clusters and simple application deployment using IaC techniques. It will not cover deeper details for managing Kubernetes resources in general or broadly managing Kubernetes and deployments.\n\nTo create a Kubernetes cluster using the IBM Cloud CLI you need to specify parameters such as zone and worker node flavor. Discover these using the following commands. In this example, we are using Zone `us-south-1` and worker node flavor `mx2.4x32`.\n\n```bash\nibmcloud ks zone ls --provider vpc-gen2 --show-flavors\nZONE=us-south-1\n\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE\nFLAVOR=mx2.4x32\n```\n\nYou also need a VPC and Subnet for the Kubernetes cluster. If they do not yet exist, they may be created using the IBM Cloud CLI:\n\n```bash\n# VPC Name: iac-iks-vpc\nibmcloud is vpc-create iac-iks-vpc\nVPC_ID=$(ibmcloud is vpcs --json | jq -r \".[] | select(.name==\\\"iac-iks-vpc\\\").id\")\n\n# Subnet Name: iac-iks-subnet with 16 IP addresses.\nibmcloud is subnet-create iac-iks-subnet $VPC_ID --zone $ZONE --ipv4-address-count 16\nSUBNET_ID=$(ibmcloud is subnets --json | jq -r \".[] | select(.name==\\\"iac-iks-subnet\\\").id\")\n```\n\nAfter the VPC is created, the default security group will not have network access rules needed by the load balancers of the Kubernetes service to talk to the ingress controllers or other applications deployed as NodePort services. Update the default security group by adding the following rule.\n\n```bash\nDEFAULT_SG_ID=$(ibmcloud is vpc-default-security-group $VPC_ID --json | jq -r \".id\")\n\nibmcloud is security-group-rule-add $DEFAULT_SG_ID inbound tcp --port-min 30000 --port-max 32767\n```\n\nIf you already have a VPC and Subnets, get their IDs with the following `ibmcloud ks` sub-commands:\n\n```bash\nibmcloud ks vpcs --provider vpc-gen2        # VPC Name: iac-iks-vpc\nVPC_ID=$(ibmcloud ks vpcs --provider vpc-gen2 --json | jq -r '.[] | select(.name==\"iac-iks-vpc\").id')\n\nibmcloud ks subnets --provider vpc-gen2 --vpc-id $VPC_ID --zone $ZONE   # Subnet Name: iac-iks-subnet\nSUBNET_ID=$(ibmcloud ks subnets --provider vpc-gen2 --vpc-id $VPC_ID --zone $ZONE --json | jq -r '.[] | select(.name==\"iac-iks-subnet\").id')\n```\n\nThe available Kubernetes versions to install are listed with the command `ibmcloud ks versions`. For IKS on Gen2, use a kubernetes cluster version > `1.18`. With all input parameters defined, including a name and Kubernetes veyou are ready to create the cluster using the `cluster create` sub-command, like this:\n\n```bash\nNAME=iac-iks-cluster\nVERSION=1.18.3\n\nibmcloud ks cluster create vpc-gen2 \\\n  --name $NAME \\\n  --zone $ZONE \\\n  --vpc-id $VPC_ID \\\n  --subnet-id $SUBNET_ID \\\n  --flavor $FLAVOR \\\n  --version $VERSION \\\n  # --workers $N \\\n  # --entitlement cloud_pak \\\n  # --service-subnet $SUBNET_CIDR \\\n  # --pod-subnet $POD_CIDR \\\n  # --disable-public-service-endpoint \\\n```\n\nThe default values for the optional parameters are:\n\n- `N`: 1, this is a one worker node cluster.\n- `SUBNET_CIDR`: 172.21.0.0/16\n- `POD_CIDR`: 172.30.0.0/16\n- `disable-public-service-endpoint`: false\n\nTo identify your Kubernetes cluster status use the command `ibmcloud ks clusters`, wait a few minutes to have it up and running.\n\nWhen the Kubernetes cluster state is `normal` get the configuration to access the cluster using the following command:\n\n```bash\nibmcloud ks cluster config --cluster $NAME\n```\n\nNow you are ready to use the `kubectl` command, these are some initial commands:\n\n```bash\nkubectl cluster-info\nkubectl get nodes\n```\n\nYou can obtain more information of the cluster with the commands:\n\n```bash\nibmcloud ks worker ls --cluster $NAME\nibmcloud ks cluster get --cluster $NAME\n```\n\nTo know more read the [Kubernetes Service (IKS)](https://cloud.ibm.com/docs/containers?topic=containers-getting-started) documentation.\n\n## IKS with Terraform\n\nAll the same actions executed with the IBM Cloud CLI has to be done with Terraform, lets create a new `main.tf` file with the IBM Provisioner using Gen 2, the given region and the data source to get the info of the user selected resource group.\n\n```hcl path=main.tf\nprovider \"ibm\" {\n  generation = 2\n  region     = var.region\n}\n\ndata \"ibm_resource_group\" \"group\" {\n  name = var.resource_group\n}\n```\n\nThe `variables.tf` file defines the required variables above, the project name and environment to use them as prefix to name the resources, the code would be like this:\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"resource_group\" {\n  default = \"Default\"\n}\nvariable \"region\" {\n  default = \"us-south\"\n}\n```\n\nTo not have to enter the variables every time we execute terraform, lets add some variables value to the `terraform.tfvars` file. Make sure this file is appended to the `.gitignore` file.\n\n```hcl path=terraform.tfvars\nproject_name = \"iac-iks-test\"\nenvironment  = \"dev\"\n\n# Optional variables\nresource_group = \"Default\"\nregion         = \"us-south\"\n```\n\nThe IKS clusters needs a VPC, Subnet(s) and Security Group Rules(s) added to the default security group of the VPC. Just like we did using the IBM Cloud CLI let's create them allowing inbound traffic to ports 30000 - 32767 for the security group rules. Same as you did on [Network](/iac-resources/network) and [Compute](/iac-resources/compute) the number of subnets is defined by the number of zones provided by the user. Lets code this in the `network.tf` file and append the following variables to `variables.tf`.\n\n```hcl path=network.tf\nresource \"ibm_is_vpc\" \"iac_iks_vpc\" {\n  name = \"${var.project_name}-${var.environment}-vpc\"\n}\n\nresource \"ibm_is_subnet\" \"iac_iks_subnet\" {\n  count                    = local.max_size\n  name                     = \"${var.project_name}-${var.environment}-subnet-${format(\"%02s\", count.index)}\"\n  zone                     = var.vpc_zone_names[count.index]\n  vpc                      = ibm_is_vpc.iac_iks_vpc.id\n  total_ipv4_address_count = 256\n  resource_group           = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_iks_security_group_rule_tcp_k8s\" {\n  count     = local.max_size\n  group     = ibm_is_vpc.iac_iks_vpc.default_security_group\n  direction = \"inbound\"\n  remote    = ibm_is_subnet.iac_iks_subnet[count.index].ipv4_cidr_block\n\n  tcp {\n    port_min = 30000\n    port_max = 32767\n  }\n}\n```\n\n```hcl path=variables.tf\n  ...\nvariable \"vpc_zone_names\" {\n  type    = list(string)\n  default = [\"us-south-1\", \"us-south-2\", \"us-south-3\"]\n}\n\nlocals {\n  max_size = length(var.vpc_zone_names)\n}\n```\n\nLast but not least, create the `iks.tf` file to define the IKS cluster using the `ibm_container_vpc_cluster` resource.\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavor\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n```\n\nThe above code also takes the Kubernetes version, worker nodes flavor and number from the variables `k8s_version`, `flavor` and `workers_count` respectively, so lets add them to the `variables.tf` file.\n\n```hcl path=variables.tf\n  ...\nvariable \"flavor\" {\n  default = \"mx2.4x32\"\n}\nvariable \"workers_count\" {\n  default = 3\n}\nvariable \"k8s_version\" {\n  default = \"1.18.3\"\n}\n```\n\nThis will create a Kubernetes cluster of 3 worker nodes with 4 CPU and 32 Gb Memory. To know the available flavors in the zone, use the following IBM Cloud CLI command:\n\n```bash\nibmcloud ks zone ls --provider vpc-gen2 --show-flavors\n\n# Or\nZONE=us-south-1\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE\n```\n\nTo sort them by CPU and memory, use the same command with `sort`:\n\n```bash\nZONE=us-south-1\nibmcloud ks flavors --provider vpc-gen2 --zone $ZONE -s | sort -k2 -k3 -n\n```\n\nThe main input parameters of the `ibm_container_vpc_cluster` resource are listed in the following table:\n\n| Input parameter | Description |\n|---|---|\n| `name` | name of the cluster |\n| `vpc_id` | ID of the VPC that you want to use for your cluster |\n| `flavor` | flavor of the VPC worker node |\n| `zones` | nested block describing the zones of this VPC cluster |\n| `zones.name` | name of the zone |\n| `zones.subnet_id` | subnet in the zone to assign the cluster |\n| `worker_count` | (optional) number of worker nodes per zone in the default worker pool. Default value `1` |\n| `kube_version` | (optional) Kubernetes version, including the major.minor version. If not set, the default version from `ibmcloud ks versions` is used |\n| `resource_group_id` | (optional) ID of the resource group. Defaults to `default` |\n| `wait_till` | (optional) marks the creation of your cluster complete when the given stage is achieved, read below to know the available stages and how this can help you speed up the terraform execution |\n| `disable_public_service_endpoint` | (optional) disable the master public service endpoint to prevent public access. Defaults to `true` |\n| `pod_subnet` | (optional) subnet CIDR to provide private IP addresses for pods. Defaults to `172.30.0.0/16` |\n| `service_subnet` | (optional) subnet CIDR to provide private IP addresses for services. Defaults to `172.21.0.0/16` |\n| `tags` | (optional) list of tags to associate with your cluster |\n\nThe creation of a cluster can take some minutes to complete. To avoid long wait times, you can specify the stage when you want Terraform to mark the cluster resource creation as completed. The cluster creation might not be fully completed and continues to run in the background, however this can help you to continue with the code execution without waiting for the cluster to be fully created.\n\nTo set the waiting stage, use the `wait_till` with one of the following stages:\n\n- **MasterNodeReady**: Terraform marks the creation of your cluster complete when the cluster master is in a ready state.\n- **OneWorkerNodeReady**: Waits until the master and at least one worker node are in a ready state.\n- **IngressReady**: Waits until the cluster master and all worker nodes are in a ready state, and the Ingress subdomain is fully set up. This is the default value.\n\nThis would be enough to have an IKS cluster running. Just need to execute `terraform apply`, however lets create workers pools, one in each subnet or zone, using the resource `ibm_container_vpc_worker_pool`. Replace the code in `iks.tf` file for the following code and modify the variables used for the number of workers and its flavor.\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavors[0]\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  wait_till         = \"OneWorkerNodeReady\"\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n\nresource \"ibm_container_vpc_worker_pool\" \"iac_iks_cluster_pool\" {\n  count             = local.max_size - 1\n  cluster           = ibm_container_vpc_cluster.iac_iks_cluster.id\n  worker_pool_name  = \"${var.project_name}-${var.environment}-wp-${format(\"%02s\", count.index + 1)}\"\n  flavor            = var.flavors[count.index + 1]\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  worker_count      = var.workers_count[count.index + 1]\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[count.index + 1]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[count.index + 1].id\n  }\n}\n```\n\n```hcl path=variables.tf\nvariable \"flavors\" {\n  type    = list(string)\n  default = [\"mx2.4x32\", \"cx2.2x4\", \"cx2.4x8\"]\n}\nvariable \"workers_count\" {\n  type    = list(number)\n  default = [3, 2, 1]\n}\n```\n\nThe main input parameters for the `ibm_container_vpc_worker_pool` resource are similar to the parameters for `ibm_container_vpc_cluster` except for `worker_pool_name` which is used to name the pool, and `cluster` with the name or ID of the cluster set this pool.\n\nUsing a file `output.tf` helps us to get some useful information about the cluster through output variables, like so.\n\n```hcl path=output.tf\noutput \"cluster_id\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.id\n}\n\noutput \"cluster_name\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.name\n}\n\noutput \"entrypoint\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.public_service_endpoint_url\n}\n```\n\nNow everything is ready to create the cluster with the wellknown Terraform commands:\n\n```bash\nterraform plan\nterraform apply\n```\n\nAfter having the cluster ready, you can use the IBM Cloud CLI to get the cluster configuration to setup `kubectl`, like so:\n\n```bash\nibmcloud ks cluster config --cluster $(terraform output cluster_id)\n```\n\nEnjoy the new cluster, here are some basic initial commands to verify the cluster is working\n\n```bash\nkubectl cluster-info\nkubectl get nodes\nkubectl get pods -A\n```\n\n### A simpler IKS cluster\n\nFor simplicity and creation speed, lets modify the `terraform.tfvars` to have a simpler cluster with one single node. This will help us to have the cluster quicker.\n\n```hcl path=terraform.tfvars\nproject_name = \"iac-iks-small-OWNER\"\nenvironment  = \"dev\"\n\n# Optional variables\nresource_group = \"Default\"\nregion         = \"us-south\"\nvpc_zone_names = [\"us-south-1\"]\nflavors        = [\"mx2.4x32\"]\nworkers_count  = [1]\nk8s_version    = \"1.18.3\"\n```\n\nRemember to get a supported and the latest Kubernetes version from the output of the command `ibmcloud ks versions`, otherwise you may get an error like this one:\n\n```\nError: Request failed with status code: 400, ServerErrorResponse: {\"incidentID\":\"5a4a1a08a275eb6d-LAX\",\"code\":\"E0156\",\"description\":\"A previous patch was specified. Only the most recent patch for a particular minor version can be specified during cluster create.\",\"type\":\"Versions\",\"recoveryCLI\":\"To list supported versions, run 'ibmcloud ks versions'.\"}\n```\n\nExecuting `terraform plan & terraform apply` will get an IKS cluster up and running quicker than before.\n\n## IKS with IBM Cloud Schematics\n\nRunning this code with IBM Cloud Schematics is the same as with the other patterns. Create the `workspace.json` file adding the variables required for this code and replacing `OWNER` for your username or id, like this one:\n\n```json path=workspace.json\n{\n  \"name\": \"iac_iks_test\",\n  \"type\": [\n    \"terraform_v0.12\"\n  ],\n  \"description\": \"Sample workspace to test IBM Cloud Schematics. Deploys an web server on a VSI with a Hello World response\",\n  \"tags\": [\n    \"app:iac_iks_test\",\n    \"owner:OWNER\",\n    \"env:dev\"\n  ],\n  \"template_repo\": {\n    \"url\": \"https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/09-containers\"\n  },\n  \"template_data\": [{\n    \"folder\": \".\",\n    \"type\": \"terraform_v0.12\",\n    \"variablestore\": [{\n        \"name\": \"project_name\",\n        \"value\": \"iac-iks-test-OWNER\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"environment\",\n        \"value\": \"dev\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"region\",\n        \"value\": \"us-south\",\n        \"type\": \"string\"\n      },\n      {\n        \"name\": \"vpc_zone_names\",\n        \"value\": [\"us-south-1\", \"us-south-2\", \"us-south-3\"],\n        \"type\": \"list(string)\"\n      },\n      {\n        \"name\": \"flavors\",\n        \"value\": [\"mx2.4x32\", \"cx2.2x4\", \"cx2.4x8\"],\n        \"type\": \"list(string)\"\n      },\n      {\n        \"name\": \"workers_count\",\n        \"value\": [3, 2, 1],\n        \"type\": \"list(number)\"\n      },\n      {\n        \"name\": \"k8s_version\",\n        \"value\": \"1.18.3\",\n        \"type\": \"string\"\n      }\n    ]\n  }]\n}\n```\n\nTo create the workspace using the IBM Cloud CLI execute the following commands:\n\n```bash\nibmcloud schematics workspace new --file workspace.json\nibmcloud schematics workspace list          # Identify the WORKSPACE_ID\nWORKSPACE_ID=\n```\n\nSet the variable `WORKSPACE_ID` because it'll be used several times. Then plan and apply the code like so.\n\n```bash\nibmcloud schematics plan --id $WORKSPACE_ID  # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n\nibmcloud schematics apply --id $WORKSPACE_ID # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n```\n\nNote the execution of apply will take some time, so check the logs either with the IBM Cloud CLI command or using the IBM Cloud Web Console. When the cluster is ready, you can use the IBM Cloud CLI to get the cluster configuration to setup `kubectl` and validate the cluster is accesible:\n\n```bash\nCLUSTER_ID=$(ibmcloud schematics workspace output --id $WORKSPACE_ID --json | jq -r '.[].output_values[].cluster_id.value')\nibmcloud ks cluster config --cluster $CLUSTER_ID\n\nkubectl cluster-info\nkubectl get nodes\nkubectl get pods -A\n```\n\n## Deploy the Application\n\nTo deploy the previously built Docker image version 1.0 we use the Kubernetes API and resources. Lets create a deployment file either by getting it from the following example or generating it with kubectl generators, like so:\n\n```bash\nmkdir kubernetes\nkubectl create deployment movies --image=us.icr.io/iac-registry/movies:1.0 --dry-run=client -o yaml > kubernetes/deployment.yaml\nkubectl expose deployment movies --port=80 --target-port=8080 --type=LoadBalancer --dry-run=client -o yaml > kubernetes/service.yaml\n```\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      containers:\n        - image: us.icr.io/iac-registry/movies:1.0\n          name: movies\n```\n\n```yaml path=kubernetes/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  ports:\n    - name: \"http\"\n      port: 80\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    app: movies\n  type: LoadBalancer\n```\n\nTo deploy the application execute the `kubectl apply` command like this:\n\n```bash\nkubectl apply -f kubernetes/deployment.yaml\nkubectl apply -f kubernetes/service.yaml\n\nkubectl get deployment movies\nkubectl get svc movies\n```\n\nTo validate the application you need to get the external IP or DNS to access the application executing the following code. You may have to wait a few minutes until the Load Balancer is ready. You can checkt thest status again using `kubectl get svc movies`.\n\n```bash\nwatch kubectl get svc movies\n\nADDRESS=$(kubectl get svc movies -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n\ncurl $ADDRESS/movies/675\n```\n\nIn a real application, it's quite common to have new or changing data. In this example, such a change to the JSON database would require a new image. If this happens very often it becomes very efficient. To address this inflexible model, you can put the JSON database in a ConfigMap. Create the `cm.yaml` file to define the ConfigMap with the content of the JSON file `data/v1/db.min.json` using this command\n\n```bash\nkubectl create configmap movies-db --from-file=./data/v1/db.min.json --dry-run=client -o yaml > kubernetes/cm.yaml\n```\n\nOr, edit the file yourself with the following content.\n\n```yaml path=kubernetes/cm.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: movies-db\ndata:\n  db.min.json: |\n    {\"movies\":[ ... HERE GOES THE JSON FILE ... ]}\n```\n\nAnd apply the code to the cluster using `kubectl`, like this.\n\n```bash\nkubectl apply -f kubernetes/cm.yaml\nkubectl get cm\n```\n\nTo make the pod access the JSON file you need to modify the Pod definition inside the deployment. Modify the `deployment.yaml` file to add the `volumes` and `volumeMounts` specifications, like so.\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\n  ...\n    spec:\n      volumes:\n        - name: db-volume\n          configMap:\n            name: movies-db\n      containers:\n        - image: us.icr.io/iac-registry/movies:1.0\n          name: movies\n          volumeMounts:\n            - name: db-volume\n              mountPath: /data\n```\n\nUpdate the new Pod applying the code, then verify it was sucessfuly applied using these commands.\n\n```bash\nkubectl apply -f kubernetes/v1.0/deployment.yaml\nkubectl get deployments,pods\n```\n\nThe application should be running as usual:\n\n```bash\nADDRESS=$(kubectl get svc movies -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\ncurl $ADDRESS/movies/675\n```\n\n<InlineNotification>\n\n**What about the JSON DB in the container?**\n\nIdeally you need to remove the line that copy the JSON database file into the image from the Dockerfile (`COPY ./data/v1 /data`), then re-build and push the image to ICR. However, this is a simple example and we will modify this application docker image even more.\n\nWith the new ConfigMap the current `/data/db.min.json` file in the container is replaced by the one from the ConfigMap. You can verify it's there with the execution of `kubectl exec --stdin --tty movies-89977dc9-7gdpd -- cat /data/db.min.json`, replacing `movies-89977dc9-7gdpd` for the name of the movies Pod.\n\n</InlineNotification>\n\nTo double check, modify the ConfigMap updating a movie or modifying the database, then access the application using `curl`. The instructions when the ConfigMap is modified are as follows.\n\n1. Modify the ConfigMap in the file `cm.yaml`\n2. Applying the changes with the command `kubectl apply -f kubernetes/cm.yaml`\n3. Delete the running pods so the Replica Set create a new pod using the new JSON database. Identify the Pod name using `kubectl get pods` then use the command `kubectl delete pod <Movies Pod Name>`\n4. Verify the change with `curl $ADDRESS/movies/`.\n\n## Persistent Volumes\n\nIn the version 1.0 of the application, the JSON database was in the ephemeral container, this may not be good practice in general so let's migrate the database to a persistent storage such as IBM Cloud Block Storage for VPC. This storage provides hypervisor-mounted, high-performance data storage for your VSI or IKS nodes that you can provision within a VPC.\n\nLet's start creating the file `pvc.yaml` with the definition of a [Persisten Volume Claim](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim) with 1Gb and the profile `ibmc-vpc-block-5iops-tier`.\n\n```yaml path=kubernetes/pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: movies\nspec:\n  storageClassName: ibmc-vpc-block-general-purpose\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n```\n\nBefore use the Persistent Volume Claim (PVC) apply the changes, it has to be ready before being used.\n\n```bash\nkubectl apply -f kubernetes/pvc.yaml\nkubectl get pvc movies\n```\n\nTo use this volume we need to modify the Pod specification in the deployment, open the `kubernetes/deployment.yaml` file to add the `volumes` and `volumeMounts` specifications.\n\nHowever, these changes don't put the initial JSON database into the volume yet. There may be different ways to do this, a possible option is to the use of [Init Containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) to dump the initial JSON database into the volume, but this option cannot be used because the volume access mode is **ReadWriteOnce** which only allows one container to access the volume at a time. Other option, and the one we will implement, is to make the Docker container copy the initial database into the volumen if there isn't any yet. The initial JSON file is provided with a ConfigMap, let's add it just like we did in the previous section.\n\nThe `deployment.yaml` file will be like this.\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      volumes:\n        - name: movies-volume\n          persistentVolumeClaim:\n            claimName: movies\n        - name: db-volume\n          configMap:\n            name: movies-db\n      containers:\n        - image: us.icr.io/iac-registry/movies:1.1\n          name: movies\n          volumeMounts:\n            - name: db-volume\n              mountPath: /data/init\n            - name: movies-volume\n              mountPath: \"/data\"\n```\n\nAs you can see the image version is different, it is `1.1`. In the new Dockerfile we remove the line `COPY ./data/v1 /data` and add the line `VOLUME /data`. Also, instead of executing the `json-server` command, it runs a script to copy the database to the right location. The new Dockerfile, to be tagged with version `1.1`, is like this.\n\n```dockerfile path=docker/1.1/Dockerfile\nFROM node:13\n\nRUN npm install -g json-server\nADD entrypoint.sh /entrypoint.sh\n\nWORKDIR /app\nVOLUME /data\n\nEXPOSE 8080\n\nENTRYPOINT [ \"/entrypoint.sh\" ]\n\nCMD [ \"\" ]\n```\n\nAnd the script to be executed as entrypoint executes the input with `exec \"$@\"`, however if no command is passed in it'll execute `json-server` after initialize the JSON database file. This script is like follows.\n\n```bash path=docker/1.1/entrypoint.sh\n#!/bin/bash\n\nif [[ -n \"$@\" ]]; then\n  exec \"$@\"\n  exit $?\nfi\n\nport=\"8080\"\nhost=\"0.0.0.0\"\ninit_db='{\"movies\": []}'\ninit_db_file=/data/init/db.min.json\ndb_file=/data/db.json\n\ninit_database() {\n  # If the DB is there and contain movies, there is nothing to do\n  [[ -e $db_file ]] && grep -q '\"movies\"' $db_file && echo \"A database file with movies already exists.\" && return\n\n  # If the init DB file is there, copy it to the DB file and return if succeed\n  [[ -e $init_db_file ]] && cp $init_db_file $db_file && echo \"initialized the database with the database file $init_db_file.\" && return\n\n  # if everything failed, initiallize the DB with empty list of movies\n  echo \"initializing the database with an empty database.\"\n  echo $init_db > $db_file\n}\n\ninit_database\n\necho \"Starting json-server on $host:$port watching for DB file $db_file\"\njson-server --port $port --host $host --watch $db_file\n```\n\nThis new image has to be built, tagged and pushed to ICR very similar to like we did with the initial version and that's what we will do in a moment. This time the context of `docker build` change to `docker/1.1` because we don't use the `./data` directory and we use the `docker/1.1/entrypoint.sh` script.\n\n```bash\ndocker build -t us.icr.io/iac-registry/movies:1.1 -f docker/1.1/Dockerfile docker/1.1\ndocker push us.icr.io/iac-registry/movies:1.1\n\nibmcloud cr images --restrict iac-registry\n```\n\nApply all files and verify the new changes executing the following commands.\n\n```bash\nkubectl apply -f kubernetes/pvc.yaml\nkubectl apply -f kubernetes/cm.yaml\nkubectl apply -f kubernetes/deployment.yaml\n\nkubectl get pvc movies\nkubectl get cm movies\nkubectl get deployment movies\nkubectl get pods\n\nADDRESS=$(kubectl get svc movies -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\ncurl $ADDRESS/movies/83\n```\n\nHaving the JSON Database in a persistent volume we can modify the database and the changes will persist the next time we deploy the application or restart the container. Having the following movie to add:\n\n```json path=data/v1/new_movie.json\n{\n  \"id\": \"32\",\n  \"title\": \"13 Assassins\",\n  \"originalTitle\": \"十三人の刺客\",\n  \"contentRating\": \"R\",\n  \"summary\": \"Cult director Takashi Miike (Ichi the Killer, Audition) delivers a bravado period action film set at the end of Japan’s feudal era. 13 Assassins - a “masterful exercise in cinematic butchery” (New York Post) - is centered around a group of elite samurai who are secretly enlisted to bring down a sadistic lord to prevent him from ascending to the throne and plunging the country into a war torn future.\",\n  \"rating\": \"9.6\",\n  \"audienceRating\": \"8.8\",\n  \"year\": \"2011\",\n  \"tagline\": \"Take up your sword.\",\n  \"duration\": \"7505063\",\n  \"originallyAvailableAt\": \"2011-07-05\",\n  \"addedAt\": \"1351391906\",\n  \"updatedAt\": \"1546942538\",\n  \"audienceRatingImage\": \"rottentomatoes://image.rating.upright\",\n  \"hasPremiumPrimaryExtra\": \"1\",\n  \"ratingImage\": \"rottentomatoes://image.rating.ripe\",\n  \"genre\": \"Action & Adventure\",\n  \"director\": \"Takashi Miike\",\n  \"writer\": [\"Kaneo Ikegami\", \"Daisuke Tengan\"],\n  \"country\": \"Japan\",\n  \"cast\": [\"Yusuke Iseya\", \"Kôji Yakusho\"]\n}\n```\n\nLets add the new movie using `curl`, scale the deployment to zero containers, then back to one and verify the new movie is still there.\n\n```bash\ncurl -X POST -H \"Content-Type: application/json\" -d@data/v1/new_movie.json $ADDRESS/movies\ncurl $ADDRESS/movies/32\n\nkubectl scale deployment movies --replicas=0\nkubectl get deployments movies\nkubectl get pods\nkubectl get pv,pvc\n\nkubectl scale deployment movies --replicas=1\nkubectl get deployments movies\nwatch kubectl get pods\n\n# When the pod is running:\ncurl $ADDRESS/movies/32\n```\n\nTo learn more about the storage provided to the persistent volume claim, see the [Block Storage for VPC](https://cloud.ibm.com/docs/containers?topic=containers-vpc-block) documentation.\n\n## External IBM Cloud Database\n\nThis section provides an example of deploying the Python API application used in the [Cloud Databases](/iac-resources/services) pattern also in the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [08_cloud-services/app](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/08_cloud-services/app).\n\nThis change requires more major changes to the Docker container so it's going to make sense to bump the tag to `2.0`. In the following Dockerfile we use a multi-stage build to reduce the size of the final Docker image. The `build` stage use Python VirtualEnv to install all the required packages then they are copied to the `app` image which is used to execute the API application.\n\n```dockerfile path=docker/2.0/Dockerfile\nFROM python:3.7-slim AS build\n\nRUN apt-get update && \\\n  apt-get install -y --no-install-recommends build-essential gcc && \\\n  pip install --upgrade pip && \\\n  pip install pip-tools\nRUN python -m venv /opt/venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCOPY app.py .\nCOPY import.py .\nCOPY requirements.txt requirements.in\nRUN pip-compile requirements.in > requirements.txt && \\\n  pip-sync && \\\n  pip install -r requirements.txt\n\nFROM python:3.7-slim AS app\nCOPY --from=build /opt/venv /opt/venv\n\nCOPY app.py .\nCOPY import.py .\nRUN chmod +x app.py import.py\n\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCMD [ \"python\", \"app.py\" ]\n```\n\nJust as with the previous versions, let's build and push the container using the following commands:\n\n```bash\ndocker build -t us.icr.io/iac-registry/movies:2.0 -f docker/2.0/Dockerfile docker/2.0\ndocker push us.icr.io/iac-registry/movies:2.0\n\nibmcloud cr images --restrict iac-registry\n```\n\nWe also need the IBM Cloud Database, with the following Terraform code in the `db.tf` file copied from the [Cloud Databases](/iac-resources/services) pattern, like so.\n\n```hcl path=db.tf\nresource \"ibm_database\" \"iac_app_db_instance\" {\n  name              = var.db_name\n  plan              = var.db_plan\n  location          = var.region\n  service           = \"databases-for-mongodb\"\n  resource_group_id = data.ibm_resource_group.group.id\n\n  adminpassword                = var.db_admin_password\n  members_memory_allocation_mb = var.db_memory_allocation\n  members_disk_allocation_mb   = var.db_disk_allocation\n}\n```\n\nThis file also requires addition of the following input variables to the `variables.tf` file and output variables to the `output.tf` file:\n\n```hcl path=variables.tf\nvariable \"db_plan\" {\n  default = \"standard\"\n}\nvariable \"db_name\" {\n  default = \"moviedb\"\n}\nvariable \"db_admin_password\" {\n  default = \"inSecurePa55w0rd\"\n}\nvariable \"db_memory_allocation\" {\n  default = \"3072\"\n}\nvariable \"db_disk_allocation\" {\n  default = \"61440\"\n}\n```\n\n```hcl path=output.tf\noutput \"db_connection_string\" {\n  value = ibm_database.iac_app_db_instance.connectionstrings.0.composed\n}\noutput \"db_connection_certbase64\" {\n  value = ibm_database.iac_app_db_instance.connectionstrings.0.certbase64\n}\noutput \"db_admin_userid\" {\n  value = ibm_database.iac_app_db_instance.adminuser\n}\noutput \"db_id\" {\n  value = ibm_database.iac_app_db_instance.id\n}\noutput \"db_password\" {\n  value = var.db_admin_password\n}\n```\n\nTo get it running, execute the `plan` and `apply` Terraform commands.\n\n```bash\nterrform plan\nterrform apply\n```\n\nBefore deploying the container to our Kubernetes cluster, do some local testing using just Docker. Execute the following commands to run the container locally, mounting the local directory `./data/v2` in a volume on the container directory `/data/init/` so the application can reach the `db.min.json` file with the initial values of the database. The initial database `db.min.json` file is different to the one used for version 1 because the `id` field is not required. Also, to allow the container to reach the IBM Cloud MongoDB Database that was created, populate environment variables with values from the Terraform output variables.\n\n The following commands will: create all the application input data, initialize the database, run the container with the API application and finally query the application with `curl`.\n\n```bash\nmkdir ./secret\nterraform output db_connection_certbase64 | base64 --decode > ./secret/db_ca.crt\n\nexport PASSWORD=$(terraform output db_password)\nexport APP_MONGODB_URI=$(terraform output db_connection_string)\nexport APP_PORT=8080\nexport APP_SSL_CA_CERT=\"/secret/db_ca.crt\"\n\ndocker run --rm \\\n  --name init-movies \\\n  -v $PWD/data/v2:/data/init \\\n  -v $PWD/secret:/secret \\\n  -e APP_SSL_CA_CERT=$APP_SSL_CA_CERT \\\n  -e PASSWORD=$PASSWORD \\\n  -e APP_MONGODB_URI=$APP_MONGODB_URI \\\n  us.icr.io/iac-registry/movies:2.0 python import.py\n\ndocker run --rm \\\n  --name movies \\\n  -p 80:$APP_PORT \\\n  -v $PWD/secret:/secret \\\n  -e APP_SSL_CA_CERT=$APP_SSL_CA_CERT \\\n  -e PASSWORD=$PASSWORD \\\n  -e APP_MONGODB_URI=$APP_MONGODB_URI \\\n  -e APP_PORT=$APP_PORT \\\n  us.icr.io/iac-registry/movies:2.0\n\ncurl http://localhost/api/healthcheck\ncurl http://localhost/api/movies\n\n\nid=$(curl -s \"http://localhost/api/movies\" | jq -r '.[0]._id | .[\"$oid\"]')\ncurl \"http://localhost/api/movies/$id\"\n```\n\nTo wipe out the database use the same docker container but instead run the `python import.py` command with the `--empty` parameter, like so:\n\n```bash\ndocker run --rm \\\n  --name drop-movies \\\n  -v $PWD/secret:/secret \\\n  -e APP_SSL_CA_CERT=$APP_SSL_CA_CERT \\\n  -e PASSWORD=$PASSWORD \\\n  -e APP_MONGODB_URI=$APP_MONGODB_URI \\\n  us.icr.io/iac-registry/movies:2.0 python import.py --empty\n```\n\nThe current Database was created with a public endpoint (it's public by default) and considering the current IKS cluster is private you may also want to migrate this database to be private as well. It was not created from the very begining because you may want to test the database from your computer, ilke we did running Docker locally.\n\nTo set this database with a private endpoint add the parameter `service_endpoints = \"private\"` to the `ibm_database.iac_app_db_instance` located in the `db.tf` file, Like so:\n\n```hcl path=db.tf\nresource \"ibm_database\" \"iac_app_db_instance\" {\n  name              = var.db_name\n  plan              = var.db_plan\n  location          = var.region\n  service           = \"databases-for-mongodb\"\n  resource_group_id = data.ibm_resource_group.group.id\n  service_endpoints = \"private\"\n\n  adminpassword                = var.db_admin_password\n  members_memory_allocation_mb = var.db_memory_allocation\n  members_disk_allocation_mb   = var.db_disk_allocation\n}\n```\n\nTo apply this migration the database has to be deleted first, applying this change now with Terrafor will cause an error because it's a parameter that cannot be set and modify the database. So, delete the database using the `destroy` Terraform command targetting the database, then apply the changes.\n\n```bash\nterraform destroy -target ibm_database.iac_app_db_instance\n\nterraform apply\n```\n\nIf the local docker container works, everything is ready to work on the Kubernetes deployment. A new ConfigMap is required with the initial data for MongoDB, another ConfigMap is required with the environment variables to have access to the database, and finally, two Secrets are required, the first [Secret](https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data) is used to store the database CA certificate and the second [Secret](https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables) stores the DB admin password. Create the ConfigMaps and Secrets with the following commands\n\n```bash\nexport PASSWORD=$(terraform output db_password)\nexport APP_MONGODB_URI=$(terraform output db_connection_string)\nexport APP_SSL_CA_CERT=\"/secret/db_ca.crt\"\n\nkubectl create configmap movies-db \\\n  --from-file=./data/v2/db.min.json \\\n  --dry-run=client -o yaml > kubernetes/cm.yaml\n\nkubectl create configmap config \\\n  --from-literal=APP_SSL_CA_CERT=$APP_SSL_CA_CERT \\\n  --from-literal=APP_MONGODB_URI=$APP_MONGODB_URI \\\n  --dry-run=client -o yaml > kubernetes/config.yaml\n\nkubectl create secret generic db-admin-password \\\n  --from-literal=PASSWORD=$PASSWORD \\\n  --dry-run=client -o yaml > kubernetes/db_admin_password.yaml\n\nmkdir ./secret\nterraform output db_connection_certbase64 | base64 --decode > ./secret/db_ca.crt\nkubectl create secret generic db-ssl-ca-cert \\\n  --from-file=./secret/db_ca.crt \\\n  --dry-run=client -o yaml > kubernetes/db_ca_cert.yaml\nrm -rf ./secret\n```\n\nThe new `deployment.yaml` uses the ConfigMap to initialize the database but this time with an [Init Container](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) to execute the `import.py` python script. Both containers get the ConfigMap with the environment variables and the two Secrets.\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      volumes:\n        - name: db-volume\n          configMap:\n            name: movies-db\n        - name: db-ca-cert\n          secret:\n            secretName: db-ssl-ca-cert\n      initContainers:\n        - name: init-db\n          image: us.icr.io/iac-registry/movies:2.0\n          command: [\"python\", \"import.py\"]\n          envFrom:\n            - secretRef:\n                name: db-admin-password\n            - configMapRef:\n                name: config\n          volumeMounts:\n            - name: db-volume\n              mountPath: /data/init\n            - name: db-ca-cert\n              mountPath: \"/secret\"\n              readOnly: true\n      containers:\n        - image: us.icr.io/iac-registry/movies:2.0\n          name: movies\n          envFrom:\n            - secretRef:\n                name: db-admin-password\n            - configMapRef:\n                name: config\n          volumeMounts:\n            - name: db-ca-cert\n              mountPath: \"/secret\"\n              readOnly: true\n```\n\nAll is set to apply the new Deployment, Services, Secrets and ConfigMaps with the following commands.\n\n```bash\nkubectl apply -f kubernetes/cm.yaml\nkubectl apply -f kubernetes/config.yaml\nkubectl apply -f kubernetes/db_admin_password.yaml\nkubectl apply -f kubernetes/db_ca_cert.yaml\nkubectl apply -f kubernetes/deployment.yaml\nkubectl apply -f kubernetes/service.yaml\n```\n\nThe PersistentVolumeClaim is not longer required for this version, so you may delete it with this command.\n\n```bash\nkubectl delete pvc movies\nwatch kubectl get pv,pvc\n```\n\n<InlineNotification>\n\n**VRF enablement**\n\nVRF stands for **Virtual Route Framework**, it's a way for routers to partition route tables logically, sort of like VLANS for L3.\n\nThe new generation of private connectivity for VPC requires that each account switch over to VRF, giving all the networks in the account it's own routing context.\n\nIf at this point you are having troubles to connect to the database from the container chances are you need to enable VRF to your account. Submit a ticket to the IBM Cloud Support team for VRF enablement.\n\nRead this [documentation](https://cloud.ibm.com/docs/resources?topic=resources-private-network-endpoints) to know more.\n</InlineNotification>\n\nOne of the differences with the version 1 is that this new architecture allows us to scale up the replicas of the pods. You can try it with these commands:\n\n```bash\nkubectl scale deployment movies --replicas=5\n\nwatch kubectl get po,deploy,rs\n```\n\nTo verify the application is working, use the same `curl` commands we've been using.\n\n```bash\nADDRESS=$(kubectl get svc movies -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n\n# Get all movies\ncurl $ADDRESS/api/movies\n\n# Get a movie\nid=$(curl -s \"http://$ADDRESS/api/movies\" | jq -r '.[0]._id | .[\"$oid\"]')\ncurl \"http://$ADDRESS/api/movies/$id\" | jq\n\n# Create a movie\nid=$(curl -s -X POST -H \"Content-Type: application/json\" -d@data/v2/new_movie.json $ADDRESS/api/movies | jq -r '.id')\ncurl $ADDRESS/api/movies/$id\n\n# Update a movie\nsed 's/13 Assassins/14 Assassins/' data/v2/new_movie.json > data/v2/update_movie.json\ncurl -s -X PUT -H \"Content-Type: application/json\" -d@data/v2/update_movie.json $ADDRESS/api/movies/$id\ncurl $ADDRESS/api/movies/$id\nrm data/v2/update_movie.json\n\n# Delete a movie\ncurl -X DELETE $ADDRESS/api/movies/$id\ncurl $ADDRESS/api/movies/$id\ncurl -s $ADDRESS/api/movies | grep '14 Assassins'\n```\n\nThere is more that you can do with this sample application, you can:\n\n- Add resource limits to the Pod so is can be scaled up or down automatically\n- Deploy a new Angular, React or Vue application to visualize the movies\n- Deploy a container with your own MongoDB to use it instead of the IBM Cloud MongoDB\n\n## Deployment Troubleshooting\n\nIf you have any problem with the validation and want to [debug or troubleshot it](https://kubernetes.io/docs/tasks/debug-application-cluster/), use the following commands to identify the root cause.\n\n```bash\nkubectl get deploy,po\n\npod_id=$(kubectl get deploy,po | grep pod/movies | head -1 | awk '{print $1}')\n\nkubectl describe pod $pod_id\nkubectl logs $pod_id\nkubectl logs $pod_id init-db\n\nkubectl exec $pod_id --container init-db -- cat /secret/db_ca.crt\nkubectl exec $pod_id --container movies -- env | grep APP\n```\n\nIf you need to login to a container replace the `command` in the deployment for `command: [\"/bin/sh\", \"-c\", \"while true; do sleep 1000;done\"]` so it doesn't fail and you have time to execute a remote bash session.\n\n```bash\nkubectl exec --stdin --tty $pod_id -- /bin/bash\nkubectl exec --stdin --tty $pod_id --container init-db  -- /bin/bash\n```\n\nIf you need to connect to the database and it has a private endpoint, deploy a MongoDB container with the `mongo` client and the required ConfigMaps and Secrets to connect to the database. Push to ICR the official MongoDB image and execute the `kubectl` generator, then modify the output file to include the ConfigMap and Secrets:\n\n```bash\ndocker pull mongo:bionic\ndocker tag mongo:bionic us.icr.io/iac-registry/mongo:bionic\ndocker push us.icr.io/iac-registry/mongo:bionic\n\nkubectl create deployment mongo --image us.icr.io/iac-registry/mongo:bionic --dry-run=client -o yaml > kubernetes/mongo.yaml\n```\n\n```yaml path=kubernetes/mongo.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo\n  name: mongo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n        - image: us.icr.io/iac-registry/mongo:bionic\n          name: mongo\n          envFrom:\n            - secretRef:\n                name: db-admin-password\n            - configMapRef:\n                name: config\n          volumeMounts:\n            - name: db-volume\n              mountPath: /data/init\n            - name: db-ca-cert\n              mountPath: \"/secret\"\n              readOnly: true\n```\n\nThen login into the container and run the `mongo` client like so.\n\n```bash\nkubectl exec --stdin --tty $(kubectl get pods | grep mongo | awk '{print $1}')  -- /bin/bash\n# verify the enviroment variable APP_MONGODB_URI has the password from $PASSWORD\nAPP_MONGODB_URI=$(echo $APP_MONGODB_URI | sed -e \"s/\\$PASSWORD/$PASSWORD/\" -e \"s/ibmclouddb/moviesdb/\")\necho $APP_MONGODB_URI\nmongo $APP_MONGODB_URI --tls --tlsCAFile $APP_SSL_CA_CERT\n```\n\nOr, instead, just execute this one-liner:\n\n```bash\nkubectl exec --stdin --tty $(kubectl get pods | grep mongo | awk '{print $1}')  -- /bin/bash -c 'mongo $(echo $APP_MONGODB_URI | sed -e \"s/\\$PASSWORD/$PASSWORD/\" -e \"s/ibmclouddb/moviesdb/\") --tls --tlsCAFile $APP_SSL_CA_CERT'\n```\n\n## Final Code\n\nAll the code used for this pattern is located and available to download in the GitHub repository https://github.com/IBM/cloud-enterprise-examples/ in the directory [09-containers](https://github.com/IBM/cloud-enterprise-examples/tree/master/iac/09-containers). The main files for the latest version (version 2) of the application are:\n\n```hcl path=network.tf\nresource \"ibm_is_vpc\" \"iac_iks_vpc\" {\n  name = \"${var.project_name}-${var.environment}-vpc\"\n}\n\nresource \"ibm_is_subnet\" \"iac_iks_subnet\" {\n  count                    = local.max_size\n  name                     = \"${var.project_name}-${var.environment}-subnet-${format(\"%02s\", count.index)}\"\n  zone                     = var.vpc_zone_names[count.index]\n  vpc                      = ibm_is_vpc.iac_iks_vpc.id\n  total_ipv4_address_count = 256\n  resource_group           = data.ibm_resource_group.group.id\n}\n\nresource \"ibm_is_security_group_rule\" \"iac_iks_security_group_rule_tcp_k8s\" {\n  count     = local.max_size\n  group     = ibm_is_vpc.iac_iks_vpc.default_security_group\n  direction = \"inbound\"\n  remote    = ibm_is_subnet.iac_iks_subnet[count.index].ipv4_cidr_block\n\n  tcp {\n    port_min = 30000\n    port_max = 32767\n  }\n}\n```\n\n```hcl path=iks.tf\nresource \"ibm_container_vpc_cluster\" \"iac_iks_cluster\" {\n  name              = \"${var.project_name}-${var.environment}-cluster\"\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  flavor            = var.flavors[0]\n  worker_count      = var.workers_count[0]\n  kube_version      = var.k8s_version\n  resource_group_id = data.ibm_resource_group.group.id\n  wait_till         = \"OneWorkerNodeReady\"\n  zones {\n    name      = var.vpc_zone_names[0]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[0].id\n  }\n}\n\nresource \"ibm_container_vpc_worker_pool\" \"iac_iks_cluster_pool\" {\n  count             = local.max_size - 1\n  cluster           = ibm_container_vpc_cluster.iac_iks_cluster.id\n  worker_pool_name  = \"${var.project_name}-${var.environment}-wp-${format(\"%02s\", count.index + 1)}\"\n  flavor            = var.flavors[count.index + 1]\n  vpc_id            = ibm_is_vpc.iac_iks_vpc.id\n  worker_count      = var.workers_count[count.index + 1]\n  resource_group_id = data.ibm_resource_group.group.id\n  zones {\n    name      = var.vpc_zone_names[count.index + 1]\n    subnet_id = ibm_is_subnet.iac_iks_subnet[count.index + 1].id\n  }\n}\n```\n\n```hcl path=db.tf\nresource \"ibm_database\" \"iac_app_db_instance\" {\n  name              = var.db_name\n  plan              = var.db_plan\n  location          = var.region\n  service           = \"databases-for-mongodb\"\n  resource_group_id = data.ibm_resource_group.group.id\n  service_endpoints = \"private\"\n\n  adminpassword                = var.db_admin_password\n  members_memory_allocation_mb = var.db_memory_allocation\n  members_disk_allocation_mb   = var.db_disk_allocation\n}\n```\n\n```hcl path=variables.tf\nvariable \"project_name\" {}\nvariable \"environment\" {}\n\nvariable \"resource_group\" {\n  default = \"Default\"\n}\nvariable \"region\" {\n  default = \"us-south\"\n}\nvariable \"vpc_zone_names\" {\n  type    = list(string)\n  default = [\"us-south-1\", \"us-south-2\", \"us-south-3\"]\n}\nvariable \"flavors\" {\n  type    = list(string)\n  default = [\"cx2.2x4\", \"cx2.4x8\", \"cx2.8x16\"]\n}\nvariable \"workers_count\" {\n  type    = list(number)\n  default = [3, 2, 1]\n}\nvariable \"k8s_version\" {\n  default = \"1.18.3\"\n}\n\nlocals {\n  max_size = length(var.vpc_zone_names)\n}\n\nvariable \"db_plan\" {\n  default = \"standard\"\n}\nvariable \"db_name\" {\n  default = \"moviedb\"\n}\nvariable \"db_admin_password\" {\n  default = \"inSecurePa55w0rd\"\n}\nvariable \"db_memory_allocation\" {\n  default = \"3072\"\n}\nvariable \"db_disk_allocation\" {\n  default = \"61440\"\n}\n```\n\n```hcl path=output.tf\noutput \"cluster_id\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.id\n}\noutput \"cluster_name\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.name\n}\noutput \"entrypoint\" {\n  value = ibm_container_vpc_cluster.iac_iks_cluster.public_service_endpoint_url\n}\n\noutput \"db_connection_string\" {\n  value = ibm_database.iac_app_db_instance.connectionstrings.0.composed\n}\noutput \"db_connection_certbase64\" {\n  value = ibm_database.iac_app_db_instance.connectionstrings.0.certbase64\n}\noutput \"db_admin_userid\" {\n  value = ibm_database.iac_app_db_instance.adminuser\n}\noutput \"db_id\" {\n  value = ibm_database.iac_app_db_instance.id\n}\noutput \"db_password\" {\n  value = var.db_admin_password\n}\n```\n\n```Dockerfile path=docker/2.0/Dockerfile\nFROM python:3.7-slim AS build\n\nRUN apt-get update && \\\n  apt-get install -y --no-install-recommends build-essential gcc && \\\n  pip install --upgrade pip && \\\n  pip install pip-tools\nRUN python -m venv /opt/venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCOPY app.py .\nCOPY import.py .\nCOPY requirements.txt requirements.in\nRUN pip-compile requirements.in > requirements.txt && \\\n  pip-sync && \\\n  pip install -r requirements.txt\n\nFROM python:3.7-slim AS app\nCOPY --from=build /opt/venv /opt/venv\n\nCOPY app.py .\nCOPY import.py .\nRUN chmod +x app.py import.py\n\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCMD [ \"python\", \"app.py\" ]\n\n```\n\n```yaml path=kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movies\n  template:\n    metadata:\n      labels:\n        app: movies\n    spec:\n      volumes:\n        - name: db-volume\n          configMap:\n            name: movies-db\n        - name: db-ca-cert\n          secret:\n            secretName: db-ssl-ca-cert\n      initContainers:\n        - name: init-db\n          image: us.icr.io/iac-registry/movies:2.3\n          command: [\"python\", \"import.py\"]\n          # command: [\"/bin/sh\", \"-c\", \"while true; do sleep 1000;done\"]\n          envFrom:\n            - secretRef:\n                name: db-admin-password\n            - configMapRef:\n                name: config\n          volumeMounts:\n            - name: db-volume\n              mountPath: /data/init\n            - name: db-ca-cert\n              mountPath: \"/secret\"\n              readOnly: true\n      containers:\n        - image: us.icr.io/iac-registry/movies:2.3\n          name: movies\n          envFrom:\n            - secretRef:\n                name: db-admin-password\n            - configMapRef:\n                name: config\n          volumeMounts:\n            - name: db-ca-cert\n              mountPath: \"/secret\"\n              readOnly: true\n```\n\n```yaml path=kubernetes/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: movies\n  name: movies\nspec:\n  ports:\n    - name: \"http\"\n      port: 80\n      protocol: TCP\n      targetPort: 8080\n  selector:\n    app: movies\n  type: LoadBalancer\n```\n\n## Clean up\n\nWhen you are done with the Kubernetes cluster should destroy it.\n\nIf you want to keep the cluster running but remove everything you have done, you can execute:\n\n```bash\nkubectl delete -f kubernetes/\n\nkubectl get configmap,secret,service,deployment,pod,pvc,pv\n```\n\nIf the cluster was created **using the IBM Cloud CLI**, execute the following commands:\n\n```bash\nNAME=iac-iks-cluster\nibmcloud ks cluster rm --cluster $NAME\n\nSubnet_Name=iac-iks-subnet\nSUBNET_ID=$(ibmcloud is subnets --json | jq -r \".[] | select(.name==\\\"$Subnet_Name\\\").id\")\nibmcloud is subnet-delete $SUBNET_ID\n\nVPC_Name=iac-iks-vpc\nVPC_ID=$(ibmcloud is vpcs --json | jq -r \".[] | select(.name==\\\"$VPC_Name\\\").id\")\nibmcloud is vpc-delete $VPC_Name\n```\n\nIf the cluster was created **using Terraform**, just need to execute the command:\n\n```bash\nterraform destroy\n```\n\nAnd, if the cluster was created **using IBM Cloud Schematics**, execute the following commands:\n\n```bash\nibmcloud schematics workspace list              # Identify the WORKSPACE_ID\nWORKSPACE_ID=\n\nibmcloud schematics destroy --id $WORKSPACE_ID  # Identify the Activity_ID\nibmcloud schematics logs  --id $WORKSPACE_ID --act-id Activity_ID\n\n# ... wait until it's done\n\nibmcloud schematics workspace delete --id $WORKSPACE_ID\nibmcloud schematics workspace list\n```\n","fileAbsolutePath":"/Users/johandry/Workspace/ibm/att-cloudnative/ibmcloud-pattern-guide/src/pages/iac-resources/container/index.mdx"}}}}